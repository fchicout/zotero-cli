TY  - JOUR
T1  - ConceptUML: Multiphase unsupervised threat detection via latent concept learning, Hidden Markov Models and topic modelling
AU  - Luong, Khanh
AU  - Mahboubi, Arash
AU  - Jarrad, Geoff
AU  - Camtepe, Seyit
AU  - Bewong, Michael
AU  - Bahutair, Mohammed
AU  - Aboutorab, Hamed
AU  - Bui, Hang Thanh
JO  - Journal of Information Security and Applications
VL  - 93
SP  - 104160
PY  - 2025
DA  - 2025/09/01/
SN  - 2214-2126
DO  - https://doi.org/10.1016/j.jisa.2025.104160
UR  - https://www.sciencedirect.com/science/article/pii/S2214212625001978
KW  - Threat hunting
KW  - Lateral movement
KW  - MITRE
KW  - CAPEC
KW  - Concept extraction
KW  - Hidden Markov Model
KW  - Topic modelling
AB  - Detecting lateral movement threats in large-scale system logs is a critical challenge due to the scarcity of labelled attack data, the presence of imbalanced datasets, and the sophisticated nature of modern adversaries. To address these issues, we propose ConceptUML, a semantic-driven, fully unsupervised threat detection framework designed to automatically identify anomalies related to lateral movement in heterogeneous log data. ConceptUML is structured around a three-phase architecture. In Phase 1 (Latent Semantic Learning), contextualized embeddings generated by Sentence-BERT are combined with Non-negative Matrix Factorization to extract abstract concepts from system logs and external threat intelligence sources such as MITRE ATT&CK and CAPEC. In Phase 2 (Unsupervised Threat Detection), a Hidden Markov Model is applied to cluster logs based on learned concepts, and each cluster is scored according to its semantic similarity to known adversarial techniques. Phase 3 (Decision Refinement) uses topic modelling to further isolate malicious event log subsets from within suspicious clusters, enabling high-precision triage. We evaluate ConceptUML using four real-world event log datasets, including Windows Event Logs and multiple subsets of the LMD-23 dataset, encompassing attacks such as exploitation of hashing techniques and remote services. The enhanced model with topic modelling achieves up to 92.54% detection quality and reduces detection error to as low as 8.14%, outperforming several baseline approaches including AutoEncoder, LogAnomaly, LOF, and DBScan. Our results confirm that ConceptUML delivers interpretable, scalable, and highly effective detection of lateral movement threats without requiring labelled training data or extensive manual feature engineering.
ER  - 

TY  - JOUR
T1  - LangurTrace: Forensic analysis of local LLM applications
AU  - Jeong, Sungjo
AU  - Lee, Sangjin
AU  - Park, Jungheum
JO  - Forensic Science International: Digital Investigation
VL  - 54
SP  - 301987
PY  - 2025
DA  - 2025/10/01/
T2  - DFRWS APAC 2025 - Selected Papers from the 5th Annual Digital Forensics Research Conference APAC
SN  - 2666-2817
DO  - https://doi.org/10.1016/j.fsidi.2025.301987
UR  - https://www.sciencedirect.com/science/article/pii/S2666281725001271
KW  - Digital forensics
KW  - Large language model (LLM)
KW  - Local LLM
KW  - Forensic artifact
KW  - Forensic tool development
AB  - A wide variety of applications have been developed to simplify the use of Large Language Models (LLMs), raising the importance of systematically analyzing their forensic artifacts. This study proposes a structured framework for LLM application environments, categorizing applications into backend runtime, client interface, and integrated platform components. Through experimental analysis of representative applications, we identify and classify artifacts such as chat records, uploaded fils, generated files, and model setup histories. These artifacts provide valuable insight into user behavior and intent. For instance, LLM-generated files can serve as direct evidence in criminal investigations, particularly in cases involving the creation or distribution of illicit media, such as CSAM. The structured environment model further enables investigators to anticipate artifacts even in applications not directly analyzed. This study lays a foundational methodology for LLM application forensics, offering practical guidance for forensic investigations. To support practical adoption and reproducibility, we also release LangurTrace, an open-source tool that automates the collection and analysis of these artifacts.
ER  - 

TY  - JOUR
T1  - Optimized detection of cyber-attacks on IoT networks via hybrid deep learning models
AU  - Bensaoud, Ahmed
AU  - Kalita, Jugal
JO  - Ad Hoc Networks
VL  - 170
SP  - 103770
PY  - 2025
DA  - 2025/04/01/
SN  - 1570-8705
DO  - https://doi.org/10.1016/j.adhoc.2025.103770
UR  - https://www.sciencedirect.com/science/article/pii/S1570870525000186
KW  - IoT security
KW  - Self-Organizing Maps (SOMs)
KW  - Deep Belief Networks (DBNs)
KW  - Autoencoders
KW  - Optimization techniques
KW  - Real-time IoT threat detection
AB  - The rapid expansion of Internet of Things (IoT) devices has significantly increased the potential for cyber-attacks, making effective detection methods crucial for securing IoT networks. This paper presents a novel approach for detecting cyber-attacks in IoT environments by combining Self-Organizing Maps (SOMs), Deep Belief Networks (DBNs), and Autoencoders. These techniques are employed to create a system capable of identifying both known and previously unseen attack patterns. A comprehensive experimental framework is established to evaluate the methodology using both simulated and real-world traffic data. The models are fine-tuned using Particle Swarm Optimization (PSO) to achieve optimal performance. The system’s effectiveness is assessed using standard cybersecurity metrics, with results showing an accuracy of up to 99.99% and Matthews Correlation Coefficient (MCC) values exceeding 99.50%. Experiments conducted on three well-established datasets NSL-KDD, UNSW-NB15, and CICIoT2023 demonstrate the model’s strong performance in detecting various attack types. These findings suggest that the proposed approach can significantly enhance the security of IoT systems by accurately identifying emerging threats and adapting to evolving attack strategies.
ER  - 

TY  - JOUR
T1  - LM-Hunter: An NLP-powered graph method for detecting adversary lateral movements in APT cyber-attacks at scale
AU  - Pérez-Gomariz, Mario
AU  - Cerdán-Cartagena, Fernando
AU  - García, Jess
JO  - Computer Networks
VL  - 262
SP  - 111181
PY  - 2025
DA  - 2025/05/01/
SN  - 1389-1286
DO  - https://doi.org/10.1016/j.comnet.2025.111181
UR  - https://www.sciencedirect.com/science/article/pii/S1389128625001495
KW  - Anomaly detection
KW  - Advanced persistent threat
KW  - Lateral movements
KW  - Cybersecurity knowledge graphs
KW  - Natural language processing
KW  - Transformers
AB  - APT (Advanced Persistent Threat) actors are highly skilled cyber attackers who employ sophisticated techniques to infiltrate and maintain unauthorized access to a network over an extended period. In the APT lifecycle, lateral movement stands out as a critical stage where intruders escalate privileges and move across the network to expand their control and access to sensitive data. While solutions such as UEBA (User and Entity Behavior Analytics) or graph analysis have been proposed to identify lateral movements, their application in real-world cybersecurity incidents remains impractical in terms of both scalability and performance. This paper introduces LM-Hunter, a new robust and efficient method for identifying stealth adversaries moving laterally through the network at scale. LM-Hunter takes advantage of graphs and Transformers, a specific architecture within NLP (Natural Language Processing), to learn the network dynamics for hunting the most suspicious lateral movements of the users. The method is validated in a real-world cybersecurity incident at a Fortune 500 company, one of the largest corporations in the United States, demonstrating its capability to identify adversarial lateral movements in large enterprise networks. LM-Hunter enhances the threat detection capabilities of Incident Response and Threat Hunting teams in real-world scenarios. The application of the method is facilitated by releasing LM-Hunter as an open-source tool, expanding the arsenal of cybersecurity teams for combating cyber threats.
ER  - 

TY  - JOUR
T1  - ChatGPT or Bard: Who is a better Certified Ethical Hacker?
AU  - Raman, Raghu
AU  - Calyam, Prasad
AU  - Achuthan, Krishnashree
JO  - Computers & Security
VL  - 140
SP  - 103804
PY  - 2024
DA  - 2024/05/01/
SN  - 0167-4048
DO  - https://doi.org/10.1016/j.cose.2024.103804
UR  - https://www.sciencedirect.com/science/article/pii/S0167404824001056
KW  - Ethical hacking
KW  - Policy
KW  - Social behavior
KW  - Readability
KW  - Similarity analysis
KW  - Cybersecurity generative ai
AB  - In this study, we compare two leading Generative AI (GAI) tools, ChatGPT and Bard, specifically in Cybersecurity, using a robust set of standardized questions from a validated Certified Ethical Hacking (CEH) dataset. In the rapidly evolving domain of Generative AI (GAI) and large language models (LLM), a comparative analysis of tools becomes essential to measure their performance. We determine the Comprehensiveness, Clarity, and Conciseness of the AI-generated responses through a detailed questioning-based framework. The study revealed an overall accuracy rate of 80.8 % for ChatGPT and 82.6 % for Bard, indicating comparable capabilities and specific differences. Bard slightly outperformed ChatGPT in accuracy, while ChatGPT exhibited superiority in Comprehensiveness, Clarity, and Conciseness of responses. Introducing a confirmation query like “Are you sure?” increased accuracy for both generative AI tools, illustrating the potential of iterative query processing in enhancing GAI tools' effectiveness. The readability evaluation placed both tools at a college reading level, with Bard marginally more accessible. While evaluating certain questions, a distinct pattern emerged where Bard provided generic denials of assistance while ChatGPT referenced “ethics.” This discrepancy illustrates the contrasting philosophies of the developers of these tools, with Bard possibly following stricter guidelines, especially in sensitive topics like Cybersecurity. We explore the implications and identify key areas for future research that become increasingly relevant as GAI tools see broader adoption.
ER  - 

TY  - JOUR
T1  - An LLM-guided SD-LDM Digital Twin Construction Strategy (LSDT) for multi-industrial scenarios: Enhancing adaptability and efficiency
AU  - Wang, Feixiang
AU  - Liu, Xiaojun
AU  - Lv, Feng
AU  - Wang, Chongxin
AU  - Shi, Jin
AU  - Zheng, Xiaotian
AU  - Li, Chao
JO  - Journal of Manufacturing Systems
VL  - 80
SP  - 995
EP  - 1012
PY  - 2025
DA  - 2025/06/01/
SN  - 0278-6125
DO  - https://doi.org/10.1016/j.jmsy.2025.04.019
UR  - https://www.sciencedirect.com/science/article/pii/S0278612525001074
KW  - Digital twin
KW  - Model challenges
KW  - Large language models
KW  - Stable diffusion
KW  - Latent diffusion models
AB  - In the rapidly evolving landscape of Industry 5.0, Digital Twin (DT) have emerged as a transformative technology across various industrial sectors. However, as DT theory and practice progress, a critical issue arises: the prolonged building time associated with implementing DTs. To address this challenge, this paper proposes a rapid DT construction method: LLM-Guided SD-LDM Digital Twin Construction Strategy (LSDT) for multi-scenarios. Firstly, we introduce a cross-modal generation framework. This framework leverages Large Language Model (LLM)-Guided Stable Diffusion- Latent Diffusion Model (SD-LDM) technology, which is capable of swiftly constructing high-quality 3D models based on limited multimodal data. Subsequently, the generated models are transferred into the Digital twin construction framework. This framework incorporates both the DT construction method and the assembly and fusion method, enabling the realization of a multi-scale, multi-level DT construction. Finally, we conducted case study in R&D laboratories, prototype warehouses, and packaging units. The multi-dimensional scoring results showed that the model construction efficiency improved significantly, with peak values reaching 39 % (across models) and 73 % (single model), while usability scores peaked at 13.84. Furthermore, the constructed DT successfully met the core Ss requirements of the scenarios. These results indicate that the LSDT method accelerates the efficiency for DT construction and offers good adaptability.
ER  - 

TY  - JOUR
T1  - An explainable framework for assisting the detection of AI-generated textual content
AU  - Yan, Sen
AU  - Wang, Zhiyi
AU  - Dobolyi, David
JO  - Decision Support Systems
VL  - 196
SP  - 114498
PY  - 2025
DA  - 2025/09/01/
SN  - 0167-9236
DO  - https://doi.org/10.1016/j.dss.2025.114498
UR  - https://www.sciencedirect.com/science/article/pii/S0167923625000995
KW  - Generative AI (GenAI)
KW  - AI-generated content (AIGC)
KW  - Large language model (LLM)
KW  - AIGC detection
KW  - Deep learning
KW  - Explainable AI (XAI)
AB  - The recent development of generative AI (GenAI) algorithms has allowed machines to create new content in a realistic way, driving the spread of AI-generated content (AIGC) on the Internet. However, generative AI models and AIGC have exacerbated several societal challenges such as security threats (e.g., misinformation), trust issues, ethical concerns, and intellectual property regulation, calling for effective detection methods and a better understanding of AI-generated vs. human-written content. In this paper, we focus on AI-generated texts produced by large language models (LLMs) and extend prior detection methods by proposing a novel framework that combines semantic information and linguistic features. Based on potential semantic and linguistic differences in AI vs. human writing, we design our Semantic-Linguistic-Detector (SemLinDetector) framework by integrating a transformer-based semantic encoder and a linguistic encoder with parallel linguistic representations. By comparing a series of benchmark models on datasets collected from various LLMs and human writers in multiple domains, our experiments show that the proposed detection framework outperforms other benchmarks in a consistent and robust manner. Moreover, our model interpretability analysis showcases our framework's potential to help understand the reasoning behind prediction outcomes and identify patterns of differences in AI-generated and human-written content. Our research adds to the growing space of GenAI by proposing an effective and responsible detection system to address the risks and challenges of GenAI, offering implications for researchers and practitioners to better understand and regulate AIGC.
ER  - 

TY  - JOUR
T1  - ChatGPT: A canary in the coal mine or a parrot in the echo chamber? Detecting fraud with LLM: The case of FTX
AU  - Gregory, Gadzinski
AU  - Vito, Liuzzi
JO  - Finance Research Letters
VL  - 70
SP  - 106349
PY  - 2024
DA  - 2024/12/01/
SN  - 1544-6123
DO  - https://doi.org/10.1016/j.frl.2024.106349
UR  - https://www.sciencedirect.com/science/article/pii/S1544612324013783
KW  - LLMs
KW  - FTX
KW  - Fraud detection
KW  - RAG
AB  - Does the paradigm shift brought by Large Language Models (LLMs) hold the promise of revolutionizing financial analysis? Our article tackles this question by exploring fraud detection in cryptocurrency exchanges, with a focus on FTX. We study the abilities of generative artificial intelligence tools like ChatGPT to serve as early-warning systems of fraud and identify red flags in the particular and difficult case where no financial information is available. We recognize several challenges to provide insights beyond human knowledge. To achieve a higher degree of scrutiny, we highlight the role of sequential interactions between the AI Chatbot and the researcher as well as the inclusion of external contents, a technique known as Retrieval Augmented Generation (RAG). Therefore, this article serves as a cautionary tale on the necessary conditions to achieve augmented intelligence.
ER  - 

TY  - JOUR
T1  - Stylometry recognizes human and LLM-generated texts in short samples
AU  - Przystalski, Karol
AU  - Argasiński, Jan K.
AU  - Grabska-Gradzińska, Iwona
AU  - Ochab, Jeremi K.
JO  - Expert Systems with Applications
VL  - 296
SP  - 129001
PY  - 2026
DA  - 2026/01/15/
SN  - 0957-4174
DO  - https://doi.org/10.1016/j.eswa.2025.129001
UR  - https://www.sciencedirect.com/science/article/pii/S0957417425026181
KW  - Stylometry
KW  - Large language models
KW  - Machine-generated text detection
KW  - AI detection
KW  - Benchmark dataset
AB  - The paper explores stylometry as a method to distinguish between texts created by Large Language Models (LLMs) and humans, addressing issues of model attribution, intellectual property, and ethical AI use. Stylometry has been used extensively to characterise the style and attribute authorship of texts. By applying it to LLM-generated texts, we identify their emergent writing patterns. The paper involves creating a benchmark dataset based on Wikipedia, with (a) human-written term summaries, (b) texts generated purely by LLMs (GPT-3.5/4, LLaMa 2/3, Orca, and Falcon), (c) processed through multiple text summarisation methods (T5, BART, Gensim, and Sumy), and (d) rephrasing methods (Dipper, T5). The 10-sentence long texts were classified by tree-based models (decision trees and LightGBM) using human-designed (StyloMetrix) and n-gram-based (our own pipeline) stylometric features that encode lexical, grammatical, syntactic, and punctuation patterns. The cross-validated results reached a performance of up to 0.87 Matthews correlation coefficient in the multiclass scenario with 7 classes, and accuracy between 0.79 and 1. in binary classification, with the particular example of Wikipedia and GPT-4 reaching up to 0.98 accuracy on a balanced dataset. Shapley Additive Explanations pinpointed features characteristic of the encyclopaedic text type, individual overused words, as well as a greater grammatical standardisation of LLMs with respect to human-written texts. These results show – crucially, in the context of the increasingly sophisticated LLMs – that it is possible to distinguish machine- from human-generated texts at least for a well-defined text type
ER  - 

TY  - JOUR
T1  - A novel approach for software vulnerability detection based on ensemble learning model
AU  - Xuan, Cho Do
AU  - Quang, Dat Bui
AU  - Quang, Vinh Dang
JO  - Computers and Electrical Engineering
VL  - 130
SP  - 110848
PY  - 2026
DA  - 2026/02/01/
SN  - 0045-7906
DO  - https://doi.org/10.1016/j.compeleceng.2025.110848
UR  - https://www.sciencedirect.com/science/article/pii/S0045790625007918
KW  - Software vulnerability detection
KW  - Embedding code, Transformer encoder, Large language models, Ensemble learning
AB  - This paper proposes a novel approach for detecting vulnerabilities in source code written in C and C++, leveraging large language models (LLMs). Specifically, the study introduces a new model called RoS-Dex, based on ensemble learning techniques and comprising two main components: Code Understanding (CU) and Vulnerability Encoder (VE). Accordingly, the CU module is developed using code embedding techniques and a transformer-based architecture, enabling it to capture the semantic features of source code comprehensively, while the VE module focuses on encoding vulnerability-related features, thereby improving classification performance. In the experimental evaluation, the RoS-Dex model demonstrated effectiveness not only on a single dataset but also across four datasets with different structures and characteristics, including REVEAL, FFMQ+QEMU, BigVul, and RealVul. Furthermore, the RoS-Dex model also showcased its adaptability by successfully passing cross-data validation tests—one of the most rigorous evaluation methods that very few LLM-based models have managed to pass successfully. These results highlight the strong potential of the proposed model for real-world applications and pave the way for future research in C and C++ vulnerability detection.
ER  - 

TY  - JOUR
T1  - LogOW: A semi-supervised log anomaly detection model in open-world setting
AU  - Ye, Jingwei
AU  - Liu, Chunbo
AU  - Gu, Zhaojun
AU  - Zhang, Zhikai
AU  - Meng, Xuying
AU  - Zhang, Weiyao
AU  - Zhang, Yujun
JO  - Journal of Systems and Software
VL  - 222
SP  - 112305
PY  - 2025
DA  - 2025/04/01/
SN  - 0164-1212
DO  - https://doi.org/10.1016/j.jss.2024.112305
UR  - https://www.sciencedirect.com/science/article/pii/S0164121224003492
KW  - Semi-supervised
KW  - Log anomaly detection
KW  - Open-world
KW  - Uncertainty estimation
KW  - Incremental pre-training
KW  - Cold-start
AB  - Log anomaly detection is a method for finding abnormal behavior and faults in systems. However, existing methods face two main challenges: the open-world problem and the cold-start problem. The open-world problem means that the test set may contain new classes that are not in the training set, while the cold-start problem means that the initial training data are scarce, both for normal and abnormal log sequences. Most existing methods assume a closed-world setting and rely on sufficient normal data, which limits their adaptability to new log environments. We propose LogOW, a novel log anomaly detection model that can learn from a few normal log sequences. The model finds emerging normal log sequences in the open-world setting through the open-world sample retrieval module. Through the incremental pre-training module, these log sequences are fine-tuned in an online mode for model parameters. First, we train a basic model from normal log sequences using Masked-Language Modeling(MLM). During the testing phase, we then combine the anomaly score and the uncertainty score obtained through a novel dynamic multi-mask to distinguish closed-world normal log sequences from the test set. Next, we cluster the open-world log sequences based on fused sequence and count features, and identify the abnormal ones and the new normal ones. Finally, we update our model with the new normal sequences in the next time period. Experiments on three log datasets and real-world airport logs show that our model outperforms traditional models in the open-world and lack of training data setting.
ER  - 

TY  - JOUR
T1  - Towards the future of pedestrian–AV interaction: Human perception vs. LLM insights on Smart Pole Interaction Unit in shared spaces
AU  - Chauhan, Vishal
AU  - Anubhav, 
AU  - Chang, Chia-Ming
AU  - Su, Xiang
AU  - Nakazato, Jin
AU  - Javanmardi, Ehsan
AU  - Orsholits, Alex
AU  - Igarashi, Takeo
AU  - Fujiwara, Kantaro
AU  - Tsukada, Manabu
JO  - International Journal of Human-Computer Studies
VL  - 205
SP  - 103628
PY  - 2025
DA  - 2025/11/01/
SN  - 1071-5819
DO  - https://doi.org/10.1016/j.ijhcs.2025.103628
UR  - https://www.sciencedirect.com/science/article/pii/S1071581925001855
KW  - Autonomous vehicles
KW  - Human-centered artificial intelligence
KW  - External human–machine interface
KW  - Large Language Models
KW  - SMart Pole Interaction Unit
AB  - As autonomous vehicles (AVs) reshape urban mobility, establishing effective communication between pedestrians and self-driving vehicles has become a critical safety imperative. This work investigates the integration of Smart Pole Interaction Units (SPIUs) as external human–machine interfaces (eHMIs) in shared spaces and introduces an innovative approach to enhance pedestrian–AV interactions. To provide subjective evidence on SPIU usability, we conduct a group design study (“Humans”) involving 25 participants (aged 18–40). We evaluate user preferences and interaction patterns using group discussion materials, revealing that 90% of the participants strongly prefer real-time multi-AV interactions facilitated by SPIU over conventional eHMI systems, where a pedestrian must look at multiple AVs individually. Furthermore, they emphasize inclusive design through multi-sensory communication channels—visual, auditory, and tactile signals—specifically addressing the needs of vulnerable road users (VRUs), including those with impairments. To complement these non-expert, real-world insights, we employ three leading Large Language Models (LLMs) (ChatGPT-4, Gemini-Pro, and Claude 3.5 Sonnet) as “experts” due to their extensive training data. Using the advantages of the multimodal vision-language processing capabilities of these LLMs, identical questions (text and images) used in human discussions are posed to generate text responses for pedestrian–AV interaction scenarios. Responses generated from LLMs and recorded conversations from human group discussions are used to extract the most frequent words. A keyword frequency analysis from both humans and LLMs is performed with three categories, Context, Safety, and Important. Our findings indicate that LLMs employ safety-related keywords 30% more frequently than human participants, suggesting a more structured, safety-centric approach. Among LLMs, ChatGPT-4 demonstrates superior response latency, Claude shows a closer alignment with human responses, and Gemini-Pro provides structured and contextually relevant insights. Our results from “Humans” and “LLMs” establish SPIU as a promising system for facilitating trust-building and safety-ensuring interactions among pedestrians, AVs, and delivery robots. Integrating diverse stakeholder feedback, we propose a prototype SPIU design to advance pedestrian–AV interactions in shared urban spaces, positioning SPIU as crucial infrastructure hubs for safe and trustworthy navigation.
ER  - 

TY  - JOUR
T1  - PTFusion: LLM-driven context-aware knowledge fusion for web penetration testing
AU  - Wang, Wenhao
AU  - Gu, Hao
AU  - Wu, Zhixuan
AU  - Chen, Hao
AU  - Chen, Xingguo
AU  - Shi, Fan
JO  - Information Fusion
VL  - 127
SP  - 103731
PY  - 2026
DA  - 2026/03/01/
SN  - 1566-2535
DO  - https://doi.org/10.1016/j.inffus.2025.103731
UR  - https://www.sciencedirect.com/science/article/pii/S1566253525007936
KW  - Large language models
KW  - Web penetration testing
KW  - Multi-agent systems
KW  - Dynamic knowledge graphs
KW  - Chain-of-Thought
AB  - This paper presents PTFusion, an LLM-driven web penetration testing framework that addresses inefficient task guidance and imprecise command execution challenges in web penetration testing. Employing a semi-decentralized multi-agent collaborative architecture, PTFusion maintains strategic coherence while enabling autonomous tactical execution, and uses the Model Context Protocol to more conveniently call different types of penetration testing tools. To effectively guide task execution, the PTFusion designs a context-aware knowledge fusion mechanism to plan tasks based on the dynamic knowledge graph and executed actions, and uses the preference-based chain-of-thought prompting to address the issue of redundant and difficult to align outputs from different types of penetration testing tools. Compared to methods like PentestGPT, PTFusion demonstrates significantl superior performance in both task completion effectiveness and stability. The context-aware knowledge fusion mechanism enables PTFusion to conduct more precise strategic planning and execute penetration testing commands with greater accuracy, ensuring reliable completion of web penetration testing tasks across various scenarios.
ER  - 

TY  - JOUR
T1  - LogPrécis: Unleashing language models for automated malicious log analysis: Précis: A concise summary of essential points, statements, or facts
AU  - Boffa, Matteo
AU  - Drago, Idilio
AU  - Mellia, Marco
AU  - Vassio, Luca
AU  - Giordano, Danilo
AU  - Valentim, Rodolfo
AU  - Houidi, Zied Ben
JO  - Computers & Security
VL  - 141
SP  - 103805
PY  - 2024
DA  - 2024/06/01/
SN  - 0167-4048
DO  - https://doi.org/10.1016/j.cose.2024.103805
UR  - https://www.sciencedirect.com/science/article/pii/S0167404824001068
KW  - Language models
KW  - Automatic log parsing
KW  - Unix shell attacks
KW  - Honeypot
KW  - Attack fingerprint
AB  - Security logs are the key to understanding attacks and diagnosing vulnerabilities. Often coming in the form of text logs, their analysis remains a daunting challenge. Language Models (LMs) have demonstrated unmatched potential in understanding natural and programming languages. The question arises as to whether and how LMs could be also used to automatise the analysis of security logs. We here systematically study how to benefit from the state-of-the-art LM to support the analysis of text-like Unix shell attack logs automatically. For this, we thoroughly designed LogPrécis. LogPrécis receives as input malicious shell sessions. It then automatically identifies and assigns the attacker tactic to each portion of the session, i.e., unveiling the sequence of the attacker's goals. This creates a unique attack fingerprint. We demonstrate LogPrécis capability to support the analysis of two large datasets containing about 400,000 unique Unix shell attacks recorded in a 2-year-long honeypot deployment. LogPrécis reduces the analysis to about 3,000 unique fingerprints. Such abstraction lets us better understand attacks, extract attack prototypes, detect novelties, and track families and mutations. Overall, LogPrécis, released as open source, demonstrates the potential of adopting LMs for security analysis and paves the way for better and more responsive defence against cyberattacks.
ER  - 

TY  - JOUR
T1  - Neurosymbolic AI for network intrusion detection systems: A survey
AU  - Bizzarri, Alice
AU  - Yu, Chung-En (Johnny)
AU  - Jalaian, Brian
AU  - Riguzzi, Fabrizio
AU  - Bastian, Nathaniel D.
JO  - Journal of Information Security and Applications
VL  - 94
SP  - 104205
PY  - 2025
DA  - 2025/11/01/
SN  - 2214-2126
DO  - https://doi.org/10.1016/j.jisa.2025.104205
UR  - https://www.sciencedirect.com/science/article/pii/S221421262500242X
KW  - Neurosymbolic artificial intelligence
KW  - Network intrusion detection system
KW  - Cybersecurity
KW  - Artificial intelligence
AB  - Current data-driven AI approaches in Network Intrusion Detection System (NIDS) face challenges related to high resource consumption, high computational demands, and limited interpretability. Moreover, they often struggle to detect unknown and rapidly evolving cyber threats. This survey explores the integration of Neurosymbolic AI (NeSy AI) into NIDS, combining the data-driven capabilities of Deep Learning (DL) with the structured reasoning of symbolic AI to address emerging cybersecurity threats. The integration of NeSy AI into NIDS demonstrates significant improvements in both the detection and interpretation of complex network threats by exploiting the advanced pattern recognition typical of neural processing and the interpretive capabilities of symbolic reasoning. In this survey, we categorise the analysed NeSy AI approaches applied to NIDS into logic-based and graph-based representations. Logic-based approaches emphasise symbolic reasoning and rule-based inference. On the other hand, graph-based representations capture the relational and structural aspects of network traffic. We examine various NeSy systems applied to NIDS, highlighting their potential and main challenges. Furthermore, we discuss the most relevant issues in the field of NIDS and the contribution NeSy can offer. We present a comparison between the main XAI techniques applied to NIDS in the literature and the increased explainability offered by NeSy systems.
ER  - 

TY  - JOUR
T1  - Severity-based triage of cybersecurity incidents using kill chain attack graphs
AU  - Sadlek, Lukáš
AU  - Yamin, Muhammad Mudassar
AU  - Čeleda, Pavel
AU  - Katt, Basel
JO  - Journal of Information Security and Applications
VL  - 89
SP  - 103956
PY  - 2025
DA  - 2025/03/01/
SN  - 2214-2126
DO  - https://doi.org/10.1016/j.jisa.2024.103956
UR  - https://www.sciencedirect.com/science/article/pii/S2214212624002588
KW  - Kill chain
KW  - Attack graph
KW  - Incident severity
KW  - Incident triage
KW  - MITRE ATT&CK
KW  - Cyber crisis
AB  - Security teams process a vast number of security events. Their security analysts spend considerable time triaging cybersecurity alerts. Many alerts reveal incidents that must be handled first and escalated to the more experienced staff to allow appropriate responses according to their severity. The current state requires an automated approach, considering contextual relationships among security events, especially detected attack tactics and techniques. In this paper, we propose a new graph-based approach for incident triage. First, it generates a kill chain attack graph from host and network data. Second, it creates sequences of detected alerts that could represent ongoing multi-step cyber attacks and matches them with the attack graph. Last, it assigns severity levels to the created sequences of alerts according to the most advanced kill chain phases that were used and the criticality of assets. We implemented the approach using the MulVAL attack graph generator and generation rules for MITRE ATT&CK techniques. The evaluation was accomplished in a testbed where multi-step attack scenarios were executed. Classification of sequences of alerts based on computed match scores obtained 0.95 area under the receiver operating characteristic curve in a feasible time. Moreover, a threshold exists for classifying 80% of positive sequences correctly and only a small percentage of negative sequences wrongly. Therefore, the approach selects malicious sequences of alerts and significantly improves incident triage.
ER  - 

TY  - JOUR
T1  - A Traffic Normalization Location Attention Network for cyber attack detection in Industrial Cyber-Physical Systems
AU  - Cheng, Minpeng
AU  - Chen, Yongyi
AU  - Zhang, Dan
JO  - Ad Hoc Networks
VL  - 178
SP  - 103965
PY  - 2025
DA  - 2025/11/01/
SN  - 1570-8705
DO  - https://doi.org/10.1016/j.adhoc.2025.103965
UR  - https://www.sciencedirect.com/science/article/pii/S1570870525002136
KW  - Industrial cyber–physical systems
KW  - Cyber attack detection
KW  - Dynamic weighting
KW  - Location information
KW  - Redundant information
AB  - Cyber attacks are known as one of the main threats of Industrial Cyber-Physical Systems (ICPSs). Although the existing Deep Learning (DL) -based methods can detect cyber attacks to a certain extent, they have shortcomings in weighting the location information of traffic sampling points and have poor suppression effect on redundant information in the spatial dimension, which may lead to an insufficient performance. Based on the above issues, a Traffic Normalization Location Attention Network (TNLAN) is proposed in this paper. Firstly, the location information between the sampling points is dynamically weighted to improve the network weights of the traffic locations. Then, the scale factors of the batch normalization layer are applied to help the detection network suppress the redundant information in the spatial dimension. The results show that TNLAN outperforms existing methods in detecting cyber attacks.
ER  - 

TY  - JOUR
T1  - JAPPI: An unsupervised endpoint application identification methodology for improved Zero Trust models, risk score calculations and threat detection
AU  - Heino, Jenny
AU  - Jalio, Christian
AU  - Hakkala, Antti
AU  - Virtanen, Seppo
JO  - Computer Networks
VL  - 250
SP  - 110606
PY  - 2024
DA  - 2024/08/01/
SN  - 1389-1286
DO  - https://doi.org/10.1016/j.comnet.2024.110606
UR  - https://www.sciencedirect.com/science/article/pii/S1389128624004389
KW  - Network security
KW  - Intrusion detection
KW  - Machine learning
KW  - Zero Trust
KW  - Risk score
AB  - The surge in global digitalization triggered by COVID-19 has led to a significant increase in internet traffic and has precipitated a rapid transformation of the network security landscape. Despite being increasingly difficult, accurate traffic inspection is vital for ensuring productivity while reliably protecting internal assets. Endpoint application identification enables high accuracy inspection and detection by providing network security solutions with specific context on individual connections. However, achieving it in real-time with standard fingerprinting methods based only on client-side traffic has proven to be a challenging problem with no comprehensive solution thus far. In this article, we present a new methodology for identifying endpoint applications from network traffic, utilizing machine learning. Our methodology leverages similarities in the pre-hash string of the JA3 algorithm for fingerprinting application specific TLS Client Hello messages. By utilizing well-known clustering algorithms it is possible to identify the underlying TLS libraries and the application from the traffic remarkably better than with simple string-based matching. Our model can categorize 99,5% of the traffic in a controlled network, and 93,8% in an uncontrolled network, compared to 0,1% and 0,2% using simple string matching. Our methodology is especially effective for enhancing Zero Trust models, calculating a risk score for network events, and improving threat detection accuracy in network security solutions.
ER  - 

TY  - JOUR
T1  - Decoding developer password patterns: A comparative analysis of password extraction and selection practices
AU  - Lykousas, Nikolaos
AU  - Patsakis, Constantinos
JO  - Computers & Security
VL  - 145
SP  - 103974
PY  - 2024
DA  - 2024/10/01/
SN  - 0167-4048
DO  - https://doi.org/10.1016/j.cose.2024.103974
UR  - https://www.sciencedirect.com/science/article/pii/S0167404824002797
KW  - Passwords
KW  - DevOps
KW  - Hard-coded passwords
KW  - Source code
KW  - DevSecOps
KW  - Large language models
AB  - Passwords play a crucial role in authentication, ensuring that only authorised entities can access sensitive information. However, user password choices are often weak and predictable, making them susceptible to cyber-attacks. Additionally, hard-coded credentials in source code can expose organisations and infrastructure to significant risks. This paper explores the patterns of passwords used by developers, examining their similarities to those of typical users. We also investigate the efficacy of large language models (LLMs) in identifying hard-coded credentials in source code. Our findings suggest that developers foster more complex and, hence, more secure password selection patterns than regular users. Nevertheless, they can use worse patterns when the context allows them. The latter, combined with the ample commits in public code repositories containing secrets, exemplifies the need for more targeted awareness campaigns and tighter integration of code security tools in the development lifecycle. Finally, we explore the capacity of LLMs to detect hard-coded credentials, highlighting their differences and limitations.
ER  - 

TY  - JOUR
T1  - Threat hunting for adversary impact inhibiting system recovery
AU  - Alsharabi, Naif
AU  - Bhardwaj, Akashdeep
AU  - Ayaba, Abdulaziz
AU  - Jadi, Amr
JO  - Computers & Security
VL  - 154
SP  - 104464
PY  - 2025
DA  - 2025/07/01/
SN  - 0167-4048
DO  - https://doi.org/10.1016/j.cose.2025.104464
UR  - https://www.sciencedirect.com/science/article/pii/S0167404825001531
KW  - Inhibit system recovery
KW  - Shadow copy
KW  - Elasticsearch
KW  - SIEM
KW  - BCEDIT
KW  - MITRE
KW  - T1490
KW  - Threat hunt
AB  - The rise of advanced cyber threats targeting critical system recovery mechanisms necessitates proactive and scalable threat-hunting solutions. This research introduces a novel methodology leveraging a Linux-based Elasticsearch server to detect adversary techniques that inhibit system recovery (T1490). By integrating Elasticsearch for centralized log storage, Kibana for dynamic visualization, and Lucene for precise query search, the proposed platform offers a cost-effective and adaptable alternative to proprietary SIEM solutions. The methodology emphasizes real-time identification of indicators of compromise (IOCs) such as shadow copy deletions, suspicious commands, and backup configuration modifications, enabling security teams to uncover adversarial behaviors before they disrupt recovery processes. Practical implementation demonstrates the platform's flexibility across diverse IT environments, accommodating logs from endpoints with varying operating systems and infrastructures. The study further highlights the adaptability of the approach, with Kibana dashboards and Lucene queries tailored to specific organizational needs, making it a versatile tool for enterprises. Additionally, the research underscores the significance of proactive detection by moving beyond traditional reactive methods, positioning organizations to address system recovery threats effectively. This work bridges a critical gap in cybersecurity by offering a scalable, open-source threat-hunting platform that aligns with the growing need for robust defenses against evolving adversary techniques. The findings hold practical significance for enhancing incident response strategies and bolstering organizational resilience, paving the way for future integration with advanced threat intelligence feeds and automated detection mechanisms. This novel approach not only strengthens the security landscape but also provides a blueprint for cost-efficient, real-world applications in defending against adversary techniques designed to inhibit system recovery.
ER  - 

TY  - JOUR
T1  - OSTIS: A novel Organization-Specific Threat Intelligence System
AU  - Arikkat, Dincy R.
AU  - P., Vinod
AU  - K.A., Rafidha Rehiman
AU  - Nicolazzo, Serena
AU  - Nocera, Antonino
AU  - Timpau, Georgiana
AU  - Conti, Mauro
JO  - Computers & Security
VL  - 145
SP  - 103990
PY  - 2024
DA  - 2024/10/01/
SN  - 0167-4048
DO  - https://doi.org/10.1016/j.cose.2024.103990
UR  - https://www.sciencedirect.com/science/article/pii/S0167404824002955
KW  - Cyber Threat Intelligence
KW  - Cybersecurity knowledge graph
KW  - Organization-specific threat intelligence
KW  - Relation extraction
KW  - Named entity recognition
KW  - Natural language processing
KW  - Explainable AI
AB  - With the increasing complexity and frequency of cyber attacks, organizations recognize the need for a proactive and targeted approach to safeguard their digital assets and operations. Every industry faces a distinct array of threats shaped by factors such as its industrial objective, geographic footprint, workforce size, revenue, partnerships, and the extent of its digital assets. This results in a wide heterogeneity in threat landscapes, which necessitates tailored threat intelligence sources. While some security practitioners may gravitate towards extensive sources, relying solely on volume-based solutions often leads to “alert fatigue”. For this reason, organization-specific threat intelligence has acquired a growing importance in cybersecurity defense. This work presents a complete and novel framework called OSTIS (Organization-Specific Threat Intelligence System) for generating and managing organization-specific Cyber Threat Intelligence (CTI) data. Our approach identifies reliable security blogs from which we gather CTI data through a custom and focused Web Crawler. Relevant content from such sources is, then, identified and extracted using automated deep-learning models. Moreover, our AI-driven solution maps CTI data to specific domain scenarios, such as education, finance, government, healthcare, industrial control systems, and IoT. To validate and gain insights from the trained models, we also include an explainable AI (XAI, for short) task carried out by leveraging the SHapley Additive exPlanations (SHAP) tool. This allows us to interpret the prediction process and discern influential content from data. The last step of our framework consists of the generation of an Organization Specific Threat Intelligence Knowledge Graph (OSTIKG), empowering organizations to identify and visualize attack patterns and incidents, promptly. To create this graph, we develop and adapt several techniques to extract diverse entities, including malware groups, campaigns, attack types, malware types, software tools, and so forth, and to identify relationships among them. Finally, through an extensive experimental campaign, we certify the validity and performance of all the components of our framework, which shows a 0.84 F1-score in the identification of relevant content, a 0.93 F1-score for the domain classification, and a 0.95 and 0.89 F1-score in the identification of entities and relations to build our OSTIKG graph.
ER  - 

TY  - JOUR
T1  - LSPR23: A novel IDS dataset from the largest live-fire cybersecurity exercise
AU  - Dijk, Allard
AU  - Halisdemir, Emre
AU  - Melella, Cosimo
AU  - Schu, Alari
AU  - Pihelgas, Mauno
AU  - Meier, Roland
JO  - Journal of Information Security and Applications
VL  - 85
SP  - 103847
PY  - 2024
DA  - 2024/09/01/
SN  - 2214-2126
DO  - https://doi.org/10.1016/j.jisa.2024.103847
UR  - https://www.sciencedirect.com/science/article/pii/S2214212624001492
KW  - Cybersecurity components
KW  - Intrusion detection
KW  - IDS dataset
KW  - Artificial intelligence
KW  - Autonomous agents
AB  - Cybersecurity threats are constantly evolving and becoming increasingly sophisticated, automated, adaptive, and intelligent. This makes it difficult for organizations to defend their digital assets. Industry professionals are looking for solutions to improve the efficiency and effectiveness of cybersecurity operations, adopting different strategies. In cybersecurity, the importance of developing new intrusion detection systems (IDSs) to address these threats has emerged. Most of these systems today are based on machine learning. But these systems need high-quality data to “learn” the characteristics of malicious traffic. Such datasets are difficult to obtain and therefore rarely available. This paper advances the state of the art and presents a new high-quality IDS dataset. The dataset originates from Locked Shields, one of the world’s most extensive live-fire cyber defense exercises. This ensures that (i) it contains realistic behavior of attackers and defenders; (ii) it contains sophisticated attacks; and (iii) it contains labels, as the actions of the attackers are well-documented. The dataset includes approximately 16 million network flows, [F3] of which approximately 1.6 million were labeled malicious. What is unique about this dataset is the use of a new labeling technique that increases the accuracy level of data labeling. We evaluate the robustness of our dataset using both quantitative and qualitative methodologies. We begin with a quantitative examination of the Suricata IDS alerts based on signatures and anomalies. Subsequently, we assess the reproducibility of machine learning experiments conducted by Känzig et al., who used a private Locked Shields dataset. We also apply the quality criteria outlined by the evaluation framework proposed by Gharib et al. Using our dataset with an existing classifier, we demonstrate comparable results (F1 score of 0.997) to the original paper where the classifier was evaluated on a private dataset (F1 score of 0.984)
ER  - 

TY  - JOUR
T1  - How informative are cybersecurity risk disclosures? Empirical analysis of firms targeted by ransomware
AU  - Adams, Matthew
AU  - Moore, Tyler
JO  - Computers & Security
VL  - 159
SP  - 104626
PY  - 2025
DA  - 2025/12/01/
SN  - 0167-4048
DO  - https://doi.org/10.1016/j.cose.2025.104626
UR  - https://www.sciencedirect.com/science/article/pii/S0167404825003153
KW  - Cybersecurity metrics
KW  - Security economics
KW  - Cybersecurity regulation
KW  - 10-K disclosure
AB  - Public companies face escalating requirements to disclose cybersecurity risks and damages in regulatory filings. In theory, such disclosures should equip investors with knowledge required to make informed decisions, while also encouraging firms to adopt more robust strategies for managing cybersecurity risks. In practice, discussions are often embedded in disparate locations of long documents full of legalese, which hinders systematic examination. This paper examines the regulatory filings of 61 firms that experienced ransomware incidents between 2018 and 2021. We describe a process whereby 7681 cyber-related statements were extracted from 314 10-K filings between 2018–23, then categorized using an iterative process inspired by grounded theory. We then perform quantitative and qualitative analysis of the statements, examining how firms discuss cybersecurity before and after experiencing an incident.
ER  - 

TY  - JOUR
T1  - Self-adaptive cyber defense for sustainable IoT: A DRL-based IDS optimizing security and energy efficiency
AU  - Jamshidi, Saeid
AU  - Amirnia, Ashkan
AU  - Nikanjam, Amin
AU  - Nafi, Kawser Wazed
AU  - Khomh, Foutse
AU  - Keivanpour, Samira
JO  - Journal of Network and Computer Applications
VL  - 239
SP  - 104176
PY  - 2025
DA  - 2025/07/01/
SN  - 1084-8045
DO  - https://doi.org/10.1016/j.jnca.2025.104176
UR  - https://www.sciencedirect.com/science/article/pii/S1084804525000736
KW  - The Internet of Things (ioT)
KW  - Intrusion Detection System (IDS)
KW  - Self-adaptation
KW  - Energy consumption
KW  - Sustainable IoT
KW  - Cybersecurity and sustainability
AB  - The Internet of Things (IoT) has revolutionized industries by creating a vast, interconnected ecosystem. Still, the rapid deployment of IoT devices has introduced severe security risks, including DDoS, DoS GoldenEye, DoS Hulk attacks, and Port scanning. Traditional Machine Learning (ML)-based Intrusion Detection Systems (IDS) often operate passively, detecting threats without taking action, and are rarely evaluated under real-time attacks. This limits our understanding of their performance within the resource constraints typical of IoT systems—an essential factor for stable, resilient systems. This paper proposes a Security Edge with Deep Reinforcement Learning (SecuEdge-DRL) specifically designed for the IoT edge, aiming to enhance security while maintaining energy efficiency, contributing to sustainable IoT operations. Our IDS integrates DRL with the MAPE-K (Monitor, Analyze, Plan, Execute, Knowledge) control loop, enabling real-time detection and adaptive response without relying on predefined data models. DRL allows continuous learning, while MAPE-K provides structured self-adaptation, ensuring the system remains effective against evolving threats. We also implemented four targeted security policies tailored to a specific attack type to enhance the IDS’s threat mitigation capabilities. Experimental findings indicate that the proposed SecuEdge-DRL achieves an average detection accuracy of 92% across diverse real-world cyber threats (e.g., DoS Hulk, DoS GoldenEyes, DDoS, and Port scanning). Statistical analysis further validates that these security policies enhance IoT systems’ defense without compromising performance, establishing our approach as a resilient, resource-efficient security solution for the IoT ecosystem.
ER  - 

TY  - JOUR
T1  - Proactive safety reasoning in human-robot collaboration in disassembly through LLM-augmented STPA and FMEA
AU  - Alenjareghi, Morteza Jalali
AU  - Ghorbani, Fardin
AU  - Keivanpour, Samira
AU  - Chinniah, Yuvin Adnarain
AU  - Jocelyn, Sabrina
JO  - Robotics and Computer-Integrated Manufacturing
VL  - 98
SP  - 103162
PY  - 2026
DA  - 2026/04/01/
SN  - 0736-5845
DO  - https://doi.org/10.1016/j.rcim.2025.103162
UR  - https://www.sciencedirect.com/science/article/pii/S0736584525002169
KW  - Large language models
KW  - Safety
KW  - STPA-FMEA
KW  - Human-robot collaboration
KW  - Risk assessment
KW  - Disassembly
AB  - Disassembly tasks in human–robot collaboration (HRC) environments present safety challenges due to hazardous materials, control system variability, and physically demanding operator tasks. To address these challenges, we propose an AI-augmented risk assessment framework integrating System-Theoretic Process Analysis (STPA) and Failure Mode and Effects Analysis (FMEA). This framework is implemented in four configurations: Term Frequency–Inverse Document Frequency (TF-IDF), Fine-tuned Large Language Models (LLMs), Retrieval-Augmented Generation (RAG), and RAG with a structured Knowledge Graph (KG) built from safety standards. The system supports real-time, standards-compliant safety reasoning by generating interpretable, context-specific recommendations. We evaluate these configurations across GPT-3.5 TURBO, GPT-4o, GPT-4.1, and open-source LLMs Qwen2.5 (3B) and Ministral (3B). Among all, RAG+KG with GPT-4.1 achieved the highest results across language-based metrics (BLEU: 68.3, ROUGE-L: 72.0, Semantic Similarity: 81.1, BERTScore (F1): 90.0) and safety-specific metrics (Hazard Recall: 92, Compliance Precision: 97, Safety Violation Rate: zero). Six safety-oriented metrics were introduced to assess compliance, hazard coverage, interpretability, and robustness. A case study on electrical vehicle (EV) battery module disassembly demonstrated the system’s effectiveness in identifying unsafe control actions, tracing failure modes, and recommending targeted mitigation strategies for mechanical, electrical, and chemical hazards, and ergonomic considerations. This framework offers a scalable, explainable approach to real-time safety analysis, advancing AI-enabled risk assessment in dynamic HRC disassembly tasks and supporting the vision of human-centered Industry 5.0 manufacturing.
ER  - 

TY  - JOUR
T1  - Cybersecurity of distributed energy resource systems in the smart grid: A survey
AU  - Chen, Juanwei
AU  - Yan, Jun
AU  - Kemmeugne, Anthony
AU  - Kassouf, Marthe
AU  - Debbabi, Mourad
JO  - Applied Energy
VL  - 383
SP  - 125364
PY  - 2025
DA  - 2025/04/01/
SN  - 0306-2619
DO  - https://doi.org/10.1016/j.apenergy.2025.125364
UR  - https://www.sciencedirect.com/science/article/pii/S0306261925000947
KW  - Cybersecurity
KW  - Five-level hierarchical DER architecture
KW  - Multiple DER applications
KW  - Vulnerabilities and attacks
KW  - Defense strategies
AB  - Distributed energy resources (DERs) are increasingly proliferating worldwide, driven by their benefits in promoting energy sustainability, efficiency, and resiliency. Coordinated approaches are crucial for aggregating diverse DERs across large areas, yet the increasing reliance on information technology exposes power systems to cyber attacks. What are the evolving cyber threats, vulnerabilities, and risks associated with integrating DERs in various applications? Moreover, how can a comprehensive defense-in-depth framework be developed to efficiently coordinate multiple stakeholders, ensuring DERs performance for power system operation against cyber attacks? To address these inquiries, this paper presents a comprehensive review of DER cybersecurity to assess its current status and identify research gaps. The review begins with an overview of DER systems and their cybersecurity risks based on the five-level hierarchical infrastructure established by the Electric Power Research Institute (EPRI). Subsequently, the study delves into current cybersecurity considerations from utilities and industries, examining requirements, guidelines/standards, and reference frameworks. The review further explores efforts in DER cyber risk analysis, mapping prominent vulnerabilities and attack schemes against different applications within the EPRI hierarchical architecture. The defense strategies proposed in the literature are also mapped, highlighting use cases for prevention, detection, and mitigation. Finally, analyzing research gaps and emerging technologies sheds light on critical DER cybersecurity issues and future research directions.
ER  - 

TY  - JOUR
T1  - Enhancing cybersecurity risk assessment using temporal knowledge graph-based explainable decision support system
AU  - Bag, Subhajit
AU  - Sarkar, Sobhan
AU  - Bose, Indranil
JO  - Decision Support Systems
VL  - 198
SP  - 114526
PY  - 2025
DA  - 2025/11/01/
SN  - 0167-9236
DO  - https://doi.org/10.1016/j.dss.2025.114526
UR  - https://www.sciencedirect.com/science/article/pii/S0167923625001277
KW  - Decision support
KW  - Cybersecurity policy assessment
KW  - Temporal knowledge graph
KW  - Attention mechanism
KW  - Interpretability
AB  - Assessing cybersecurity policies is crucial for any organization to combat evolving cyber threats. The absence of a comprehensive dataset has prevented previous studies from analyzing the risk of organizations’ cybersecurity policies. Past studies have not considered temporal information in the policies. Analysis of cybersecurity policies using attention mechanism requires automated determination of optimal number of attention units which remains unaddressed. Moreover, absence of interpretation in cybersecurity studies creates a barrier to understanding policy vulnerabilities and developing targeted solutions. To address these challenges, we develop a decision support system which (i) enhances risk classification of organization’s cybersecurity policies, (ii) develops a comprehensive cybersecurity policy dataset from the websites of 190 companies, transformed into a knowledge graph to capture entity relationships among various policies, (iii) integrates temporal information into the knowledge graph by incorporating time stamps from event sequences in cyberattack information, (iv) develops Explainable Factor Analysis based Multi-Head Attention mechanism, which automates the determination of the optimal number of attention units and optimizes data allocation across attention units using factor analysis, and (v) utilizes attention heatmaps and shapley values for interpretability. Our cybersecurity policy dataset is used as a case study with four benchmark datasets for further validation. Results reveal that our model outperforms the other state-of-the-art, achieving an 87.78% F1 score, followed by robustness checking and statistical significance testing. Finally, Shapley values are used to interpret the model’s output to identify vulnerabilities within the organizational policies, providing crucial insights enabling decision-makers to enhance their cybersecurity policies and mitigate potential threats.
ER  - 

TY  - JOUR
T1  - SIEVE: Generating a cybersecurity log dataset collection for SIEM event classification
AU  - Artioli, Pierpaolo
AU  - Dentamaro, Vincenzo
AU  - Galantucci, Stefano
AU  - Magrì, Alessio
AU  - Pellegrini, Gianluca
AU  - Semeraro, Gianfranco
JO  - Computer Networks
VL  - 266
SP  - 111330
PY  - 2025
DA  - 2025/07/01/
SN  - 1389-1286
DO  - https://doi.org/10.1016/j.comnet.2025.111330
UR  - https://www.sciencedirect.com/science/article/pii/S138912862500297X
KW  - SIEM
KW  - Cybersecurity
KW  - Dataset
KW  - Log classification
KW  - Text classification
KW  - Natural Language Processing (NLP)
AB  - Effective cyber threat monitoring relies on deploying robust Security Information and Event Management (SIEM) systems. SIEM applications receive security events generated by different devices, systems, and applications. They should properly correlate them to identify potential cyber threats based on tactics, techniques, and procedures (TTP), bypassing other security mechanisms (e.g., firewall, IDS, etc.). Given that logs are primarily generated to notify relevant system events and activities in a human-readable format, supervised Natural Language Processing (NLP) techniques could be used to train models that complement conventional parsing methodologies by automatically suggesting event classification into pre-defined categories. Training such models requires a substantial amount of pre-classified (labeled) data of different types to provide the learning patterns and nuances needed to make accurate predictions. Since the number of security event datasets is scarce due to privacy or availability reasons, and the few publicly available ones are often limited in terms of event diversity, number of labels, or simply unfit for the task at hand, an effective synthetic dataset for training SIEM-related machine learning event classification algorithms could be very useful. For these reasons, this paper proposes the generation of a synthetic dataset specifically designed to train SIEM systems for log-type classification. This research paper, starting from an in-depth methodological analysis of the prominent Cybersecurity related datasets available in the literature, introduces SIEVE (Siem Ingesting EVEnts), a synthetic dataset collection built from publicly available log samples using SPICE (Semantic Perturbation and Instantiation for Content Enrichment), a novel text augmentation and perturbation technique. SPICE is shown to be effective in generating realistic logs. Each instance of the dataset collection displays different levels of augmentation. Subsequent performance assessments were conducted through comprehensive benchmarking against various NLP classification models. Tests were conducted by training the classifiers using SIEVE and testing them on both the same SIEVE logs and real logs. The results of the experiments show that the best model among those tested is SVM (MaF1 0.9323 - 0.9737), which maintains its performance with slight degradation, even in tests on real logs (MaF1 0.9477 - 0.9636). BERT, on the other hand, performs better than SVM in most of the tests on SIEVE (MaF1 0.9528 - 0.9730) but does not show robustness when tested on real logs (MaF1 0.8864 - 0.9182).
ER  - 

TY  - JOUR
T1  - From one attack domain to another: Contrastive transfer learning with siamese networks for APT detection
AU  - Benabderrahmane, Sidahmed
AU  - Rahwan, Talal
JO  - Knowledge-Based Systems
VL  - 332
SP  - 114877
PY  - 2026
DA  - 2026/01/15/
SN  - 0950-7051
DO  - https://doi.org/10.1016/j.knosys.2025.114877
UR  - https://www.sciencedirect.com/science/article/pii/S095070512501915X
KW  - Anomaly detection
KW  - Deep learning
KW  - Transfer learning
KW  - Cyber-security
KW  - Advanced persistent threats
AB  - Advanced Persistent Threats (APT) pose a major cybersecurity challenge due to their stealth, persistence, and adaptability. Traditional machine learning detectors struggle with class imbalance, high-dimensional features, and scarce real-world traces. They often lack transferability-performing well in the training domain but degrading in novel attack scenarios. We propose a hybrid transfer framework that integrates Transfer Learning, Explainable AI (XAI), contrastive learning, and Siamese networks to improve cross-domain generalization. An attention-based autoencoder supports knowledge transfer across domains, while Shapley Additive exPlanations (SHAP) select stable, informative features to reduce dimensionality and computational cost. A Siamese encoder trained with a contrastive objective aligns source and target representations, increasing anomaly separability and mitigating feature drift. We evaluate on real-world traces from the DARPA Transparent Computing (TC) program and augment with synthetic attack scenarios to test robustness. Across source to target transfers, the approach delivers improved detection scores with classical and deep baselines, demonstrating a scalable, explainable, and transferable solution for APT detection.
ER  - 

TY  - JOUR
T1  - IIoT-enabled digital twin for legacy and smart factory machines with LLM integration
AU  - Gautam, Anuj
AU  - Aryal, Manish Raj
AU  - Deshpande, Sourabh
AU  - Padalkar, Shailesh
AU  - Nikolaenko, Mikhail
AU  - Tang, Ming
AU  - Anand, Sam
JO  - Journal of Manufacturing Systems
VL  - 80
SP  - 511
EP  - 523
PY  - 2025
DA  - 2025/06/01/
SN  - 0278-6125
DO  - https://doi.org/10.1016/j.jmsy.2025.03.022
UR  - https://www.sciencedirect.com/science/article/pii/S0278612525000834
KW  - Large language model
KW  - Industrial Internet of Things
KW  - Digital twin
KW  - Multi-agent framework
KW  - Retrieval-augmented generation
AB  - Recent advancements in Large Language Models (LLMs) have significantly transformed the field of natural data interpretation, translation, and user training. However, a notable gap exists when LLMs are tasked to assist with real-time context-sensitive machine data. The paper presents a multi-agent LLM framework capable of accessing and interpreting real-time and historical data through an Industrial Internet of Things (IIoT) platform for evidence-based inferences. Real-time data is acquired from several legacy machine artifacts (such as seven-segment displays, toggle switches, and knobs), smart machines (such as 3D printers), and building data (such as sound sensors and temperature measurement devices) through MTConnect data streaming protocol. Further, a multi-agent LLM framework that consists of four specialized agents – a supervisor agent, a machine-expertise agent, a data visualization agent, and a fault-diagnostic agent is developed for context-specific manufacturing tasks. This LLM framework is then integrated into a digital twin to visualize the unstructured data in real time. The paper also explores how LLM-based digital twins can serve as real time virtual experts through an avatar, minimizing reliance on traditional manuals or supervisor-based expertise. To demonstrate the functionality and effectiveness of this framework, we present a case study consisting of legacy machine artifacts and modern machines. The results highlight the practical application of LLM to assist and infer real-time machine data in a digital twin environment. Peer-review under responsibility of the scientific committee of the NAMRI/SME.
ER  - 

TY  - JOUR
T1  - Evolving cybersecurity of AI-featured digital products and services: Rise of standardisation and certification?
AU  - Rampášek, Michal
AU  - Mesarčík, Matúš
AU  - Andraško, Jozef
JO  - Computer Law & Security Review
VL  - 56
SP  - 106093
PY  - 2025
DA  - 2025/04/01/
SN  - 2212-473X
DO  - https://doi.org/10.1016/j.clsr.2024.106093
UR  - https://www.sciencedirect.com/science/article/pii/S0267364924001584
KW  - Artificial intelligence
KW  - Foundation models
KW  - Cybersecurity law
KW  - Cybersecurity Act
KW  - Cyber Resilience Act
KW  - AI Act
KW  - NIS2
KW  - Standardisation
KW  - Certification
KW  - Future of cybersecurity
AB  - The field of cybersecurity has changed dramatically since the Cybersecurity Strategy for the Digital Decade was presented by the European Commission and the High Representative of the Union for Foreign Affairs and Security Policy in December 2020. The Cybersecurity Strategy highlights the potential of AI as a new technology, but also the need for cyber security of AI technology. Indeed, since the strategy was adopted, AI has shown that it has enormous potential for growth, but also several risks and vulnerabilities that this new technology brings. The paper analyses the shift and further development in the field of cybersecurity of digital products and services, AI itself as a technology, as well as products and services that will contain an AI component. In our opinion, the way to ensure that not only AI technology itself, but also products and services are cyber-secure, is to achieve a high level of standardisation of best practices, as there are many gaps in this area. The adoption of technical standards will fully form a path for conformity assessment and certification of not only AI systems but also AI-featured digital products and services. However, the current regulatory trend is to adopt a comprehensive legal regulation of AI even before such technical standards are fully developed and adopted. We consider this risky. Despite the well-intentioned effort to define and regulate AI, the purpose set forth in the AIA may not be achieved, as the requirements adopted in this way can very quickly become unnecessarily burdensome or even outdated due to increasing technological development. The proof of this is also the recent rise of large ML models, known as foundation models, which significantly changed the previous understanding of the creation of AI systems. It will be the technological development of AI, AI specific standardisation, and subsequent certification of digital products and services, which will govern future activities in building Europe's cyber resilience.
ER  - 

TY  - JOUR
T1  - Enhancing cybersecurity in Edge IIoT networks: An asynchronous federated learning approach with a deep hybrid detection model
AU  - Muhammad Salman Bukhari, Syed
AU  - Zafar, Muhammad Hamza
AU  - Houran, Mohamad Abou
AU  - Qadir, Zakria
AU  - Kumayl Raza Moosavi, Syed
AU  - Sanfilippo, Filippo
JO  - Internet of Things
VL  - 27
SP  - 101252
PY  - 2024
DA  - 2024/10/01/
SN  - 2542-6605
DO  - https://doi.org/10.1016/j.iot.2024.101252
UR  - https://www.sciencedirect.com/science/article/pii/S2542660524001938
KW  - Cybersecurity
KW  - Federated learning
KW  - Industrial Internet of Things (IIoT)
KW  - Network intrusion detection
KW  - Data privacy
KW  - Convolutional Neural Network (CNN)
KW  - Gated Recurrent Unit (GRU)
KW  - Long Short-Term Memory (LSTM) networks
KW  - Asynchronous learning
AB  - In the rapidly evolving field of the Industrial Internet of Things (IIoT), advancements in wireless technology have resulted in significant cybersecurity vulnerabilities. These weaknesses pose serious risks such as damage to manufacturing systems, theft of intellectual property, and substantial financial losses. This study introduces an advanced deep hybrid learning model in an asynchronous federated learning setup, aimed at improving the detection of cyberattacks and ensuring robust data privacy. The combination of Convolutional Neural Networks (CNN), Gated Recurrent Units (GRU), and Long Short-Term Memory (LSTM) networks provides an effective solution for quickly identifying anomalies in IIoT sensor traffic. Our model operates asynchronously, ensuring data remains localised to improve security while avoiding the need for complete node synchronisation. Demonstrating outstanding effectiveness, the model achieves an accuracy of 1.00%, precision of 1.00%, recall of 1.00%, and an F1 score of 1.00% across a variety of IIoT environments. These results highlight the model’s exceptional adaptability and its capability to rapidly respond to emergent threats, marking a significant step forward in the protection of IIoT infrastructures and the rigorous maintenance of data privacy.
ER  - 

TY  - JOUR
T1  - FLADEN: Federated Learning for Anomaly DEtection in IoT Networks
AU  - Hendaoui, Fatma
AU  - Meddeb, Rahma
AU  - Trabelsi, Lamia
AU  - Ferchichi, Ahlem
AU  - Ahmed, Rawia
JO  - Computers & Security
VL  - 155
SP  - 104446
PY  - 2025
DA  - 2025/08/01/
SN  - 0167-4048
DO  - https://doi.org/10.1016/j.cose.2025.104446
UR  - https://www.sciencedirect.com/science/article/pii/S016740482500135X
KW  - Federated learning
KW  - Intrusion detection
KW  - Machine learning
KW  - Accuracy
KW  - Precision
KW  - IoT
AB  - Sensitive applications are strict in terms of data privacy. In this context, intrusion detection systems cannot access the data and analyze it to discover attacks signatures. As a result, it is necessary to analyze data locally without disclosing it to a third party. Machine learning models can achieve this task. This paper proposes a machine-learning framework for intrusion detection on IoT networks. The proposed framework enables participating entities to analyze their data more efficiently and privately. A new real-world dataset is generated using online threat intelligence sources. FLADEN updates the federated learning library to optimize processing time with an accuracy of 99.85%. The proposed framework was applied to machine learning models and shows a precision of 99. 89%, an F1 score of 99. 93%, and a recall of 99.91%. This work presents implications for those researchers who may focus on large-scale anomaly detection with privacy preservation in IoT networks.
ER  - 

TY  - JOUR
T1  - Bibliometric analysis of maritime cybersecurity: Research status, focus, and perspectives
AU  - Peng, Peng
AU  - Xie, Xiaowei
AU  - Claramunt, Christophe
AU  - Lu, Feng
AU  - Gong, Fuzhong
AU  - Yan, Ran
JO  - Transportation Research Part E: Logistics and Transportation Review
VL  - 195
SP  - 103971
PY  - 2025
DA  - 2025/03/01/
SN  - 1366-5545
DO  - https://doi.org/10.1016/j.tre.2025.103971
UR  - https://www.sciencedirect.com/science/article/pii/S1366554525000122
KW  - Maritime cybersecurity
KW  - Maritime digitalization
KW  - Bibliometric analysis
KW  - Research keyword communities
KW  - Cyber-attacks
KW  - Autonomous vessel
AB  - Maritime cybersecurity has emerged as a critical and rapidly evolving research field, necessitated by the increasing reliance on digital technologies and interconnectivity within the global maritime industry. In this paper, we adopt a bibliometric analysis method to review the existing academic publications pertaining to maritime cybersecurity, aiming to provide a comprehensive overview of the development status and research focus. The results show that: 1) Research on maritime cybersecurity is currently undergoing significant development; 2) Most articles on marine cybersecurity are published by researchers from North America and Europe, with most of them stem from the US, Norway, and UK; 3) Most international collaborations are limited at a regional level, and the major regions include North America and Europe; 4) Five closely related research keyword communities show that maritime cybersecurity research hotspots focus on transport-related cyber-attacks, autonomous vessel, AIS, maritime communication, and UAV. The above thorough examination of the current research on maritime cybersecurity also shows that there are some weaknesses in existing studies. For example, the research topic of maritime cybersecurity has not yet received adequate attention and the research hotspot is relatively concentrated. Based on the findings, we propose perspectives of the research on maritime cybersecurity from the aspects of the effectiveness of regulations, funding and investment opportunities, digitalization in the maritime industry and cybersecurity, advances in maritime communication systems, and unmanned aerial vehicles and maritime cybersecurity.
ER  - 

TY  - JOUR
T1  - Integrating system calls and position-specific scoring for enhanced anomaly detection in Internet of Things environments
AU  - Shamim, Nouman
AU  - Asim, Muhammad
AU  - Baker, Thar
AU  - Pervez, Zeeshan
AU  - Awad, Ali Ismail
AU  - Zomaya, Albert Y.
JO  - Computers & Security
VL  - 158
SP  - 104613
PY  - 2025
DA  - 2025/11/01/
SN  - 0167-4048
DO  - https://doi.org/10.1016/j.cose.2025.104613
UR  - https://www.sciencedirect.com/science/article/pii/S0167404825003025
KW  - Internet of Things (IoT)
KW  - IoT security
KW  - Attack detection
KW  - Anomaly detection
KW  - System calls analysis
KW  - Position-specific scoring
AB  - Identifying attacks on Internet of Things (IoT) systems through anomaly detection is an effective approach and remains a crucial area of research. The core method involves collecting system-related data during normal operation to establish a baseline of typical behavior and then continuously monitoring for deviations from this baseline. Using system call sequences for anomaly detection is a well-established and important field. System call sequences effectively capture the behavior of a target system at a low level, allowing identification of any changes in this behavior; however, these approaches face several challenges, including high false-positive rates, the need for segmentation of long sequences, and the difficulty of detecting anomalies when the system call data comes from multiple processes. This work presents a novel anomaly-detection approach that uses a position-specific scoring mechanism to analyze the content and structural properties of system call sequences. The proposed approach addresses key challenges in this field, including fixed-length segmentation of system call sequences, predetermined anomaly-detection thresholds, the detection of anomalies in both single and multiple processes, and high false-positive rates. We extensively evaluated the proposed approach using system-call-specific public datasets (ADFA-LD and UNM) of a diverse nature. The performance of the proposed content-based, structure-based, and combined content- and structure-based anomaly-detection methods was evaluated using ten-fold cross-validation. The proposed anomaly-detection approach achieves an impressive detection rate of 1.0, along with exceptionally low false-positive rates of 0.001 and 0.017 when evaluated on the UNM and ADFA-LD datasets, respectively.
ER  - 

TY  - JOUR
T1  - TIMFuser: A multi-granular fusion framework for cyber threat intelligence
AU  - Ma, Chunyan
AU  - Jiang, Zhengwei
AU  - Zhang, Kai
AU  - Ling, Zhiting
AU  - Jiang, Jun
AU  - You, Yizhe
AU  - Yang, Peian
AU  - Feng, Huamin
JO  - Computers & Security
VL  - 148
SP  - 104141
PY  - 2025
DA  - 2025/01/01/
SN  - 0167-4048
DO  - https://doi.org/10.1016/j.cose.2024.104141
UR  - https://www.sciencedirect.com/science/article/pii/S0167404824004462
KW  - Cyber threat intelligence
KW  - TTP
KW  - Multi-granular fusion
KW  - Attack behavior extraction
KW  - Attack technique recognition
AB  - Cyber attack campaigns with multiple technical variants are becoming increasingly sophisticated and diverse, posing great threats to institutions and every individual. Cyber Threat Intelligence (CTI) offers a novel technical solution to transition from passive to active defense against cyber attacks. To counter these attacks, security practitioners need to condense CTIs from extensive CTI sources, primarily in the form of unstructured CTI reports. Unstructured CTI reports provide detailed threat information and describe multi-step attack behaviors, which are essential for uncovering complete attack scenarios. Nevertheless, automatic analysis of unstructured CTI reports is challenging. Furthermore, manual analysis is often limited to a few CTI sources. In this paper, we propose a multi-granular fusion framework for CTIs from massive CTI sources, comprising a comprehensive pipeline with six subtasks. Many current CTI extraction systems are limited by mining intelligence from a single source, thereby leading to challenges such as producing a fragmented view of attack campaigns and lower value density. We fuse the attack behaviors and attack techniques of the attack campaigns using innovative and improved multi-granular fusion methods and offer a comprehensive view of the attack. TIMFuser fills a critical gap in the automated analysis and fusion of multi-source CTIs, especially in the multi-granularity aspect. In our evaluation of 739 real-world CTI reports from 542 sources, experimental results demonstrate that TIMFuser can enable security analysts to obtain a complete view of real-world attack campaigns, in terms of fused attack behaviors and attack techniques.
ER  - 

TY  - JOUR
T1  - Blockchain-Enabled Supply Chain Resilience: Leveraging Multimodal Large Language Models for Intelligent Automation in the Era of Global Disruptions and Stablecoin Integration
AU  - Huang, Jun
AU  - Lin, Fei
AU  - Zhang, Tengchao
AU  - Ni, Qinghua
AU  - Tian, Yonglin
AU  - Wu, Naiqi
JO  - IFAC-PapersOnLine
VL  - 59
IS  - 34
SP  - 48
EP  - 53
PY  - 2025
DA  - 2025/01/01/
T2  - 1st IFAC Workshop on Blockchain Intelligence and Knowledge Automation BIKA 2025
SN  - 2405-8963
DO  - https://doi.org/10.1016/j.ifacol.2025.12.438
UR  - https://www.sciencedirect.com/science/article/pii/S2405896325031556
KW  - Blockchain
KW  - Supply Chain Management
KW  - Multimodal Large Language Models
KW  - Stablecoins
KW  - Global Disruptions
KW  - Knowledge Automation
AB  - In an era marked by escalating global supply chain disruptions—exacerbated by geopolitical tensions, climate events, and economic volatility—the integration of blockchain technology with multimodal large language models (MLLMs) emerges as a pivotal strategy for enhancing resilience and intelligent automation. This paper explores the synergies between blockchain’s decentralized, immutable ledger capabilities and MLLMs’ proficiency in processing diverse data modalities, such as textual contracts, visual sensor feeds, and auditory logistics signals, to automate knowledge extraction and decision-making in supply chain management (SCM). By addressing hotspots like the fragility of global trade networks and the role of stablecoins in facilitating secure, cross-border transactions, we analyze how this hybrid framework mitigates risks, ensures traceability, and optimizes efficiency. For instance, blockchain provides tamper-proof data foundations for provenance tracking, while MLLMs enable predictive analytics for disruptions, such as those seen in recent Suez Canal blockages or pandemic-induced shortages. Furthermore, stablecoins integrate seamlessly into smart contracts to reduce financial volatility and enable real-time settlements, fostering trust in fragmented ecosystems. Drawing on recent advancements in AI-driven SCM trends, we propose a conceptual architecture that aligns with blockchain intelligence paradigms, demonstrating its importance for knowledge automation in volatile markets. Our analysis underscores the transformative potential of this integration, offering insights for policymakers and practitioners amid projections of intensified supply chain chaos by 2025. Contributions include a novel framework for MLLM-empowered blockchain systems, with implications for sustainable and resilient global SCM.
ER  - 

TY  - JOUR
T1  - A multilayered deep learning framework for cyber attack detection and mitigation in a heterogeneous IIoT ecosystem
AU  - Iqbal, Arshad
AU  - Asghar, Sohail
AU  - Tamimy, Manzoor Ilahi
JO  - Journal of Information Security and Applications
VL  - 96
SP  - 104301
PY  - 2026
DA  - 2026/01/01/
SN  - 2214-2126
DO  - https://doi.org/10.1016/j.jisa.2025.104301
UR  - https://www.sciencedirect.com/science/article/pii/S2214212625003382
KW  - Cybersecurity
KW  - Cyber deception
KW  - Edge computing
KW  - Deep learning
KW  - Intrusion detection system (IDS)
KW  - Industrial internet of things (IIoT)
KW  - Random forest
KW  - Multilayer perceptron (MLP)
AB  - Intrusion Detection Systems (IDSs) for the Internet of Things (IoT) and Industrial IoT (IIoT) face significant challenges, including high false-positive rates (especially for minority-class attacks) and excessive computational requirements, which hinder their deployment on edge devices. Consequently, alert overload is common because operators receive a large volume of alerts that provide little insight into the problems they address. To address this crucial gap, this study presents DeepGuard, a new four-layer framework that significantly improves the security posture of IoT and industrial IoT environments. DeepGuard combines binary and multiclass classifications, intelligent alarming, and cyber deception into a single, effective defence mechanism. The system incorporates a random forest classifier for feature selection, which extracts the most relevant data features and processes them for use with an optimised multilayer perceptron (MLP). This method achieved an unprecedented accuracy of 99.9% with a low false-positive rate (FPR) of 0.2%, surpassing the state-of-the-art research studies. We further demonstrated the practical feasibility of DeepGuard by implementing it on computationally constrained, edge devices. With a computational complexity of O(nlogn) and a memory footprint of less than 100 KB, DeepGuard breaks the long-standing trade-off between detection accuracy and operational performance that has inhibited the adoption of IDS at an industrial scale. In addition to a detection-only approach, DeepGuard includes an embedded honeypot layer that proactively profiles emerging and unknown attacks, thereby enabling automated mitigation responses. Thorough evaluations of the WUSTL-IIoT-2021 and X-IIoTID-2022 datasets demonstrated a new state-of-the-art performance and the feasibility of DeepGuard for protecting critical infrastructure.
ER  - 

TY  - JOUR
T1  - Modelling cybersecurity environment using Lotka-Volterra equations, and its stochastic analysis
AU  - Atıcı, Sinan
AU  - Tuna, Gurkan
JO  - Expert Systems with Applications
VL  - 296
SP  - 129096
PY  - 2026
DA  - 2026/01/15/
SN  - 0957-4174
DO  - https://doi.org/10.1016/j.eswa.2025.129096
UR  - https://www.sciencedirect.com/science/article/pii/S0957417425027137
KW  - Cyber-ecosystem
KW  - Lotka-Volterra
KW  - Stochastic model
KW  - Lyapunov analysis
KW  - Stochastic differential equation
KW  - Agent-based model
KW  - Monte Carlo simulation
AB  - Conventional cybersecurity models often analyse vulnerabilities, cyberattacks, and security measures in isolation, neglecting their dynamic interdependencies and feedback mechanisms. Such approaches fail to capture emergent behaviours, struggle to model evolving threats in complex environments, and lack a holistic perspective, limiting their practical applicability. To address these shortcomings, this study introduces an ecological modelling framework using extended Lotka-Volterra (LV) equations, conceptualising vulnerabilities, attacks, and security hardening efforts as a three-component cyber-ecosystem. The approach integrates deterministic modelling of core interactions—such as growth, mitigation, and feedback mechanisms—with stochastic methods, including Stochastic Differential Equation (SDE), Agent-Based Modelling (ABM), and Monte Carlo Simulation (MCS), to address real-world uncertainties. Lyapunov stability analysis is employed to evaluate system resilience under perturbations. The framework extends classical predator–prey dynamics by incorporating security hardening as a regulatory mechanism interacting with both vulnerabilities and attacks. Simulations examine stability regimes across parameter configurations, while phase-space trajectories are analysed to identify behavioural patterns influenced by stochastic factors. Analytical methods assess critical thresholds in vulnerability growth rates and security deployment efficiency that govern equilibrium conditions. By adapting ecological principles to cybersecurity, this work highlights the challenges of managing interdependent system components. The analysis explores how coordinated mitigation strategies influence stability, with integrated deterministic-stochastic modelling offering a systems perspective on resilience. The model’s practical utility is exemplified through a hypothetical Smart Grid SCADA scenario, showcasing its ability to guide resource optimisation based on vulnerability and attack dynamics. This approach contributes a methodological framework for analysing cybersecurity ecosystems through quantitative stability metrics. It facilitates the evaluation of adaptive security architectures and resource allocation strategies, providing a robust bridge between theoretical modelling and practical cybersecurity challenges.
ER  - 

TY  - JOUR
T1  - A threat modeling language for generating attack graphs of substation automation systems
AU  - Rencelj Ling, Engla
AU  - Ekstedt, Mathias
JO  - International Journal of Critical Infrastructure Protection
VL  - 41
SP  - 100601
PY  - 2023
DA  - 2023/07/01/
SN  - 1874-5482
DO  - https://doi.org/10.1016/j.ijcip.2023.100601
UR  - https://www.sciencedirect.com/science/article/pii/S1874548223000148
KW  - Cyber security
KW  - Vulnerability analysis
KW  - Threat modeling language
KW  - Attack graph
KW  - Substation automation systems
AB  - The substation automation system consists of many different complex assets and data flows. The system is also often externally connected to allow for remote management. The complexity and remote access to the substation automation system makes it vulnerable to cyber attacks. It also makes it difficult to assess the overall security of the system. One method of assessing the potential threats against a system is threat modeling. In this paper we create a language for producing threat models specifically for the substation automation systems. We focus on the method used to create the language where we review industry designs, build the language based on existing languages and consider attack scenarios from a literature study. Finally we present the language, model two different attack scenarios and generate attack graphs from the threat models.
ER  - 

TY  - JOUR
T1  - Application of a digital intelligent assistant to support industrial processes: the case of adaptive allocation in the face of cyber attacks
AU  - Colabianchi, Silvia
JO  - Procedia Computer Science
VL  - 253
SP  - 1555
EP  - 1564
PY  - 2025
DA  - 2025/01/01/
T2  - 6th International Conference on Industry 4.0 and Smart Manufacturing
SN  - 1877-0509
DO  - https://doi.org/10.1016/j.procs.2025.01.217
UR  - https://www.sciencedirect.com/science/article/pii/S187705092500225X
KW  - chatbot
KW  - adaptive automation
KW  - industry 4.0
KW  - cybersecurity
AB  - The research proposes the application of Digital Intelligent Assistants (DIAs) as proactive agents that can support employees in dealing with cybersecurity issues in sustainable industrial processes underlying the importance of a fruitful Human-Artificial Intelligence collaboration. Cyber-attacks around the world are constantly increasing. DIAs are becoming more effective, also thanks to the use of Large Language Models. Users are required to recall security procedures and rules. Moreover, attacks are constantly evolving and following different patterns. The study presents how a DIA can be a backup agent during and after an attack. The application of digital intelligent assistance technology helps to reduce the cognitive load and pressure that users feel during downtime. In addition, the solution enhances attack reporting by decreasing the shame experienced by the victims. The research proposes a methodological design defining the agent’s technical and functional characteristics and its adaptive relationship with human characteristics. The solution is developed using the RASA framework and evaluated through a case study based on a phishing attack scenario.
ER  - 

TY  - JOUR
T1  - Enhancing medical digital twins within metaverse using blockchain, NFTs and LLMs
AU  - Islayem, Ruba
AU  - Musamih, Ahmad
AU  - Salah, Khaled
AU  - Jayaraman, Raja
AU  - Yaqoob, Ibrar
JO  - Internet of Things
VL  - 32
SP  - 101648
PY  - 2025
DA  - 2025/07/01/
SN  - 2542-6605
DO  - https://doi.org/10.1016/j.iot.2025.101648
UR  - https://www.sciencedirect.com/science/article/pii/S2542660525001623
KW  - Blockchain
KW  - Digital twins
KW  - Metaverse
KW  - NFTs
KW  - LLMs
AB  - Medical digital twins (MDTs) are rapidly emerging as transformative tools in healthcare. They provide virtual representations of medical devices and systems that facilitate real-time analysis and enhance decision-making. However, challenges such as secure data management, access control, and the lack of immersive and intelligent patient interactions limit their effectiveness. In this paper, we propose a solution integrating blockchain technology, Non-Fungible Tokens (NFTs), and Large Language Models (LLMs) within a metaverse environment to enhance MDT functionality. Blockchain and NFTs ensure secure ownership and access control, while the metaverse offers an engaging platform for user interaction. An LLM-powered non-player character (NPC) enables intelligent real-time user interactions and personalized insights. We develop two blockchain smart contracts for user registration, NFT ownership, and access control, and utilize decentralized InterPlanetary File System (IPFS) storage for the metaverse, MDT metadata, and interaction logs. We present the system architecture, sequence diagrams, and algorithms, along with the implementation and testing details. We conduct cost, security, and response time analyses to evaluate the smart contracts and LLM performance and compare our solution with existing approaches. We discuss practical implications, as well as challenges and limitations of the proposed solution. Finally, we explore the generalization of our system for various applications. The smart contract code and metaverse files are publicly available on GitHub.
ER  - 

TY  - JOUR
T1  - Tensor networks for explainable machine learning in cybersecurity
AU  - Aizpurua, Borja
AU  - Palmer, Samuel
AU  - Orús, Román
JO  - Neurocomputing
VL  - 639
SP  - 130211
PY  - 2025
DA  - 2025/07/28/
SN  - 0925-2312
DO  - https://doi.org/10.1016/j.neucom.2025.130211
UR  - https://www.sciencedirect.com/science/article/pii/S0925231225008835
KW  - Tensor networks
KW  - Explainable AI
KW  - Cybersecurity
KW  - Anomaly detection
KW  - Matrix Product States (MPS)
KW  - Adversary-Generated Threat Intelligence
AB  - In this paper we show how tensor networks help in developing explainability of machine learning algorithms. Specifically, we develop an unsupervised clustering algorithm based on Matrix Product States (MPS) and apply it in the context of a real use-case of adversary-generated threat intelligence. Our investigation proves that MPS rival traditional deep learning models such as autoencoders and GANs in terms of performance, while providing much richer model interpretability. Our approach naturally facilitates the extraction of feature-wise probabilities, Von Neumann Entropy, and mutual information, offering a compelling narrative for classification of anomalies and fostering an unprecedented level of transparency and interpretability, something fundamental to understand the rationale behind artificial intelligence decisions.
ER  - 

TY  - JOUR
T1  - A robust cross-domain IDS using BiGRU-LSTM-attention for medical and industrial IoT security
AU  - Gueriani, Afrah
AU  - Kheddar, Hamza
AU  - Mazari, Ahmed Cherif
AU  - Ghanem, Mohamed Chahine
JO  - ICT Express
PY  - 2025
DA  - 2025/09/06/
SN  - 2405-9595
DO  - https://doi.org/10.1016/j.icte.2025.08.011
UR  - https://www.sciencedirect.com/science/article/pii/S2405959525001298
KW  - Transformers
KW  - Intrusion detection
KW  - Anomalies detection
KW  - Cyber-security
KW  - Deep learning
KW  - Internet of Medical Things
KW  - Industrial Internet of Things
AB  - The increased Internet of Medical Things (IoMT) and the Industrial Internet of Things (IIoT) interconnectivity has introduced complex cybersecurity challenges, exposing sensitive data, patient safety, and industrial operations to advanced cyber threats. To mitigate these risks, this paper introduces a novel transformer-based intrusion detection system (IDS), termed BiGAT-ID—a hybrid model that combines bidirectional gated recurrent units (BiGRU), long short-term memory (LSTM) networks, and multi-head attention (MHA). The proposed architecture is designed to effectively capture bidirectional temporal dependencies, model sequential patterns, and enhance contextual feature representation. Extensive experiments on two benchmark datasets; CICIoMT2024 (medical IoT) and EdgeIIoTset (industrial IoT); demonstrate the model’s cross-domain robustness, achieving detection accuracies of 99.13% and 99.34%, respectively. Additionally, the model exhibits exceptional runtime efficiency, with inference times as low as 0.0002 seconds per instance in IoMT and 0.0001 seconds in IIoT scenarios. Coupled with a low false positive rate, BiGAT-ID proves to be a reliable and efficient IDS for deployment in real-world heterogeneous IoT environments.
ER  - 

TY  - JOUR
T1  - Unleashing offensive artificial intelligence: Automated attack technique code generation
AU  - Iturbe, Eider
AU  - Llorente-Vazquez, Oscar
AU  - Rego, Angel
AU  - Rios, Erkuden
AU  - Toledo, Nerea
JO  - Computers & Security
VL  - 147
SP  - 104077
PY  - 2024
DA  - 2024/12/01/
SN  - 0167-4048
DO  - https://doi.org/10.1016/j.cose.2024.104077
UR  - https://www.sciencedirect.com/science/article/pii/S0167404824003821
KW  - Offensive AI
KW  - Cybersecurity
KW  - Artificial intelligence
KW  - Large Language Model
KW  - Risk assessment
AB  - Artificial Intelligence (AI) technology is revolutionizing the digital world and becoming the cornerstone of the modern digital systems. The capabilities of cybercriminals are expanding as they adopt new technologies like zero-day exploits or new business models such as hacker-as-a-service. While AI capabilities can improve cybersecurity measures, this same technology can also be utilized as an offensive cyber weapon to create sophisticated and intricate cyber-attacks. This paper describes an AI-powered mechanism for the automatic generation of attack techniques, ranging from initial attack vectors to impact-related actions. It presents a comprehensive analysis of simulated attacks by highlighting the attack tactics and techniques that are more likely to be generated using AI technology, specifically Large Language Model (LLM) technology. The work empirically demonstrates that LLM technology can be easily used by cybercriminals for attack execution. Moreover, the solution can complement Breach and Attack Simulation (BAS) platforms and frameworks that automate the security assessment in a controlled manner. BAS could be enhanced with AI-powered attack simulation by bringing forth new ways to automatically program multiple attack techniques, even multiple versions of the same attack technique. Therefore, AI-enhanced attack simulation can assist in ensuring digital systems are bulletproof and protected against a great variety of attack vectors and actions.
ER  - 

TY  - JOUR
T1  - Textual adversarial attacks in cybersecurity named entity recognition
AU  - Jiang, Tian
AU  - Liu, Yunqi
AU  - Cui, Xiaohui
JO  - Computers & Security
VL  - 150
SP  - 104278
PY  - 2025
DA  - 2025/03/01/
SN  - 0167-4048
DO  - https://doi.org/10.1016/j.cose.2024.104278
UR  - https://www.sciencedirect.com/science/article/pii/S0167404824005844
KW  - Cyber Threat Intelligence
KW  - Named Entity Recognition
KW  - Fine-tuned models
KW  - Adversarial examples
KW  - Word substitution
KW  - Adversarial detection
AB  - In the cybersecurity domain, Cyber Threat Intelligence (CTI) includes procedures that lead to textual reports and different types of pieces of information and evidence on cyber threats. To better understand the behaviors of attackers and construct attack graphs, identifying attack-relevant entities in diverse CTI texts precisely and efficiently becomes more important, and Named Entity Recognition (NER) models can help extract entities automatically. However, such fine-tuned models are usually vulnerable to adversarial attacks. In this paper, we first construct an attack framework that can explore textual adversarial attacks in the cybersecurity NER task by generating adversarial CTI texts. Then, we analyze the most important parts of speech (POSs) from the perspective of grammar, and propose a word-substitution-based attack method. To confront adversarial attacks, we also introduce a method to detect potential adversarial examples. Experimental results show that cybersecurity NER models are also vulnerable to adversarial attacks. Among all attack methods, our method can generate adversarial texts that keep a balanced performance in several aspects. Furthermore, adversarial examples generated by all attack methods perform well in the study of transferability, and they can help improve the robustness of NER models through adversarial training. On the defense side, our detection method is simple but effective against multiple types of textual adversarial attacks.
ER  - 

TY  - JOUR
T1  - Evolving Threats, Emerging Laws: Poland's 2023 Answer to the Smishing Challenge
AU  - Zieliński, Sebastian
JO  - Computer Law & Security Review
VL  - 54
SP  - 106013
PY  - 2024
DA  - 2024/09/01/
SN  - 2212-473X
DO  - https://doi.org/10.1016/j.clsr.2024.106013
UR  - https://www.sciencedirect.com/science/article/pii/S0267364924000803
KW  - Cybersecurity
KW  - Smishing
KW  - Phishing
KW  - Electronic communication
KW  - Abuses in electronic communication
AB  - In the face of rising cybersecurity threats like 'smishing'—SMS-based phishing attacks—this article examines how legislative efforts can effectively address these challenges. This article provides a comprehensive analysis of cybersecurity challenges, focusing on the still growing phenomenon of 'smishing', within the legislative context. In particular, it explores the legal landscape of cybercrime through the lens of Poland's recently enacted Act on Combating Abuses in Electronic Communication, as well as the European Union's Cybersecurity Strategy for the Digital Decade. The first one serves as a significant case study for examining legislative efforts aimed at mitigating cybersecurity risks in the field of electronic communications. The article describes the multi-layered, collaborative business-state approach of the Polish law, which can provide a solid framework for addressing current and future cyber security threats. The act stands as a promising tool for fortifying national cybersecurity infrastructure and could serve as a useful example for other jurisdictions grappling with similar issues. The law also engages citizens actively in its cybersecurity initiatives, promoting collective responsibility. In the broader European Union context, while the Polish Act undergoes scrutiny, this analysis also seeks to explore its alignment with the objectives outlined in the 2020′s European Union's Cybersecurity Strategy for the Digital Decade. This examination aims to evaluate the extent to which the Polish legislative framework resonates with the overarching goals set forth by the European Union, thereby contributing to a deeper understanding of the synergy between national initiatives and the broader European cybersecurity strategy context.
ER  - 

TY  - JOUR
T1  - Cybersecurity for tactical 6G networks: Threats, architecture, and intelligence
AU  - Suomalainen, Jani
AU  - Ahmad, Ijaz
AU  - Shajan, Annette
AU  - Savunen, Tapio
JO  - Future Generation Computer Systems
VL  - 162
SP  - 107500
PY  - 2025
DA  - 2025/01/01/
SN  - 0167-739X
DO  - https://doi.org/10.1016/j.future.2024.107500
UR  - https://www.sciencedirect.com/science/article/pii/S0167739X24004643
KW  - Mission-critical communications
KW  - Tactical network
KW  - Public safety
KW  - Edge intelligence
KW  - 6G
KW  - Threats
KW  - Risk analysis
KW  - Security solutions
KW  - Survey
KW  - Security
KW  - Cybersecurity
AB  - Edge intelligence, network autonomy, broadband satellite connectivity, and other concepts for private 6G networks are enabling new applications for public safety authorities, e.g., for police and rescue personnel. Enriched situational awareness, group communications with high-quality video, large scale IoT, and remote control of vehicles and robots will become available in any location and situation. We analyze cybersecurity in intelligent tactical bubbles, i.e., in autonomous rapidly deployable mobile networks for public safety operations. Machine learning plays major roles in enabling these networks to be rapidly orchestrated for different operations and in securing these networks from emerging threats, but also in enlarging the threat landscape. We explore applicability of different threat and risk analysis methods for mission-critical networked applications. We present the results of a joint risk prioritization study. We survey security solutions and propose a security architecture, which is founded on the current standardization activities for terrestrial and non-terrestrial 6G and leverages the concepts of machine learning-based security to protect mission-critical assets at the edge of the network.
ER  - 

TY  - JOUR
T1  - A novel LLM-based classifier for predicting bug-fixing time in Bug Tracking Systems
AU  - Ardimento, Pasquale
AU  - Capuzzimati, Michele
AU  - Casalino, Gabriella
AU  - Schicchi, Daniele
AU  - Taibi, Davide
JO  - Journal of Systems and Software
VL  - 230
SP  - 112569
PY  - 2025
DA  - 2025/12/01/
SN  - 0164-1212
DO  - https://doi.org/10.1016/j.jss.2025.112569
UR  - https://www.sciencedirect.com/science/article/pii/S0164121225002389
KW  - Bug fixing time
KW  - Software maintenance and evolution
KW  - Large language models
KW  - Zero-shot learning
AB  - Predicting whether a newly submitted bug will be resolved quickly or slowly is a crucial aspect of the bug triage process, as it enables project managers to estimate software maintenance efforts and manage development workflows more effectively. This paper proposes a deep learning approach for classifying bug reports into two categories—FAST or SLOW—based on their expected fixing time. The method leverages a feature set composed of the bug description and reporter comments and adopts a transfer learning strategy using pre-trained Large Language Models (LLMs). The problem is framed as a supervised text classification task, where LLMs exploit their ability to learn rich contextual representations of language. We introduce a novel classification workflow that guides the LLM through a structured prompt, combining two design patterns: the persona pattern to contextualize the task and the input semantic pattern to organize textual information. The workflow relies on zero-shot learning to assess whether the intrinsic knowledge embedded in the LLMs is sufficient for this prediction task. We conducted a comprehensive evaluation of three state-of-the-art LLMs across multiple real-world datasets sourced from Bugzilla, encompassing a diverse range of software projects. The experimental results demonstrate that the proposed method is effective in accurately identifying fast-resolving bugs. Among the evaluated models, LLaMA3-8B consistently delivered superior performance. Additionally, the absence of statistically significant performance variations across datasets highlights the generalizability of the approach. Notably, the LLMs maintained strong performance even on small and imbalanced datasets, underscoring their robustness and practical applicability in real-world, data-scarce scenarios.
ER  - 

TY  - JOUR
T1  - Parallelized derivation algorithm for anomaly detection in internet of things environments
AU  - Khan, Abdul Qadir
AU  - Tamani, Nouredine
AU  - El Jaouhari, Saad
JO  - Expert Systems with Applications
VL  - 296
SP  - 128958
PY  - 2026
DA  - 2026/01/15/
SN  - 0957-4174
DO  - https://doi.org/10.1016/j.eswa.2025.128958
UR  - https://www.sciencedirect.com/science/article/pii/S0957417425025758
KW  - Knowledge base system
KW  - Context
KW  - Derivation
KW  - Parallelization
KW  - Complexity
KW  - Internet of things
KW  - Cybersecurity
KW  - Anomaly detection
KW  - Inconsistency checking
AB  - The widespread adoption of the Internet of Things (IoT) has revolutionized user experiences but also raised concerns about security and privacy. To effectively safeguard against such cyber-threats, there is a need for approaches that can efficiently understand the usages of users’ IoT devices, applications, and data to better detect suspicious activities and protect users’ sensitive information within specific contexts. The semantic modeling capabilities of Knowledge Base System (KBS), make them well-suited for addressing these issues. From a semantics point of view, an anomaly in KBS can be modeled by a fragment of rules that can lead to inconsistencies inside the logical framework. Therefore, we map the anomaly detection problem to the inconsistency checkingproblem in KBS. As consistency checking is a hard problem, we introduce in this paper a new computational approach that reduces the complexity of the reasoning process by taking into account, in the logical modeling, theintrinsic idiosyncrasies of the application domain. We first introduce an algorithm for Knowledge Base (KB) rewriting into a context-based KB. Then, we detail our Lightweight Contextual Derivation Algorithm (LCDA) for inconsistency checking in KBS. Further, we propose an improvement of LCDA by adding parallelization, in ParallelLCDA (PLCDA), to accelerate the derivation process. We implemented and tested our algorithms in a practical use case in a smart home to demonstrate their effectiveness in the cybersecurity domain. Based on the obtained results, LCDA and PLCDA yield promising results in terms of response time in comparison with the regular logical chasing derivation approach.
ER  - 

TY  - JOUR
T1  - Towards a standardized methodology and dataset for evaluating LLM-based digital forensic timeline analysis
AU  - Studiawan, Hudan
AU  - Breitinger, Frank
AU  - Scanlon, Mark
JO  - Forensic Science International: Digital Investigation
VL  - 54
SP  - 301982
PY  - 2025
DA  - 2025/10/01/
T2  - DFRWS APAC 2025 - Selected Papers from the 5th Annual Digital Forensics Research Conference APAC
SN  - 2666-2817
DO  - https://doi.org/10.1016/j.fsidi.2025.301982
UR  - https://www.sciencedirect.com/science/article/pii/S2666281725001222
KW  - LLM evaluation
KW  - Forensic timeline analysis
KW  - Large language models
KW  - ChatGPT
KW  - log2timeline/plaso
AB  - Large language models (LLMs) have widespread adoption in many domains, including digital forensics. While prior research has largely centered on case studies and examples demonstrating how LLMs can assist forensic investigations, deeper explorations remain limited, i.e., a standardized approach for precise performance evaluations is lacking. Inspired by the NIST Computer Forensic Tool Testing Program, this paper proposes a standardized methodology to quantitatively evaluate the application of LLMs for digital forensic tasks, specifically in timeline analysis. The paper describes the components of the methodology, including the dataset, timeline generation, and ground truth development. In addition, the paper recommends the use of BLEU and ROUGE metrics for the quantitative evaluation of LLMs through case studies or tasks involving timeline analysis. Experimental results using ChatGPT demonstrate that the proposed methodology can effectively evaluate LLM-based forensic timeline analysis. Finally, we discuss the limitations of applying LLMs to forensic timeline analysis.
ER  - 

TY  - JOUR
T1  - Edge propagation for link prediction in requirement-cyber threat intelligence knowledge graph
AU  - Zhang, Yang
AU  - Chen, Jiarui
AU  - Cheng, Zhe
AU  - Shen, Xiong
AU  - Qin, Jiancheng
AU  - Han, Yingzheng
AU  - Lu, Yiqin
JO  - Information Sciences
VL  - 653
SP  - 119770
PY  - 2024
DA  - 2024/01/01/
SN  - 0020-0255
DO  - https://doi.org/10.1016/j.ins.2023.119770
UR  - https://www.sciencedirect.com/science/article/pii/S0020025523013555
KW  - Critical information infrastructure
KW  - Cyber threat intelligence
KW  - Graph neural network
KW  - Link prediction
KW  - Knowledge graph
AB  - Critical information infrastructure (CII) is a critical component of national socioeconomic systems and one of the primary targets of cyberattacks. Unfortunately, CII's security administration struggles to keep up with the rapidly evolving and complex cyber threats. In this research, we combine cybersecurity threat intelligence (CTI) with management security requirements (SR) data to construct a knowledge graph (KG) named RCTI and predict new knowledge on the heterogeneous graph. In addition, we propose EGNN, a novel GNN-based model that defines the representation of edges and develop an algorithm for propagating edge information. Experiments on three public datasets and the RCTI graph show that the EGNN achieves state-of-the-art performance. Finally, we use the EGNN model to predict new links on the RCTI graph, which by manual analysis achieves a 97% connectivity rate between the CTI and SR entities. Therefore, the EGNN can effectively detect management vulnerabilities and enhance CII's cybersecurity capability in the event of cybersecurity incidents.
ER  - 

TY  - JOUR
T1  - Experimental investigation of memory-related software aging in LLM systems
AU  - Santos, César
AU  - Machida, Fumio
AU  - Andrade, Ermeson
JO  - Journal of Systems and Software
VL  - 231
SP  - 112653
PY  - 2026
DA  - 2026/01/01/
SN  - 0164-1212
DO  - https://doi.org/10.1016/j.jss.2025.112653
UR  - https://www.sciencedirect.com/science/article/pii/S016412122500322X
KW  - CPU
KW  - GPU
KW  - Software aging
KW  - LLMs
KW  - Memory degradation
AB  - Large Language Models (LLMs) have been increasingly adopted in a wide range of applications, many of which require long-running inference processes. However, these systems may be subject to software aging phenomena, leading to progressive performance degradation and potential failures. In this work, we experimentally investigate memory-related software aging in LLM inference. We performed 48-hour experiments with three open-source models (Pythia, OPT, and GPT-Neo) under low, medium, and high workloads, monitoring memory consumption at both system and process levels. Using the Mann–Kendall test and Sen’s slope estimator, we observed monotonic growth in RAM usage across all models on Central Processing Units (CPUs), with OPT presenting the steepest slopes. Process-level analysis further revealed that LLM processes were the primary contributors to memory growth, along with background services. Additionally, we conducted identical experiments on Graphics Processing Units (GPUs). Unlike the experiments without a GPU, GPU-based experiments revealed bounded oscillations and abrupt resets likely due to driver-level memory management, while host RAM and process-level monitoring still revealed clear symptoms of aging. These findings demonstrate that software aging manifests differently across execution environments, reinforcing the need for environment-specific monitoring approaches.
ER  - 

TY  - JOUR
T1  - Adapting Convolutional Autoencoder for DDoS Attack Detection via Joint Reconstruction Learning and Refined Anomaly Scoring
AU  - Han, Seulki
AU  - Son, Sangho
AU  - Sakong, Won
AU  - Jung, Haemin
JO  - Computers, Materials and Continua
VL  - 85
IS  - 2
SP  - 2893
EP  - 2912
PY  - 2025
DA  - 2025/09/23/
SN  - 1546-2218
DO  - https://doi.org/10.32604/cmc.2025.067211
UR  - https://www.sciencedirect.com/science/article/pii/S1546221825008793
KW  - Anomaly detection
KW  - DDoS attack detection
KW  - convolutional autoencoder
AB  - As cyber threats become increasingly sophisticated, Distributed Denial-of-Service (DDoS) attacks continue to pose a serious threat to network infrastructure, often disrupting critical services through overwhelming traffic. Although unsupervised anomaly detection using convolutional autoencoders (CAEs) has gained attention for its ability to model normal network behavior without requiring labeled data, conventional CAEs struggle to effectively distinguish between normal and attack traffic due to over-generalized reconstructions and naive anomaly scoring. To address these limitations, we propose CA-CAE, a novel anomaly detection framework designed to improve DDoS detection through asymmetric joint reconstruction learning and refined anomaly scoring. Our architecture connects two CAEs sequentially with asymmetric filter allocation, which amplifies reconstruction errors for anomalous data while preserving low errors for normal traffic. Additionally, we introduce a scoring mechanism that incorporates exponential decay weighting to emphasize recent anomalies and relative traffic volume adjustment to highlight high-risk instances, enabling more accurate and timely detection. We evaluate CA-CAE on a real-world network traffic dataset collected using Cisco NetFlow, containing over 190,000 normal instances and only 78 anomalous instances—an extremely imbalanced scenario (0.0004% anomalies). We validate the proposed framework through extensive experiments, including statistical tests and comparisons with baseline models. Despite this challenge, our method achieves significant improvement, increasing the F1-score from 0.515 obtained by the baseline CAE to 0.934, and outperforming other models. These results demonstrate the effectiveness, scalability, and practicality of CA-CAE for unsupervised DDoS detection in realistic network environments. By combining lightweight model architecture with a domain-aware scoring strategy, our framework provides a robust solution for early detection of DDoS attacks without relying on labeled attack data.
ER  - 

TY  - JOUR
T1  - Exploring topic models to discern cyber threats on Twitter: A case study on Log4Shell
AU  - Wang, Yue
AU  - Bashar, Md Abul
AU  - Chandramohan, Mahinthan
AU  - Nayak, Richi
JO  - Intelligent Systems with Applications
VL  - 20
SP  - 200280
PY  - 2023
DA  - 2023/11/01/
SN  - 2667-3053
DO  - https://doi.org/10.1016/j.iswa.2023.200280
UR  - https://www.sciencedirect.com/science/article/pii/S2667305323001059
KW  - Social media analysis
KW  - Topic modeling
KW  - Software vulnerability and threat
KW  - Cyber threat intelligence
KW  - Log4Shell
KW  - Threat hunting
AB  - Gathering information about cyber threats from various sources can help organisations improve proactive cyber defense and mitigate potential cyber attacks. Recently, Twitter has shown to be beneficial in providing timely Cyber Threat Intelligence (CTI) concerning cyber threats, software vulnerabilities and exploits. However, manually identifying and investigating useful insights, patterns, and trends from abundant unstructured tweets is difficult. This work proposes an end-to-end data-driven framework to collect, analyze, and monitor tweets using unsupervised topic modeling techniques. A novel visualization technique is also proposed to monitor the dynamic topic trends over time, offering an interpretable way to gain insights into a topic's lifecycle. A case study is conducted on the Log4shell vulnerability incident to demonstrate the applicability of the proposed framework. Experiments are carried out on a real-world Twitter dataset collected from 47 users within the CTI community. Results indicate that the proposed framework can discover emerging topics relevant to real-world cybersecurity incidents, with Log4Shell-related topics identified before the common public disclosure date by the National Vulnerability Database (NVD). This framework can expedite the data processing workflow and visualization for cyber threat analysis, enabling organizations to identify trends and patterns that can potentially indicate a security breach or attack.
ER  - 

TY  - JOUR
T1  - Rethinking driver fatigue detection as anomaly identification: A hypergraph-transformer approach
AU  - He, Jibo
AU  - Yuan, Hao
AU  - Li, Bingdong
AU  - Zhang, Huiliang
AU  - Meng, Xin
AU  - Yin, Jiali
AU  - Li, Yan
JO  - Information Processing & Management
VL  - 63
IS  - 3
SP  - 104553
PY  - 2026
DA  - 2026/04/01/
SN  - 0306-4573
DO  - https://doi.org/10.1016/j.ipm.2025.104553
UR  - https://www.sciencedirect.com/science/article/pii/S0306457325004947
KW  - Driver fatigue detection
KW  - Multivariate time series
KW  - Hypergraph neural networks
KW  - Transformer models
KW  - Anomaly detection
KW  - Wearable sensors
AB  - Driver fatigue contributes to approximately 20 % of traffic accidents worldwide, yet existing detection methods face critical limitations: they rely on specialized medical equipment like brain monitoring headsets that are impractical for deployment, or use single source data which may have individual differences, and treat fatigue as discrete classification rather than recognizing its gradual onset. To address these practical constraints, we first introduce CBW dataset, a multi-source dataset designed specifically for fatigue detection using daily accessible sensors. Our dataset contains 600+ h of synchronized vehicle CAN-bus data (7 dimensions) and consumer smartwatch measurements (11 dimensions) from real-world highway driving, eliminating the need for specialized medical equipment while capturing comprehensive driver state information. Furthermore, we reconceptualize driver fatigue detection as an anomaly detection problem for the first time, leveraging the natural observation that normal driving patterns are abundant and safe to collect while fatigue represents rare deviations. We present Hypergraph-Transformer Driving Fatigue Detection (HG-TransDFD ), a hypergraph-based anomaly detection framework that identifies fatigue through deviations from normal driving behavior. Our approach integrates vehicle telemetry and physiological data through hierarchical hypergraph learning, capturing complex cross-source relationships that traditional graph structures or simple fusion methods cannot effectively model. Experimental results demonstrate remarkable advances in both performance and practicality: HG-TransDFD achieves F1 scores of 0.71, 0.73, and 0.75 on CAN-bus, Watch, and Synchronized subsets respectively, outperforming baseline methods by 5.9-6.3 % through effective multi-source data integration. Our approach also requires only 10 % of labeled data to match the performance of traditional methods trained on 50 % labeled data, representing a 5 ×  improvement in label efficiency.
ER  - 

TY  - JOUR
T1  - Collaborative anomaly detection in log data: Comparative analysis and evaluation framework
AU  - García Gómez, André
AU  - Landauer, Max
AU  - Wurzenberger, Markus
AU  - Skopik, Florian
AU  - Weippl, Edgar
JO  - Future Generation Computer Systems
VL  - 175
SP  - 108090
PY  - 2026
DA  - 2026/02/01/
SN  - 0167-739X
DO  - https://doi.org/10.1016/j.future.2025.108090
UR  - https://www.sciencedirect.com/science/article/pii/S0167739X2500384X
KW  - Machine learning
KW  - CIDS
KW  - IDS
KW  - Anomaly detection
KW  - AI
KW  - Log analysis
AB  - Log Anomaly Collaborative Intrusion Detection Systems (CIDS) are designed to detect suspicious activities and security breaches by analyzing log files using anomaly detection techniques while leveraging collaboration between multiple entities (e.g., different systems, organizations, or network nodes). Unlike traditional Intrusion Detection Systems (IDS) that require centralized algorithm updates and data aggregation, CIDS enable decentralized updates without extensive data exchange, improving efficacy, scalability, and compliance with regulatory constraints. Additionally, inter-detector communication helps to reduce the number of false positives. These systems are particularly useful in distributed environments, where individual system have limited visibility into potential threats. This paper reviews the current landscape of Log Anomaly CIDS and introduces an open-source framework designed to create benchmark datasets for evaluating system performance. We categorize log anomaly detectors into three categories: Sequential-wise, Embedding-wise, and Graph-wise. Furthermore, our open framework facilitates rigorous evaluation against different challenges identifying weaknesses in existing methods like Deeplog and enhancing model robustness.
ER  - 

TY  - JOUR
T1  - Cybersecurity preparedness of small-to-medium businesses: A Western Australia study with broader implications
AU  - Chidukwani, Alladean
AU  - Zander, Sebastian
AU  - Koutsakis, Polychronis
JO  - Computers & Security
VL  - 145
SP  - 104026
PY  - 2024
DA  - 2024/10/01/
SN  - 0167-4048
DO  - https://doi.org/10.1016/j.cose.2024.104026
UR  - https://www.sciencedirect.com/science/article/pii/S0167404824003316
KW  - Cybersecurity threat awareness
KW  - Cybersecurity challenges in SMBs
KW  - Cybersecurity controls
KW  - Cybersecurity factors in SMBs
KW  - Cybersecurity guidance for SMBs
KW  - Cybersecurity gaps in SMBs
KW  - Cybersecurity legislation and SMBs
KW  - Cybersecurity legislation compliance
KW  - SMB cybersecurity awareness
KW  - NIST CSF in SMBs
AB  - This study was prompted by the scarcity of focused quantitative research on the cybersecurity of SMBs. Our research aimed to understand the factors influencing SMBs' approach to cybersecurity, their level of threat awareness and the importance placed on cybersecurity. It also explored the extent to which NIST CSF practices are implemented by SMBs while also detecting and ranking the prevalent challenges faced by SMBs. Additionally, resources that SMBs turn to for help and guidance were also evaluated. While the survey-based study was on Western Australian SMBs, the results are of more general and wider interest. Our study found the lack of funds to be the biggest hindrance to cybersecurity, along with a lack of knowledge on where to start implementing good security practices. SMBs also lacked familiarity with relevant regulations and frameworks. The study highlights areas for improvement, such as access control mechanisms, individual user accounts, formalised policies and procedures, and dedicated budgets. SMBs heavily rely on Google search for cybersecurity information, emphasising the need for optimised search results from authoritative sources. IT service providers and informal networks also emerge as important sources of cybersecurity guidance, while local universities could assist SMBs but remain underutilised in this regard. Interestingly, factors such as organisational size, industry sector, and revenue level did not significantly impact SMBs' perception of vulnerability to cyber threats. However, further investigation is needed to evaluate the effectiveness of different IT service models for SMBs' cybersecurity needs. Overall, the research provides valuable insights into the specific gaps and challenges faced by SMBs in the cybersecurity domain, as well as their preferred methods of seeking and consuming cybersecurity assistance. The findings can guide the development of targeted strategies and policies to enhance the cybersecurity posture of SMBs.
ER  - 

TY  - JOUR
T1  - UAV Leveraging GenAI/LLMs, a Brief Survey
AU  - Cidjeu, Diderot D.
AU  - Ebongue Fendji, Jean Louis Kedieng
AU  - Kamla, Vivent Corneille
AU  - Tchappi, Igor
JO  - Procedia Computer Science
VL  - 265
SP  - 382
EP  - 389
PY  - 2025
DA  - 2025/01/01/
T2  - 20th International Conference on Future Networks and Communications/ 22nd International Conference on Mobile Systems and Pervasive Computing/15th International Conference on Sustainable Energy Information Technology (FNC/MobiSPC/SEIT 2025)
SN  - 1877-0509
DO  - https://doi.org/10.1016/j.procs.2025.07.195
UR  - https://www.sciencedirect.com/science/article/pii/S1877050925022458
KW  - Unmanned Aerial Vehicles
KW  - Generative AI
KW  - Large Language Models
AB  - Unmanned Aerial Vehicles (UAVs) have emerged as a technology with applications in both military and civilian fields. In the same vein, Large Language Models (LLMs) have emerged in various domains recently and have demonstrated remarkable capabilities, particularly in natural language processing. Several authors have focused on the integration of LLMs into UAVs, in order to increase their capabilities, adaptation, interconnection, and interaction with humans. This paper is a concise survey of the work done in this direction over the period of 2021 - 2024. We selected and analyzed papers according to the Systematic Mapping Study leading to the identification of the subtopics covered, the tools used, the contributions of the authors, and the limitations. A multi-criteria taxonomy is also proposed, with discussions on trends for future research.
ER  - 

TY  - JOUR
T1  - Harnessing GPT-4 for generation of cybersecurity GRC policies: A focus on ransomware attack mitigation
AU  - McIntosh, Timothy
AU  - Liu, Tong
AU  - Susnjak, Teo
AU  - Alavizadeh, Hooman
AU  - Ng, Alex
AU  - Nowrozy, Raza
AU  - Watters, Paul
JO  - Computers & Security
VL  - 134
SP  - 103424
PY  - 2023
DA  - 2023/11/01/
SN  - 0167-4048
DO  - https://doi.org/10.1016/j.cose.2023.103424
UR  - https://www.sciencedirect.com/science/article/pii/S0167404823003346
KW  - GPT
KW  - Cybersecurity policies
KW  - Ransomware
KW  - Policy generation
KW  - GRC
AB  - This study investigated the potential of Generative Pre-trained Transformers (GPTs), a state-of-the-art large language model, in generating cybersecurity policies to deter and mitigate ransomware attacks that perform data exfiltration. We compared the effectiveness, efficiency, completeness, and ethical compliance of GPT-generated Governance, Risk and Compliance (GRC) policies, with those from established security vendors and government cybersecurity agencies, using game theory, cost-benefit analysis, coverage ratio, and multi-objective optimization. Our findings demonstrated that GPT-generated policies could outperform human-generated policies in certain contexts, particularly when provided with tailored input prompts. To address the limitations of our study, we conducted our analysis with thorough human moderation, tailored input prompts, and the inclusion of legal and ethical experts. Based on these results, we made recommendations for corporates considering the incorporation of GPT in their GRC policy making.
ER  - 

TY  - JOUR
T1  - Harnessing collective intelligence of multi-agent LLM systems for sensor failure reasoning in smart manufacturing
AU  - Gong, Wei
AU  - Qiao, Shuang
AU  - Cao, Chenhong
AU  - Tan, Shilei
AU  - Ye, Junliang
AU  - Liu, Haoxiang
AU  - Chen, Si
AU  - Wang, Xuesong
JO  - Journal of Industrial Information Integration
VL  - 49
SP  - 101012
PY  - 2026
DA  - 2026/01/01/
SN  - 2452-414X
DO  - https://doi.org/10.1016/j.jii.2025.101012
UR  - https://www.sciencedirect.com/science/article/pii/S2452414X25002353
KW  - Smart manufacturing
KW  - Sensor fault diagnosis
KW  - Multi-agent systems
KW  - Large language models
AB  - In smart manufacturing, accurate sensor fault diagnosis is essential for operational integrity. However, the direct application of Large Language Models (LLMs) to this task yields unstructured analyses and inefficient resource use. To address these challenges, we propose a novel multi-agent framework that instills a structured, modular, and adaptive reasoning process. The framework features a Reasoning Module to classify problem complexity and a Decision Module that employs a difficulty-aware workflow. Simple problems are resolved directly, while complex cases activate a deliberative debate among multiple agents to form a consensus. Evaluated on the specialized FailureSensorIQ benchmark, our framework significantly boosts the performance of open-source LLMs. For example, Llama3.1-8B-instruct’s accuracy surged from 36.5% to 54.6%—an 18.1 percentage point improvement. Crucially, our method empowers smaller 7B/8B models to surpass larger, proprietary models like GPT-4o-mini. Ablation studies validate that our dynamic routing mechanism provides an optimal trade-off between diagnostic accuracy and computational cost. This work establishes a new paradigm for industrial fault diagnosis, improving accuracy, interpretability, and resource efficiency, thereby paving the way for reliable and accessible AI in critical manufacturing systems.
ER  - 

TY  - JOUR
T1  - A chit-chat between Llama 2 and ChatGPT for the automated creation of exploits
AU  - Caturano, Francesco
AU  - Ciotola, Jordan
AU  - Romano, Simon Pietro
AU  - Varlese, Mario
JO  - Computer Networks
VL  - 270
SP  - 111501
PY  - 2025
DA  - 2025/10/01/
SN  - 1389-1286
DO  - https://doi.org/10.1016/j.comnet.2025.111501
UR  - https://www.sciencedirect.com/science/article/pii/S1389128625004682
KW  - Exploit generation
KW  - Generative AI
KW  - Large language models
KW  - Coding automation
KW  - Llama 2
KW  - ChatGPT
AB  - Software exploitation is the process of taking advantage of vulnerabilities in software systems in order to perform unintended activities. Its understanding leads to improved defensive measures and informed decision making about which security mechanisms to prioritize. However, creating a software exploit is typically a time-consuming and manual task that demands a deep understanding of programming, network protocols, operating system internals, and computer architectures. Additionally, it requires the ability to integrate this knowledge through complex reasoning and problem-solving techniques. This paper proposes an approach to tackle the aforementioned problems by encouraging a conversation between Large Language Models (LLMs) with the purpose of generating software exploits. First, the chosen LLMs are provided with the necessary context knowledge, through modern techniques of fine-tuning and prompt engineering. Then, the exploitation methodology is divided into several steps: vulnerable program analysis, identification of the exploit, planning of the exploitation process, discovery of architecture internals, and production of the exploit software. A first Large Language Model (LLM) is designed to ask questions to a second LLM regarding the execution of the above mentioned steps. The final output from the second LLM provides fully automated, functional exploit code. This method demonstrates how two LLMs – one possessing capabilities in exploitation and coding, and the other with expertise in computer architecture – can collaborate to successfully exploit Buffer Overflow vulnerabilities.
ER  - 

TY  - JOUR
T1  - Research on the application of large language model in electric power enterprise audit
AU  - Huang, He
AU  - Cao, Peixiang
AU  - Liu, Yao
AU  - Lv, Yan
AU  - Tong, Min
JO  - Procedia Computer Science
VL  - 247
SP  - 1388
EP  - 1394
PY  - 2024
DA  - 2024/01/01/
T2  - The 11th International Conference on Applications and Techniques in Cyber Intelligence
SN  - 1877-0509
DO  - https://doi.org/10.1016/j.procs.2024.10.166
UR  - https://www.sciencedirect.com/science/article/pii/S1877050924029673
KW  - Large language model
KW  - Electric power enterprises
KW  - Enterprise audit
AB  - Large language model is a new technology in the field of natural language processing, which can be applied in the audit scenario of electric power enterprises to provide auditors with automatic audit methods based on intelligent language model. Based on the application background of big language model in the audit field of electric power enterprises, this paper introduces the main advantages of big language model, and analyzes the implementation methods, application processes and principles of big language model in electric power enterprise audit, so as to provide reference and help.
ER  - 

TY  - JOUR
T1  - Belt rotation in pipe conveyors: Development of an overlap monitoring system using digital twins, industrial Internet of things, and autoregressive language models
AU  - dos Santos e Santos, Leonardo
AU  - Ribeiro Filho, Paulo Roberto Campos Flexa
AU  - Macêdo, Emanuel Negrão
JO  - Measurement
VL  - 230
SP  - 114546
PY  - 2024
DA  - 2024/05/15/
SN  - 0263-2241
DO  - https://doi.org/10.1016/j.measurement.2024.114546
UR  - https://www.sciencedirect.com/science/article/pii/S0263224124004317
KW  - Belt rotation
KW  - Digital twin
KW  - industrial Internet of Things
KW  - Autoregressive language model
AB  - Belt rotation is a critical failure mode in pipe conveyors. However, overlap position monitoring remains unexplored. Existing monitoring solutions are expensive and lack scalability, primarily relying on detecting angle variations beyond predefined thresholds. We developed a digital twin of a low-cost test rig for measuring belt rotation using 3D prototyping, infrared distance sensors, and a 3-axis accelerometer. We implemented a controller-responder industrial Internet of Things network using ESP-NOW protocol. Applying a minimum viable prompt, we fine-tuned an autoregressive language model for predicting the overlap position. We then developed a software application for cloud computing and local storage of measurements. Additionally, we developed a human–machine interface for future real-case applications. The bias and precision for the predicted angle (0.2294° ± 11.8822°), angular speed (0.1630 ± 0.7344 RPM), number of laps (0.0001336 ± 0.0448 laps), diameter (–0.1984 ± 6.3743 mm), and outer-edge side (100 % agreement) were suitable for the intended use.
ER  - 

TY  - JOUR
T1  - Proactive threat detection in enterprise systems using Wazuh: A MITRE ATT&CK Evaluation
AU  - Winkler, Aidan M.
AU  - Sharma, Prinkle
JO  - Computers & Security
VL  - 159
SP  - 104702
PY  - 2025
DA  - 2025/12/01/
SN  - 0167-4048
DO  - https://doi.org/10.1016/j.cose.2025.104702
UR  - https://www.sciencedirect.com/science/article/pii/S0167404825003918
KW  - MITRE ATT&CK
KW  - Wazuh
KW  - SIEM
KW  - Adversary emulation
KW  - Threat detection
KW  - Atomic red team
KW  - Cybersecurity
AB  - The proactive detection of advanced adversarial behaviors remains a critical challenge for Security Information and Event Management (SIEM) platforms, particularly as attackers adopt stealthy, multi-phase campaigns. This paper presents a cross-platform, MITRE ATT&CK aligned evaluation framework for systematically measuring the SIEM detection coverage, responsiveness, and accuracy. The framework was demonstrated through the Wazuh SIEM platform and atomic red team testing, targeting four high-impact tactics: Collection, Command-and-Control (C2), Exfiltration, and Impact. The results show a high detection rate for C2 and Impact techniques, and partial detection for Collection and Ex-filtration tactics owing to gaps in correlation and telemetry depth. The overall detection rate was approximately 85%, with platform-specific differences driven by the endpoint logging capabilities. Quantitative performance analysis yielded a precision of 91.4%, recall of 85.2%, and false positive rate of 4.8%, confirming both detection effectiveness and operational feasibility. The main contributions of this study are as follows: (i) a reproducible, ATT&CK aligned framework adaptable to both open source and commercial SIEMs, (ii) actionable detection rule enhancements to improve Security Operations Centerwork (SOC) operations, and (iii) scalability considerations for deployment in enterprise environments. By integrating structured adversary modeling with operational SOCs flows, the proposed framework advances proactive cyber defence in complex enterprise environments.
ER  - 

TY  - JOUR
T1  - A novel approach for malicious URL detection using RoBERTa and sparse autoencoder
AU  - Huang, Zhiqing
AU  - Ban, Tian
AU  - Zhang, Yanxin
JO  - Journal of Information Security and Applications
VL  - 94
SP  - 104214
PY  - 2025
DA  - 2025/11/01/
SN  - 2214-2126
DO  - https://doi.org/10.1016/j.jisa.2025.104214
UR  - https://www.sciencedirect.com/science/article/pii/S2214212625002510
KW  - Malicious URL
KW  - Anomaly detection
KW  - Fine-tuning model
KW  - RoBERTa
KW  - Autoencoder
KW  - Unsupervised learning
AB  - Detecting malicious URLs within requests is an effective method for blocking Web threats. Current methods for detecting malicious URLs mainly rely on supervised machine learning algorithms to construct classification models, which consequently demand high-quality training data. And these methods also have limitations in detecting malicious samples, resulting in a high false negative rate when encountering unknown anomalies. This paper proposes an anomaly detection method based on RoBERTa and sparse autoencoder for detecting malicious URLs. This method initially involves preprocessing the URL samples. Subsequently, RoBERTa is used to extract features from URLs and converts them into feature vectors. Sparse autoencoder is utilized to detect malicious samples ultimately. During the model training process, only benign samples are used as input. It enables sparse autoencoder to effectively reconstruct the characteristics of benign samples to identify malicious ones. This method was tested on the dataset composed of CSIC2010 and PRDREQ. The experimental results show that the detection model achieves an accuracy of 0.9921, a recall of 0.9863, and an F1 score of 0.9887, outperforming all baseline methods.
ER  - 

TY  - JOUR
T1  - Toward efficient vibe coding: An LLM-based agent for low-code software development
AU  - Malamas, Nikolaos
AU  - Tsardoulias, Emmanouil
AU  - Panayiotou, Konstantinos
AU  - Symeonidis, Andreas L.
JO  - Journal of Computer Languages
VL  - 85
SP  - 101367
PY  - 2025
DA  - 2025/11/01/
SN  - 2590-1184
DO  - https://doi.org/10.1016/j.cola.2025.101367
UR  - https://www.sciencedirect.com/science/article/pii/S259011842500053X
KW  - Large Language Models
KW  - Conversational interfaces
KW  - DSLs
KW  - DSL agent
KW  - Conversational low-code development
KW  - Vibe coding
AB  - The Software Engineering (SE) domain increasingly adopts low-code and no-code approaches to simplify application development and deployment. Two dominant paradigms have emerged in this space: Model-driven Engineering (MDE), leveraging Domain-specific Languages (DSLs) to abstract implementation and reduce the knowledge and expertise required, and LLM-based vibe coding, where developers interact with Large Language Models (LLMs) using natural language, allowing for rapid prototyping and code generation through conversations. Although DSLs provide precise abstractions and formal correctness, they often require specialized knowledge and have a steep learning curve. Conversely, vibe coding enables fluid and natural interactions, but struggles with domain specificity and frequently produces erroneous or unstructured code, which is difficult to integrate into formal development workflows. To harness the strengths of both paradigms, we present DSL Agent, an LLM-powered conversational interface for DSL-based application development. The DSL Agent is embedded within Locsys, a modern low-code development platform. It combines the flexibility and intuitiveness of LLM-based vibe coding with the rigor of DSLs by dynamically generating accurate and valid DSL models based on user descriptions, embedded into a unified conversational interface that leverages prompt engineering and in-context learning techniques. This offers a simpler and more intuitive interface, accelerates the development process, and reduces the expertise barrier. The agent is evaluated by more than 130 workshop participants of varying expertise levels, on two DSLs of different complexity. Evaluation metrics, including valid model rate, user satisfaction, and development time, indicate a significant improvement in valid model generation, productivity, and ease of use compared to traditional DSL-based SE workflows. These results highlight the potential of the DSL Agent to improve the entire DSL-based development life cycle by offering an efficient, intuitive, and user-friendly interface.
ER  - 

TY  - JOUR
T1  - Redefining the Programmer: Human-AI Collaboration, LLMs, and Security in Modern Software Engineering
AU  - Cruz, Elyson De La
AU  - Le, Hanh
AU  - Meduri, Karthik
AU  - Nadella, Geeta Sandeep
AU  - Gonaygunta, Hari
JO  - Computers, Materials and Continua
VL  - 85
IS  - 2
SP  - 3569
EP  - 3582
PY  - 2025
DA  - 2025/09/23/
SN  - 1546-2218
DO  - https://doi.org/10.32604/cmc.2025.068137
UR  - https://www.sciencedirect.com/science/article/pii/S1546221825008926
KW  - Human-AI collaboration
KW  - large language models
KW  - AI security
KW  - developer identity
KW  - ethical AI in software development
KW  - AI-assisted programming
AB  - The rapid integration of artificial intelligence (AI) into software development, driven by large language models (LLMs), is reshaping the role of programmers from traditional coders into strategic collaborators within Industry 4.0 ecosystems. This qualitative study employs a hermeneutic phenomenological approach to explore the lived experiences of Information Technology (IT) professionals as they navigate a dynamic technological landscape marked by intelligent automation, shifting professional identities, and emerging ethical concerns. Findings indicate that developers are actively adapting to AI-augmented environments by engaging in continuous upskilling, prompt engineering, interdisciplinary collaboration, and heightened ethical awareness. However, participants also voiced growing concerns about the reliability and security of AI-generated code, noting that these tools can introduce hidden vulnerabilities and reduce critical engagement due to automation bias. Many described instances of flawed logic, insecure patterns, or syntactically correct but contextually inappropriate suggestions, underscoring the need for rigorous human oversight. Additionally, the study reveals anxieties around job displacement and the gradual erosion of fundamental coding skills, particularly in environments where AI tools dominate routine development tasks. These findings highlight an urgent need for educational reforms, industry standards, and organizational policies that prioritize both technical robustness and the preservation of human expertise. As AI becomes increasingly embedded in software engineering workflows, this research offers timely insights into how developers and organizations can responsibly integrate intelligent systems to promote accountability, resilience, and innovation across the software development lifecycle.
ER  - 

TY  - JOUR
T1  - SAFE-TAP: Semantic-aware and fused embedding for TAP rule security detection
AU  - Kuang, Zhejun
AU  - Zhu, Yusheng
AU  - Sun, Dawen
AU  - Zhao, Jian
AU  - Xing, Yongheng
AU  - Wang, Feng
AU  - Sun, Lei
JO  - Neurocomputing
VL  - 656
SP  - 131529
PY  - 2025
DA  - 2025/12/01/
SN  - 0925-2312
DO  - https://doi.org/10.1016/j.neucom.2025.131529
UR  - https://www.sciencedirect.com/science/article/pii/S0925231225022015
KW  - Internet of things
KW  - Trigger-action programming
KW  - Security detection
KW  - IFTTT
KW  - Natural language processing (NLP)
AB  - Trigger-Action Programming (TAP) has emerged as a widely adopted paradigm for enabling automated interoperability among IoT devices. Despite its convenience, TAP introduces significant security vulnerabilities. To address this issue, we propose SAFE-TAP, a novel framework for detecting malicious TAP rules that integrates global semantic understanding with temporal feature analysis. To further enhance the detection performance, we introduce an innovative data augmentation strategy that leverages Large Language Models (LLMs) to generate semantically consistent rule variations. This approach improves data set balance and enhances the generalizability of the model. Experimental results demonstrate that SAFE-TAP outperforms baseline methods, and the incorporation of LLM-based data augmentation significantly improves detection performance under imbalanced data scenarios.
ER  - 

TY  - JOUR
T1  - Cybersecurity in the age of generative AI: A systematic taxonomy of AI-powered vulnerability assessment and risk management
AU  - Mirtaheri, Seyedeh Leili
AU  - Movahed, Narges
AU  - Shahbazian, Reza
AU  - Pascucci, Valerio
AU  - Pugliese, Andrea
JO  - Future Generation Computer Systems
VL  - 175
SP  - 108107
PY  - 2026
DA  - 2026/02/01/
SN  - 0167-739X
DO  - https://doi.org/10.1016/j.future.2025.108107
UR  - https://www.sciencedirect.com/science/article/pii/S0167739X25004017
KW  - Generative AI
KW  - Cybersecurity
KW  - Vulnerability assessment
KW  - Risk management
KW  - Systematic review
AB  - The article discusses the transformative impact of Generative AI (GenAI) to the field of vulnerability assessment (VA) and risk management (RM) right from the beginning of their life cycle to the end in cybersecurity (CS). Through a systematic review of over 100 publications (2021-2025), we develop a comprehensive taxonomy classifying GenAI’s dual offensive and defensive applications in VA/RM. The survey spells out the dominant techniques of GenAI and also points towards challenging aspects, which include security, explainability, and trustworthiness. The resultant findings reinforce the belief that GenAI could help resolve many traditional VA/RM challenges, thus providing fertile ground for research and practice in this area.
ER  - 

TY  - JOUR
T1  - Wavy-attention network for real-time cyber-attack detection in a small modular pressurized water reactor digital control system
AU  - Ayodeji, Abiodun
AU  - Di Buono, Antonio
AU  - Pierce, Iestyn
AU  - Ahmed, Hafiz
JO  - Nuclear Engineering and Design
VL  - 424
SP  - 113277
PY  - 2024
DA  - 2024/08/01/
SN  - 0029-5493
DO  - https://doi.org/10.1016/j.nucengdes.2024.113277
UR  - https://www.sciencedirect.com/science/article/pii/S0029549324003777
KW  - Cybersecurity
KW  - Small modular reactor
KW  - Deep learning
KW  - Industrial control system
KW  - Intrusion detection system
KW  - Artificial Intelligence
AB  - Global interest in advanced reactors has been reignited by recent investments in small modular reactors and micro-reactor design. The use of digital devices is essential for meeting the size and modularity requirements of small modular reactor controls. By fully digitizing the small modular reactor control systems, critical information can be obtained to optimize control, reduce costs, and extend the reactor’s lifetime. However, the potential for cyber-attacks on digital devices leaves digital control systems vulnerable. To address this risk, this study presents a novel wavy-attention network for sensor attack detection in nuclear plants. The wavy-attention network comprises stacks of batch-normalized, dilated, one-dimensional convolution neural networks, and sequential self-attention modules, superior to conventional single-layer networks on sequence classification tasks. To evaluate the proposed wavy-attention network architecture, the International Atomic Energy Agency’s Asherah Nuclear Simulator and a false data injection toolbox found in the literature, both implemented in MATLAB/SIMULINK, are utilized. This approach leverages changes in process measurements to identify and classify cyber-attacks on priority signals using the proposed wavy-attention network. Three false data injection attacks are simulated on the simulator’s pressure, temperature, and level sensors to obtain representative process measurements. The wavy-attention network is trained and validated with normal and compromised process variables obtained from the simulator. The performance of the wavy-attention network to discriminate between the reactor states using the test set shows 99% accuracy, as opposed to other baseline models such as vanilla convolution neural networks, long short-term memory networks, and bi-directional long short-term memory networks with 90%, 77%, and 91% accuracy, respectively. An ablation study is also conducted to test the contribution of each component of the proposed architecture. The theoretical framework of the proposed wavy-attention network and its implementation for nuclear reactor digital sensor attack detection are discussed in this paper.
ER  - 

TY  - JOUR
T1  - BlockLLM: A futuristic LLM-based decentralized vehicular network architecture for secure communications
AU  - Arshad, Usama
AU  - Halim, Zahid
JO  - Computers and Electrical Engineering
VL  - 123
SP  - 110027
PY  - 2025
DA  - 2025/04/01/
SN  - 0045-7906
DO  - https://doi.org/10.1016/j.compeleceng.2024.110027
UR  - https://www.sciencedirect.com/science/article/pii/S0045790624009522
KW  - Blockchain
KW  - Vehicular Network Architecture (VNA)
KW  - Large Language Models (LLM)
KW  - Vehicle-to-Everything (V2X)
KW  - Data integrity
KW  - Scalability
AB  - Autonomous vehicles and Intelligent Transportation Systems (ITS) represent some of the most transformative technological advancements. However, these complex systems face significant challenges, including data security, scalability, node reliability, and secure communication. Current approaches often integrate Artificial Intelligence (AI), Machine Learning (ML), and blockchain to enhance robustness and flexibility, but issues like latency, privacy, and node selfishness remain. This paper proposes BlockLLM, a comprehensive vehicular network architecture that combines blockchain and Large Language Models (LLMs) to address these core issues in vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) communication. Blockchain technology ensures data integrity, nonrepudiation, and trustless communication across nodes, while LLMs provide adaptive decision-making and reduce latency in real-time data exchange. BlockLLM’s incentive and reputation mechanisms further enhance node participation and reliability. Simulation results demonstrate an 18% reduction in latency, a 12% improvement in throughput, and consistent data integrity across high-traffic scenarios. These findings indicate that BlockLLM offers a robust, scalable solution for secure V2X communication, making it a promising model for future autonomous transportation systems.
ER  - 

TY  - JOUR
T1  - An ensemble of fuzzy soft expert set with deep learning on attack detection for secure industrial cyber-physical systems
AU  - Alotaibi, Sultan Refa
AU  - Alrayes, Fatma S.
AU  - Mansouri, Wahida
AU  - Alqahtani, Hamed
AU  - Alajmani, Samah Hazzaa
AU  - Alotaibi, Moneerah
AU  - Alallah, Fouad Shoie
AU  - Alshareef, Abdulrhman
JO  - Journal of Radiation Research and Applied Sciences
VL  - 18
IS  - 2
SP  - 101464
PY  - 2025
DA  - 2025/06/01/
SN  - 1687-8507
DO  - https://doi.org/10.1016/j.jrras.2025.101464
UR  - https://www.sciencedirect.com/science/article/pii/S1687850725001761
KW  - Industrial cyber-physical systems
KW  - Grey wolf optimizer
KW  - Fuzzy set
KW  - Heuristic search
KW  - Neutrosophic model
KW  - Interval neutrosophic set
AB  - The most effective tool for forming uncertainty in problems of decision-making is neutrosophic set (NS) and its additions, like interval NS (INS), complex NS (CNS), and interval complex NS (ICNS). An effective tool for signifying vagueness and uncertainty in the decision process is NS, which is more generalization of a classical set, fuzzy set (FS), and intuitionistic fuzzy set (IFS) by including 3 degrees of falsehood, truth, and indeterminacy of a definite statement. A cyber-physical system (CPS) combines numerous connected physical methods, networking units, and computing resources. Also, it observes the applications and processes of the computational techniques. Interconnection of the cyber and physical world starts threatening security tasks, particularly with the enlarging intricacy of communication systems. Despite the struggles to contest these tasks, it is very complex to identify and examine cyber-physical threats in a compound CPS. Many researchers have implemented machine learning (ML) and deep learning (DL)-based methods for analyzing cyber-physical security methods. This study develops an Enhanced Single Valued Model using the Heuristic Search on Attack Detection (ESVM-HSAD) model in Industrial CPS. The foremost intention of the ESVM-HSAD technique is to concentrate on the recognition and classification of cyberattacks in CPS. The ESVM-HSAD method utilizes the Grey Wolf Optimizer (GWO) as a feature selection (FS) technique to select an optimum set of features. For attack recognition, the ESVM-HSAD methodology utilizes a single-valued fuzzy soft expert set (SVSFES) method and an ensemble of two DL classifiers, such as GRU and Convolutional auto-encoder (CAE). Finally, the recognition outcomes of the ensemble model are enhanced by utilizing the Whale Optimization Algorithm (WOA). Many experiments were performed to detect the improved performance of the ESVM-HSAD approach. The comparative study of the ESVM-HSAD approach exhibited a superior accuracy value of 99.01 % when equated to other methods.
ER  - 

TY  - JOUR
T1  - Exploring perceptions of decision-makers and specialists in defensive machine learning cybersecurity applications: The need for a standardised approach
AU  - Alshaikh, Omar
AU  - Parkinson, Simon
AU  - Khan, Saad
JO  - Computers & Security
VL  - 139
SP  - 103694
PY  - 2024
DA  - 2024/04/01/
SN  - 0167-4048
DO  - https://doi.org/10.1016/j.cose.2023.103694
UR  - https://www.sciencedirect.com/science/article/pii/S0167404823006041
KW  - Machine learning
KW  - Cybersecurity
KW  - Capabilities
KW  - ML application
KW  - Perception
KW  - Cybercrime
KW  - Thematic analysis
KW  - Themes
AB  - Machine learning (ML) utilisation has achieved a vast global impact. This is evident in the cybersecurity sector, where ML has wide-ranging applications, such as identifying and blocking threats, uncovering unusual software and user behaviour, and many others. However, the increase in successful cyberattacks demonstrates that the effectiveness of ML in cybersecurity applications can be questioned. Although the attacks may be new, ML is often adopted due to its ability to handle diverse and often unforeseen situations – a capability that is not possible using traditional rule-based security mechanisms. As both the rate of attacks and adoption of ML solutions are increasing, there is a need to determine whether ML-based security solutions are meeting the expectations of businesses and whether businesses are genuinely aware of the ML capabilities and limitations. Moreover, current literature shows a significant variation in how ML solutions are evaluated in cybersecurity applications, which might result in a poor understanding of ML capabilities. This paper explores the common perceptions and observations of decision-makers and specialists using ML for cybersecurity regarding its capabilities, implementation, evaluation, and communication. A semi-structured interview is conducted with individuals in various managerial positions to perform this investigation. The finding of this study reveals a pressing need for a standard to manifest ML capabilities. As significant variation in the understanding of Machine Learning Cyber Security (MLCS) capabilities is observed, a standard could help better communicate MLCS capabilities. It is observed that external influences heavily impact ML adoption decisions, potentially leading to misinterpretation of ML capabilities.
ER  - 

TY  - JOUR
T1  - Span-level emotion-cause-category triplet extraction with instruction tuning LLMs and data augmentation
AU  - Li, Xiangju
AU  - Yang, Dong
AU  - Zhu, Xiaogang
AU  - Huang, Faliang
AU  - Zhang, Peng
AU  - Zhao, Zhongying
JO  - Applied Soft Computing
VL  - 185
SP  - 113938
PY  - 2025
DA  - 2025/12/01/
SN  - 1568-4946
DO  - https://doi.org/10.1016/j.asoc.2025.113938
UR  - https://www.sciencedirect.com/science/article/pii/S1568494625012517
KW  - Fine-grained emotion cause analysis
KW  - Emotion-cause-category extraction
KW  - Instruction-tuned LLMs
KW  - Span-level triplet extraction
KW  - LLM-based data augmentation
AB  - Span-level emotion-cause-category triplet extraction is a fine-grained task in emotion cause analysis that aims to identify emotion spans, cause spans, and their corresponding emotion categories from documents. Existing methods, including clause-level emotion-cause pair extraction and span-level emotion-cause detection, often suffer from redundant information and difficulties in accurately classifying emotion categories, particularly when emotions are expressed implicitly or ambiguously. To overcome these challenges, this study explores a fine-grained approach to span-level emotion-cause-category triplet extraction and introduces an innovative framework that leverages instruction tuning and data augmentation techniques based on large language models. The proposed method employs task-specific triplet extraction instructions and utilizes low-rank adaptation to fine-tune large language models, eliminating the necessity for intricate task-specific architectures. Furthermore, an LLM-based data augmentation strategy is developed to address data scarcity by guiding large language models in generating high-quality synthetic training data. Extensive experimental evaluations demonstrate that the proposed approach significantly outperforms existing baseline methods, achieving at least a 12.8 % improvement in span-level emotion-cause-category triplet extraction metrics. The results demonstrate the method’s effectiveness and robustness, offering a promising avenue for advancing research in emotion cause analysis.
ER  - 

TY  - JOUR
T1  - HEOD: Human-assisted Ensemble Outlier Detection for cybersecurity
AU  - Najafi, Pejman
AU  - Cheng, Feng
AU  - Meinel, Christoph
JO  - Computers & Security
VL  - 146
SP  - 104040
PY  - 2024
DA  - 2024/11/01/
SN  - 0167-4048
DO  - https://doi.org/10.1016/j.cose.2024.104040
UR  - https://www.sciencedirect.com/science/article/pii/S0167404824003456
KW  - Cybersecurity
KW  - Intrusion detection
KW  - APT
KW  - Security analytics
KW  - Data-driven security
KW  - Anomaly detection
KW  - Outlier detection
AB  - Despite extensive academic research in anomaly detection within the cybersecurity domain, its successful adoption in real-world settings remains limited. This paper addresses the challenges of applying outlier detection techniques for threat detection within the context of Security Information and Event Management (SIEM) systems. It particularly highlights the significance of contextualization and explainability, while challenging the assumption that outliers invariably indicate malicious activity. It proposes a simple yet effective outlier detection technique designed to mimic a Security Operation Center (SOC) analyst’s reasoning process in finding anomalies/outliers and deciding maliciousness. The approach emphasizes explainability and simplicity, achieved by combining the output of simple, context-aware univariate submodels that calculate an outlier score for each entry. The proposed technique is first evaluated on a public dataset, demonstrating its ability to achieve high performance in detecting outliers compared to other well-known algorithms. Furthermore, to assess the practicality in a real-world scenario, the approach is deployed in production alongside the SIEM of a large international enterprise with over 100,000 assets, utilizing 20 terabytes of Endpoint Detection and Response (EDR) logs to detect Living-off-the-Land Binaries (LOLBins). The proposed framework can empower SOC analysts in developing scalable, effective, and interpretable outlier-based threat detection use cases.
ER  - 

TY  - JOUR
T1  - MoPHoney: An adaptive honeyword generation system based on Mixture-of-prompts
AU  - Chen, Yiren
AU  - Yang, Xiaobo
AU  - Dong, Fangming
AU  - Jiang, Bo
AU  - Lu, Zhigang
AU  - Liu, Baoxu
JO  - Journal of Systems Architecture
VL  - 170
SP  - 103617
PY  - 2026
DA  - 2026/01/01/
SN  - 1383-7621
DO  - https://doi.org/10.1016/j.sysarc.2025.103617
UR  - https://www.sciencedirect.com/science/article/pii/S1383762125002899
KW  - Honeywords
KW  - Password leaks discovery
KW  - Large language models
KW  - Mixture of prompts
KW  - Honeyword system architecture
AB  - With the advancement of password-cracking technologies, database security is encountering critical challenges. Honeywords, decoy passwords alongside the real password, serve as a key mechanism to detect unauthorized access from password leaks. However, most existing honeyword generation techniques (HGTs) rely on static strategies or single-model generators, resulting in insufficient robustness across threat scenarios. To alleviate this issue, we propose MoPHoney, an adaptive HGT based on mixture-of-prompts (MoP) powered by large language models (LLMs). MoPHoney initially employs a LightGBM-based router to predict a soft probability distribution over password types, which determines weights of the corresponding prompts for LLM. Then, adaptive styles of honeywords are generated through diverse prompt-guided pipelines, each enhanced via retrieval-augmented generation (RAG) to improve contextual realism. Next, the output is filtered by an LLM-based adversary that discards failed honeywords. Finally, honeyword files are stored using a new strategy to further enhance the complexity of password-guessing. We evaluate MoPHoney against four representative honeyword threat techniques on three real-world datasets and a PII-based password dataset. Compared with baseline HGTs, MoPHoney achieves superior flatness (average ɛ1-flatness below 0.078 at k=20), success-number, and resistance to DoS attack (average FPP below 0.004). Even when k varies from 5 to 50, MoPHoney maintains stable flatness and keeps false alarms under 0.5%, demonstrating robust scalability across different honeyword counts. These results not only highlight the effectiveness of input-adaptive prompts, in-context passwords, and adversarial strategies in HGTs but also show the feasibility of LLMs for generating decoys for cyber threat hunting.
ER  - 

TY  - JOUR
T1  - Reducing the risk of social engineering attacks using SOAR measures in a real world environment: A case study
AU  - Waelchli, Sandro
AU  - Walter, Yoshija
JO  - Computers & Security
VL  - 148
SP  - 104137
PY  - 2025
DA  - 2025/01/01/
SN  - 0167-4048
DO  - https://doi.org/10.1016/j.cose.2024.104137
UR  - https://www.sciencedirect.com/science/article/pii/S0167404824004425
KW  - Cyber security
KW  - Social engineering
KW  - SOAR
KW  - Security automation
KW  - Incident response
AB  - The global cost of successful cyberattacks is increasing annually, with there being a shift towards social engineering threats in recent years. Cybercriminals are increasingly targeting humans rather than technical systems, recognizing data as a critical resource, especially in the finance industry where breaches can lead to substantial losses and reputational damage. The present case study proposes measures to reduce human susceptibility to social engineering attacks, leveraging SOAR (Security Automation, Orchestration, and Response) technology for incident response automation. The study covers various issues in cybersecurity, SOAR, and social engineering, through analyzing interviews with expert practitioners in the field, addressing cybersecurity skills shortages and current cyber threats. Four social engineering vignettes were developed, representing real threats, along with specific SOAR measures implemented using Microsoft Sentinel. These measures were simulated to demonstrate their effectiveness by reducing the employee's vulnerability to social engineering attacks. The risk of social engineering attacks was successfully reduced by implementing a responsive approach through the developed SOAR measures. Some of the measures reduced the risk by locking user accounts or forcing password changes after a detected cyber incident while another measure was developed for awareness enhancements. Given the current shortage of cybersecurity professionals, technologies like SOAR are becoming increasingly relevant for security teams. However, SOAR alone cannot address all challenges posed by social engineering and should be viewed as a complementary measure rather than a standalone solution.
ER  - 

TY  - JOUR
T1  - Application of Generative Artificial Intelligence in Minimizing Cyber Attacks on Vehicular Networks
AU  - Guntuka, Sony
AU  - Shakshuki, Elhadi
JO  - Procedia Computer Science
VL  - 251
SP  - 140
EP  - 149
PY  - 2024
DA  - 2024/01/01/
T2  - 15th International Conference on Emerging Ubiquitous Systems and Pervasive Networks / 14th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare EUSPN/ICTH 2024
SN  - 1877-0509
DO  - https://doi.org/10.1016/j.procs.2024.11.094
UR  - https://www.sciencedirect.com/science/article/pii/S1877050924033283
KW  - Cyber attacks
KW  - GenAI
KW  - Vehicular Networks
AB  - This paper explores the innovative applications of Generative Artificial Intelligence (GenAI) for strengthening the cybersecurity of vehicular networks. With the advent of intelligent transport systems and autonomous vehicles, the cybersecurity landscape has evolved significantly, which necessitating new strategies to tackle sophisticated threats. GenAI provides advanced capabilities for automating defenses, enhancing threat intelligence, and fostering dynamic security frameworks in vehicular networks. However, the incorporation of GenAI also introduces new risks, requiring robust ethical, legal, and technical oversight. This research paper outlines the current state of GenAI in vehicular network cybersecurity, showcases the Vehicular Threat Intelligence Flowchart (VTIF), focuses on the threat detection rule algorithm in VTIF, highlights the potential benefits and challenges, and proposes future research directions for developing resilient and ethical cybersecurity mechanisms.
ER  - 

TY  - JOUR
T1  - Manod: A multi-modal anomaly detection framework for distributed system
AU  - Liu, Wen
AU  - Sun, Degang
AU  - Yang, Haitian
AU  - Wang, Yan
AU  - Huang, Weiqing
JO  - Neural Networks
VL  - 193
SP  - 107999
PY  - 2026
DA  - 2026/01/01/
SN  - 0893-6080
DO  - https://doi.org/10.1016/j.neunet.2025.107999
UR  - https://www.sciencedirect.com/science/article/pii/S0893608025008809
KW  - Distributed system
KW  - Anomaly detection
KW  - Multimodal learning
KW  - Time series analysis
KW  - Log modeling
KW  - Deep learning
AB  - Distributed infrastructure has been widely deployed in large-scale software systems in recent years to meet the growing demand for applications, due to its scalability and resource-sharing characteristics. Accurately predicting and identifying anomalies is critical to ensure the stable and reliable running of complex distributed systems. System abnormalities can often be reflected through key performance indicators and logs. Metrics provide quantitative measures of system performance and operational status, while logs record various events that occur in the system. Current approaches typically rely on a single data source to detect anomalies, which may lead to false positives and limit the accuracy of failure detection. A combination of these two data modalities can provide a comprehensive view of the system behavior. In this work, we propose a semi-supervised fault detection method, Manod, to monitor the health state of the system based on multimodal data. To obtain the discriminative representations, it employs a graph-based hierarchical encoding approach and leverages pre-trained language models for modeling metrics and logs, respectively. Then, it adopts a novel gated attention fusion method to integrate heterogeneous information. Extensive experiments on two datasets validate the effectiveness of our proposed Manod. It achieves F1-scores of 0.870 and 0.934 on one simulation dataset (D1) and one real-world dataset (D2), respectively, and significantly outperforms all baseline models. This demonstrates its capacity in mitigating both false positives and false negatives.
ER  - 

TY  - JOUR
T1  - Empowering cybersecurity analysis: Unifying CVE, CWE, and CPE through knowledge graphs
AU  - Benzekki, Kamal
AU  - Messai, Mohamed-Lamine
JO  - Computers & Security
VL  - 160
SP  - 104726
PY  - 2026
DA  - 2026/01/01/
SN  - 0167-4048
DO  - https://doi.org/10.1016/j.cose.2025.104726
UR  - https://www.sciencedirect.com/science/article/pii/S0167404825004158
KW  - CVE
KW  - CWE
KW  - CPE
KW  - Patch
KW  - Cybersecurity
KW  - Knowledge graph
KW  - Vulnerability
AB  - Properly managing and mitigating software vulnerabilities and weaknesses is crucial for cybersecurity. The MITRE Corporation’s community-driven initiatives, Common Vulnerabilities and Exposures (CVE) and Common Weakness Enumeration (CWE), play a vital role by offering standardized identifiers and descriptions for known issues. However, the intricate relationships between these vulnerabilities present major challenges for effective mitigation. To address this, there is increasing interest in using graph-based modeling techniques to integrate CVE and CWE data into a unified knowledge graph. Our research aims to connect this data to enhance vulnerability analysis and management. We propose a method to build and refine a cybersecurity knowledge graph that consolidates information from the CVE, CWE, and Common Platform Enumeration (CPE) databases. This interconnected data network improves vulnerability classification, assessments, and the identification of recurring threat patterns and their relationships, leading to more effective patch recommendations.
ER  - 

TY  - JOUR
T1  - Multi-level fine-tuning, data augmentation, and few-shot learning for specialized cyber threat intelligence
AU  - Bayer, Markus
AU  - Frey, Tobias
AU  - Reuter, Christian
JO  - Computers & Security
VL  - 134
SP  - 103430
PY  - 2023
DA  - 2023/11/01/
SN  - 0167-4048
DO  - https://doi.org/10.1016/j.cose.2023.103430
UR  - https://www.sciencedirect.com/science/article/pii/S0167404823003401
KW  - Cyber threat intelligence
KW  - Few-shot learning
KW  - Transfer learning
KW  - Data augmentation
KW  - Information overload
AB  - Gathering cyber threat intelligence from open sources is becoming increasingly important for maintaining and achieving a high level of security as systems become larger and more complex. However, these open sources are often subject to information overload. It is therefore useful to apply machine learning models that condense the amount of information to what is necessary. Yet, previous studies and applications have shown that existing classifiers are not able to process information about emerging cybersecurity events, such as new malware names or novel attack contexts, due to their low generalisation capability. Therefore, we propose a system to overcome this problem by training a new classifier for each new incident. Since this requires a lot of labelled data using standard training methods, we combine three different low-data regime techniques – transfer learning, data augmentation, and few-shot learning – to train a high-quality classifier from very few labelled instances. We evaluated our approach using a novel dataset derived from the Microsoft Exchange Server data breach of 2021 which was labelled by three experts. Our findings reveal an increase in F1 score of more than 21 points compared to standard training methods and more than 18 points compared to a state-of-the-art method in few-shot learning. Furthermore, the classifier trained with this method and 32 instances is only less than 5 F1 score points worse than a classifier trained with 1800 instances.
ER  - 

TY  - JOUR
T1  - Self-X-based secure human-cyber-physical system (SSHCPS) for autonomous manufacturing in the era of industry 5.0
AU  - Bajestani, Mahdi Sadeqi
AU  - Kim, Changjong
AU  - Lee, Kyung-Chang
AU  - Kim, Duck Bong
JO  - Advanced Engineering Informatics
VL  - 69
SP  - 104054
PY  - 2026
DA  - 2026/01/01/
SN  - 1474-0346
DO  - https://doi.org/10.1016/j.aei.2025.104054
UR  - https://www.sciencedirect.com/science/article/pii/S1474034625009474
KW  - Self-X
KW  - Human-in-the-loop
KW  - Cyber-Physical System
KW  - Autonomous Manufacturing
KW  - Industry 5.0
AB  - The transition from Industry 4.0 to Industry 5.0 marks a paradigm shift from technology-driven automation toward secure, resilient, and human-centric manufacturing. While Industry 4.0 enhanced efficiency through cyber-physical systems (CPS), the Internet of Things (IoT), and artificial intelligence (AI), it often overlooks human involvement and introduces heightened cybersecurity risks. Industry 5.0 seeks to overcome these limitations by emphasizing sustainability, resilience, and human-centricity through collaboration between humans and intelligent systems. As a step toward maturing the Industry 5.0 paradigm, we propose the Self-X-based secure human-cyber-physical system (SSHCPS) as a new conceptual framework for autonomous manufacturing. This study introduces an original architecture that integrates Self-X capabilities, human-in-the-loop (HITL) interaction, and cybersecurity (CS). The architecture is structured into four interlinked modules: HITL, Digital Twin, Physical Twin, and CS. As the foundation of this architecture, we categorized the Self-X terms from the literature, merged them into 23 Self-X principles, redefined them, and mapped them into the core values of Industry 5.0 and the SSHCPS modules. Furthermore, we demonstrate the applicability of SSHCPS through potential applications such as lightless and de-urbanized factories, humanoid-enabled SMEs, and the aerospace industry, while also identifying key technical challenges and future direction. This study establishes SSHCPS as a forward-looking foundation for secure, efficient, and human-centered autonomous manufacturing systems in the Industry 5.0 era.
ER  - 

TY  - JOUR
T1  - FHGraph: A Novel Framework for Fake News Detection Using Graph Contrastive Learning and LLM
AU  - Li, Yuanqing
AU  - Dai, Mengyao
AU  - Zhang, Sanfeng
JO  - Computers, Materials and Continua
VL  - 83
IS  - 1
SP  - 309
EP  - 333
PY  - 2025
DA  - 2025/03/26/
SN  - 1546-2218
DO  - https://doi.org/10.32604/cmc.2025.060455
UR  - https://www.sciencedirect.com/science/article/pii/S1546221825002991
KW  - Graph contrastive learning
KW  - fake news detection
KW  - data augmentation
KW  - class imbalance
KW  - LLM
AB  - Social media has significantly accelerated the rapid dissemination of information, but it also boosts propagation of fake news, posing serious challenges to public awareness and social stability. In real-world contexts, the volume of trustable information far exceeds that of rumors, resulting in a class imbalance that leads models to prioritize the majority class during training. This focus diminishes the model’s ability to recognize minority class samples. Furthermore, models may experience overfitting when encountering these minority samples, further compromising their generalization capabilities. Unlike node-level classification tasks, fake news detection in social networks operates on graph-level samples, where traditional interpolation and oversampling methods struggle to effectively generate high-quality graph-level samples. This challenge complicates the identification of new instances of false information. To address this issue, this paper introduces the FHGraph (Fake News Hunting Graph) framework, which employs a generative data augmentation approach and a latent diffusion model to create graph structures that align with news communication patterns. Using the few-sample learning capabilities of large language models (LLMs), the framework generates diverse texts for minority class nodes. FHGraph comprises a hierarchical multiview graph contrastive learning module, in which two horizontal views and three vertical levels are utilized for self-supervised learning, resulting in more optimized representations. Experimental results show that FHGraph significantly outperforms state-of-the-art (SOTA) graph-level class imbalance methods and SOTA graph-level contrastive learning methods. Specifically, FHGraph has achieved a 2% increase in F1 Micro and a 2.5% increase in F1 Macro in the PHEME dataset, as well as a 3.5% improvement in F1 Micro and a 4.3% improvement in F1 Macro on RumorEval dataset.
ER  - 

TY  - JOUR
T1  - A machine learning approach for detecting cybersecurity vulnerabilities in the internet of medical things
AU  - Oragwu, Ukamaka
AU  - Aziz, Benjamin
AU  - Rezvy, Shahadate
JO  - Knowledge-Based Systems
VL  - 327
SP  - 114150
PY  - 2025
DA  - 2025/10/09/
SN  - 0950-7051
DO  - https://doi.org/10.1016/j.knosys.2025.114150
UR  - https://www.sciencedirect.com/science/article/pii/S0950705125011918
KW  - Cybersecurity
KW  - Internet of medical devices
KW  - Machine learning
KW  - Open datasets
KW  - Vulnerability detection
AB  - Cyber incidents in healthcare are rising, targeting medical devices with outdated security. While networking medical devices improves performance, it also expands the attack surface, requiring urgent security measures. Our study introduces a novel approach to vulnerability assessment in networked medical devices using machine learning on the CICIoMT2024 dataset from the Canadian Institute for Cybersecurity. This dataset includes traffic data from 40 IoMT devices under 18 attack types (DDoS, DoS, Recon, MQTT, and spoofing) and normal lifecycle behaviour (power, idle, active, interaction). Its diverse protocols mirror real-world healthcare environments. We conducted a three-stage vulnerability assessment. First, we built a lifecycle classifier to predict device states under attack, with Random Forest achieving the highest accuracy (0.8915), precision (0.8905), recall (0.8915), and F1-score (0.8907). Second, we developed a device classifier to identify attack types per device, where Random Forest again performed best (accuracy: 0.9789, precision: 0.9791, recall: 0.9789, F1-score: 0.9789). Lastly, we analysed device vulnerabilities across lifecycle stages using visual plots. Our findings highlight the importance of lifecycle-aware security strategies to protect IoMT devices from evolving Cyber threats.
ER  - 

TY  - JOUR
T1  - Neurosymbolic learning and domain knowledge-driven explainable AI for enhanced IoT network attack detection and response
AU  - Kalutharage, Chathuranga Sampath
AU  - Liu, Xiaodong
AU  - Chrysoulas, Christos
JO  - Computers & Security
VL  - 151
SP  - 104318
PY  - 2025
DA  - 2025/04/01/
SN  - 0167-4048
DO  - https://doi.org/10.1016/j.cose.2025.104318
UR  - https://www.sciencedirect.com/science/article/pii/S0167404825000070
KW  - Neurosymbolic learning
KW  - Attack detection
KW  - Explainable artificial intelligence
KW  - Expert knowledge
KW  - Threat intelligence
AB  - In the dynamic landscape of network security, where cyberattacks continuously evolve, robust and adaptive detection mechanisms are essential, particularly for safeguarding Internet of Things (IoT) networks. This paper introduces an advanced anomaly detection model that utilizes Artificial Intelligence (AI) to identify network anomalies based on traffic features, explaining the most influential factors behind each detected anomaly. The model integrates domain knowledge stored in a knowledge graph to verify whether the detected anomaly constitutes a legitimate attack. Upon validation, the model identifies which core cybersecurity principles—Confidentiality, Integrity, or Availability (CIA)—are violated by mapping influential feature values. This is followed by an alignment with the MITRE ATT&CK framework to provide insights into potential attack tactics, techniques, and intelligence-driven countermeasures. By leveraging explainable AI (XAI) and incorporating expert domain knowledge, our approach bridges the gap between complex AI predictions and human-understandable decision-making, thereby enhancing both detection accuracy and result interpretability. This transparency facilitates faster responses and real-time decision-making while improving adaptability to new, unseen cyber threats. Our evaluation on network traffic datasets demonstrates that the model not only excels in detecting and explaining anomalies but also achieves an overall detection accuracy of 0.97 with the integration of domain knowledge for attack legitimacy. Furthermore, it provides 100% accuracy for threat intelligence based on the MITRE ATT&CK framework, ensuring that security measures are verifiable, actionable, and ultimately strengthen IoT environment defenses by delivering real-time threat intelligence and responses, thus minimizing human response time.
ER  - 

TY  - JOUR
T1  - ArtPerception: ASCII art-based jailbreak on LLMs with recognition pre-test
AU  - Yang, Guan-Yan
AU  - Cheng, Tzu-Yu
AU  - Teng, Ya-Wen
AU  - Wang, Farn
AU  - Yeh, Kuo-Hui
JO  - Journal of Network and Computer Applications
VL  - 244
SP  - 104356
PY  - 2025
DA  - 2025/12/01/
SN  - 1084-8045
DO  - https://doi.org/10.1016/j.jnca.2025.104356
UR  - https://www.sciencedirect.com/science/article/pii/S108480452500253X
KW  - Black-box attack
KW  - Large Language Models
KW  - Jailbreak attack
KW  - Security
KW  - Safety alignment
KW  - ASCII art
KW  - Natural language processing
KW  - Computer applications security
KW  - Testing methodology
KW  - Adversarial attacks
KW  - Adversarial prompting
KW  - Red teaming
KW  - Model robustness
AB  - The integration of Large Language Models (LLMs) into computer applications has introduced transformative capabilities but also significant security challenges. Existing safety alignments, which primarily focus on semantic interpretation, leave LLMs vulnerable to attacks that use non-standard data representations. This paper introduces ArtPerception, a novel black-box jailbreak framework that strategically leverages ASCII art to bypass the security measures of state-of-the-art (SOTA) LLMs. Unlike prior methods that rely on iterative, brute-force attacks, ArtPerception introduces a systematic, two-phase methodology. Phase 1 conducts a one-time, model-specific pre-test to empirically determine the optimal parameters for ASCII art recognition. Phase 2 leverages these insights to launch a highly efficient, one-shot malicious jailbreak attack. We propose a Modified Levenshtein Distance (MLD) metric for a more nuanced evaluation of an LLM’s recognition capability. Through comprehensive experiments on four SOTA open-source LLMs, we demonstrate superior jailbreak performance. We further validate our framework’s real-world relevance by showing its successful transferability to leading commercial models, including GPT-4o, Claude Sonnet 3.7, and DeepSeek-V3, and by conducting a rigorous effectiveness analysis against potential defenses such as LLaMA Guard and Azure’s content filters. Our findings underscore that true LLM security requires defending against a multi-modal space of interpretations, even within text-only inputs, and highlight the effectiveness of strategic, reconnaissance-based attacks. Content Warning: This paper includes potentially harmful and offensive model outputs.
ER  - 

TY  - JOUR
T1  - Retentive network-based time series anomaly detection in cyber-physical systems
AU  - Min, Zhaoyi
AU  - Xiao, Qianqian
AU  - Abbas, Muhammad
AU  - Zhang, Duanjin
JO  - Engineering Applications of Artificial Intelligence
VL  - 145
SP  - 110215
PY  - 2025
DA  - 2025/04/01/
SN  - 0952-1976
DO  - https://doi.org/10.1016/j.engappai.2025.110215
UR  - https://www.sciencedirect.com/science/article/pii/S0952197625002155
KW  - Retentive network
KW  - Time series anomaly detection
KW  - Convolution neural network
KW  - Transformer
KW  - Dynamic threshold
KW  - Cyber-physical systems
AB  - Time series data are ubiquitous in the operation of cyber-physical systems (CPS), encompassing network traffic data, sensor measurements, and other relevant data streams. Intelligent anomaly detection methods are crucial for identifying deviations in these time series, which can enhance system maintainability and reliability. In this study, we propose an unsupervised reconstruction-based anomaly detection method utilizing retentive network (RetNet), a novel variant of Transformer. A one-dimensional convolutional neural network is employed to map the raw time series into the RetNet model space, analogous to word embedding techniques in natural language processing. The rotary position embedding introduced in RetNet can simultaneously incorporate absolute and relative positional information, which is beneficial for modeling temporal dependency in time series. Meanwhile, the multi-scale retention mechanism of RetNet facilitates the learning of informative representations of the dominant normal patterns from training data. The observed anomalies result in larger reconstruction errors, which are subsequently detected by the peaks-over-threshold (POT) method using a dynamic threshold. We evaluate the proposed method on four benchmark datasets. Experimental results demonstrate that the proposed method outperforms the best baseline by 4.07% in terms of the F1 score.
ER  - 

TY  - JOUR
T1  - Voting-based ensemble classifiers model on ransomware detection for cybersecurity driven iiot in cloud computing infrastructure
AU  - Alhayan, Fatimah
AU  - Abdullah, Monir
AU  - Alshuhail, Asma
AU  - Arasi, Munya A.
AU  - Alrusaini, Othman
AU  - Alahmari, Sultan
AU  - Yahya, Abdulsamad Ebrahim
AU  - Zanin, Samah Al
JO  - Alexandria Engineering Journal
VL  - 129
SP  - 1198
EP  - 1211
PY  - 2025
DA  - 2025/10/01/
SN  - 1110-0168
DO  - https://doi.org/10.1016/j.aej.2025.08.028
UR  - https://www.sciencedirect.com/science/article/pii/S1110016825009251
KW  - Industrial Internet of Things
KW  - Cybersecurity
KW  - Ransomware detection
KW  - Ensemble classifier
KW  - Walrus optimization
AB  - The smart factory environment was converted into an Industrial Internet of Things (IIoT) environment because it is an open approach and interconnected. This has made smart manufacturing plants susceptible to cyberattacks and has openly led to real damage. Many cyberattacks targeting smart factories were controlled using malware. So, a solution that effectively identifies malware by analyzing and monitoring network traffic for malware threats in a smart factory IIoT environment is vital. However, attaining precise real malware recognition in such environments was challenging. Ransomware is a kind of malware that encodes the victim's data and demands payment to restore access. The effective recognition of ransomware attacks is highly based on how its features are learned and how accurately its activities are recognized. This article proposes a Voting-Based Ensemble Classifiers Model on Ransomware Detection for Cybersecurity (VBECM-RDCS) technique for IIoT in cloud computing infrastructure. The VBECM-RDCS technique utilizes the squirrel search algorithm (SSA) model for feature subset selection. Furthermore, a voting ensemble classifier for ransomware detection employs the convolutional autoencoder (CAE) integrated with bidirectional gated recurrent unit (Bi-GRU). Finally, the walrus optimization algorithm (WAOA) model is implemented for optimum hyperparameter tuning to improve the recognition performance of ensemble methods. The simulation study of the VBECM-RDCS technique is examined under the ransomware detection dataset. The VBECM-RDCS technique attained a superior accuracy value of 99.76 % under 2000 training epochs, outperforming existing models in the experimental evaluation.
ER  - 

TY  - JOUR
T1  - Automatic assertion generation with LLM for ICS program verification
AU  - Yang, Kai
AU  - Li, Fugao
AU  - Li, Ting
AU  - Zhang, Yingjun
AU  - Sun, Limin
JO  - Journal of Systems and Software
VL  - 232
SP  - 112659
PY  - 2026
DA  - 2026/02/01/
SN  - 0164-1212
DO  - https://doi.org/10.1016/j.jss.2025.112659
UR  - https://www.sciencedirect.com/science/article/pii/S0164121225003280
KW  - Assertion generation
KW  - Industrial control systems
KW  - Large language models
KW  - Formal verification
KW  - Program security
AB  - Industrial Control System (ICS) programs are critical to industrial automation However, many ICS programs suffer from insufficient verification and pervasive logic flaws, posing significant risks to both system integrity and human safety. Existing ICS program verification approaches rely heavily on predefined logical assertions derived from expert knowledge, utilizing formal verification or testing-based methods to ensure correctness. This process is labor-intensive, requiring substantial manual effort and specialized expertise, which hampers automation and scalability. In this paper, we present a novel approach to automated assertion generation for ICS program verification, leveraging the critical insight that ICS execution logic is often documented in system specifications and user manuals. These documents can be used as constraints for validating logical correctness of ICS program. In particular, we abstract the constraints from ICS specification using large language models and map the variable names from these constraints to ICS program, enabling the automatic generation of ICS assertions. We implemented a prototype of our approach and conducted real-world experiments to demonstrate its effectiveness under 155 PLC projects. The results showed that our proposed method successfully generated 1251 assertions, achieving the correctness rate of 88.5%. Furthermore, we performed comparative experiments using different models to highlight the hardware efficiency and robustness of our solution.
ER  - 

TY  - JOUR
T1  - Advancing software security: DCodeBERT for automatic vulnerability detection and repair
AU  - Bensaoud, Ahmed
AU  - Kalita, Jugal
JO  - Journal of Industrial Information Integration
VL  - 45
SP  - 100834
PY  - 2025
DA  - 2025/05/01/
SN  - 2452-414X
DO  - https://doi.org/10.1016/j.jii.2025.100834
UR  - https://www.sciencedirect.com/science/article/pii/S2452414X25000585
KW  - Large Language Models
KW  - Adversarial attacks
KW  - Vulnerability detection
KW  - Data privacy
KW  - Natural language processing
AB  - The exponential growth of software complexity has led to a corresponding increase in software vulnerabilities, necessitating robust methods for automatic vulnerability detection and repair. This paper proposes DCodeBERT, a large language model (LLM) fine-tuned for vulnerability detection and repair in software code. Leveraging the pre-trained CodeBERT model, DCodeBERT is designed to understand both natural language and programming language context, enabling it to effectively identify vulnerabilities and suggest repairs. We conduct experiments to evaluate DCodeBERT’s performance, comparing it against several baseline models. The results demonstrate that DCodeBERT outperforms the baselines in both vulnerability detection and repair tasks across multiple programming languages, showcasing its effectiveness in enhancing software security.
ER  - 

TY  - JOUR
T1  - CTIMD: Cyber threat intelligence enhanced malware detection using API call sequences with parameters
AU  - Chen, Tieming
AU  - Zeng, Huan
AU  - Lv, Mingqi
AU  - Zhu, Tiantian
JO  - Computers & Security
VL  - 136
SP  - 103518
PY  - 2024
DA  - 2024/01/01/
SN  - 0167-4048
DO  - https://doi.org/10.1016/j.cose.2023.103518
UR  - https://www.sciencedirect.com/science/article/pii/S0167404823004285
KW  - Malware detection
KW  - API sequence
KW  - Cyber threat intelligence
KW  - Deep learning
AB  - Dynamic malware analysis that monitors the sequences of API calls of the program in a sandbox has been proven to be effective against code obfuscation and unknown malware. However, most existing works ignore the run-time parameters by only considering the API names, or lack an effective way to capture the correlations between parameter values and malicious activities. In this paper, we propose CTIMD, a deep learning based dynamic malware detection method, which integrates the threat knowledge from CTIs (Cyber Threat Intelligences) into the learning on API call sequences with run-time parameters. It first extracts IOCs (Indicators of Compromise) from CTIs and uses IOCs to assist the identification of the security-sensitive levels of API calls. Then, it embeds API calls and the associated security-sensitive levels into a unified feature space. Finally, it feeds the feature vector sequences into deep neural networks to train the malware detection model. We conducted experiments on two datasets. The experiment results show that CTIMD significantly outperforms existing methods depending on raw API call sequences (F1-score is improved by 4.0 %∼41.3 %), and also has advantage over existing state-of-the-art methods that consider both API calls and run-time parameters (F1-score is improved by 1.2 %∼6.5 %).
ER  - 

TY  - JOUR
T1  - Human-guided collective LLM intelligence for strategic planning via two-stage information retrieval
AU  - Kim, Sangyeop
AU  - Ha, Junguk
AU  - Lee, Hangyeul
AU  - Park, Sohhyung
AU  - Cho, Sungzoon
JO  - Information Processing & Management
VL  - 63
IS  - 1
SP  - 104288
PY  - 2026
DA  - 2026/01/01/
SN  - 0306-4573
DO  - https://doi.org/10.1016/j.ipm.2025.104288
UR  - https://www.sciencedirect.com/science/article/pii/S0306457325002298
KW  - Collective intelligence
KW  - Large language models
KW  - Strategic planning
KW  - Human-AI collaboration
KW  - Text generation
KW  - Information retrieval
AB  - Modern businesses face increasing challenges in strategic planning due to the immense volume of digital information. The rapid growth of available data sources – from market trends and competitor activities to real-time economic indicators – makes comprehensive analysis within tight timeframes arduous. To address these challenges, large language models (LLMs) have emerged as potential tools, efficiently analyzing extensive information across diverse domains. However, LLMs face critical limitations: they cannot access proprietary information or real-time data and cannot engage in collaborative refinement processes that human experts traditionally use to develop and improve strategic analyses. This study introduces the Collective Intelligence of AI Consultants (CIAIC) framework, where specialized AI agents function as individual consultants, collaborating like a consulting team to enhance strategic analysis. The framework combines real-time data integration with collaborative AI mechanisms in a five-stage process: (1) human-guided objective definition, (2) retrieval-augmented draft generation, (3) supplementary data retrieval through multi-agents, (4) draft revision via collective intelligence, and (5) multi-perspective strategic plan compilation. Experimental evaluations using PESTEL and SWOT analyses demonstrate the effectiveness of this collective approach through both quantitative metrics and human preference assessments.
ER  - 

TY  - JOUR
T1  - DeTinyLLM: Efficient detection of machine-generated text via compact paraphrase transformation
AU  - Tan, Shilei
AU  - Zhou, Yongcheng
AU  - Liu, Haoxiang
AU  - Wang, Xuesong
AU  - Chen, Si
AU  - Gong, Wei
JO  - Information Fusion
VL  - 127
SP  - 103713
PY  - 2026
DA  - 2026/03/01/
SN  - 1566-2535
DO  - https://doi.org/10.1016/j.inffus.2025.103713
UR  - https://www.sciencedirect.com/science/article/pii/S1566253525007675
KW  - Machine-generated text detection
KW  - Text information fusion
KW  - DeTinyLLM
KW  - Paraphrase transformation
AB  - The growing fusion of human-written and machine-generated text poses significant challenges in distinguishing their origins, as advanced large language models (LLMs) increasingly mimic human linguistic patterns. Existing detection methods, such as SimLLM, rely on querying proprietary LLMs for proofreading to measure similarity, which incurs high computational costs and instability due to dependency on fluctuating model updates. To address these limitations, we propose DeTinyLLM, a novel framework that leverages fusion-driven compact paraphrase models for efficient and stable detection. First, we train a lightweight transformation model (e.g., fine-tuned T5-large) to rewrite machine-generated text into human-like text, effectively “de-AI-ifying” it through iterative fusion of syntactic and semantic features. For detection, the input text and its rewritten version are fused and classified via a hybrid neural network, capitalizing on divergence patterns between human and machine text. Experiments across diverse datasets demonstrate that DeTinyLLM achieves state-of-the-art accuracy (surpassing SimLLM by 4.3 % in ROC-AUC) while reducing inference latency by 77.2 %. By eliminating reliance on proprietary LLMs and integrating multi-level fusion of linguistic signals, this work advances scalable, cost-effective solutions for real-world deployment in AI-generated text detection systems.
ER  - 

TY  - JOUR
T1  - Automated discovery and mapping ATT&CK tactics and techniques for unstructured cyber threat intelligence
AU  - Li, Lingzi
AU  - Huang, Cheng
AU  - Chen, Junren
JO  - Computers & Security
VL  - 140
SP  - 103815
PY  - 2024
DA  - 2024/05/01/
SN  - 0167-4048
DO  - https://doi.org/10.1016/j.cose.2024.103815
UR  - https://www.sciencedirect.com/science/article/pii/S0167404824001160
KW  - Cyber threat intelligence
KW  - MITRE ATT&CK
KW  - Multi-label classification
KW  - Network security
AB  - As cyber attacks are growing, Cyber Threat Intelligence (CTI) enhances the ability of security systems to resist novel cyber threats. However, since most CTI is unstructured data written in natural language, it needs to be understood and summarized by security experts to be effectively utilized. To address the problem, we adopt the ATT&CK matrix as the taxonomy to propose a method for automated mapping of unstructured threat intelligence to tactics and techniques. The proposed method contains a pre-processor for text denoising, a label extractor for classifying which tactics and techniques category the text belongs to, and a post-processor for correcting the classification results. The label extractor consists of two multi-label classifiers based on DistilBERT for tactics and techniques classification respectively. The post-processor corrects the classification results based on the relations between tactics, techniques, and sub-techniques in the matrix, eliminating errors caused by the independence between categories. In the evaluation, we collect the text data from the ATT&CK knowledge base and real cyber threat reports to build an experiment dataset, which contains 26,602 sentence samples. We apply the proposed method to the dataset to verify its effectiveness. The results show that the proposed method can accurately retrieve tactics and techniques with F0.5 score of 85.50% and 75.17% respectively, which outperforms the baseline method by about 10%.
ER  - 

TY  - JOUR
T1  - On-device derivation of IoT usage control policies: Automating U-XACML policy generation from natural language with LLMs in smart homes environments
AU  - Alajramy, Loay
AU  - Simoni, Marco
AU  - Rasori, Marco
AU  - Saracino, Andrea
AU  - Mori, Paolo
JO  - Future Generation Computer Systems
VL  - 175
SP  - 108067
PY  - 2026
DA  - 2026/02/01/
SN  - 0167-739X
DO  - https://doi.org/10.1016/j.future.2025.108067
UR  - https://www.sciencedirect.com/science/article/pii/S0167739X25003620
KW  - Internet of Things
KW  - Usage control
KW  - Access control
KW  - LLM
KW  - Smart home
KW  - On-device AI
AB  - In this paper, we present a framework that integrates AI-based derivation of Access and Usage Control policies for IoT devices, using Large Language Models (LLMs) to automate the generation of policies from unstructured natural language commands. The framework employs a hybrid approach, combining LLMs with dedicated libraries to ensure efficient on-device execution. Our approach is based on a two-step process: first, a fine-tuned LLM converts user commands into structured JSON policy representations; then, a transformation module translates the JSON policies into fully compliant U-XACML policies. To ensure generality across different domains, we introduce a taxonomy-driven dataset creation, which enables policy creation for different environments such as smart homes, smart offices, and healthcare settings. Our evaluation demonstrates that the system achieves 93 % accuracy in policy generation and 91 % accuracy when handling ambiguous or noisy inputs. It also reaches 98 % agreement with expert-defined policies in real-world scenarios. Finally, on-device performance evaluations confirm the feasibility of running the model in practical settings, demonstrating reliable inference under constrained hardware conditions.
ER  - 

TY  - JOUR
T1  - Anomaly diagnosis of connected autonomous vehicles: A survey
AU  - Fang, Yukun
AU  - Min, Haigen
AU  - Wu, Xia
AU  - Wang, Wuqi
AU  - Zhao, Xiangmo
AU  - Martinez-Pastor, Beatriz
AU  - Teixeira, Rui
JO  - Information Fusion
VL  - 105
SP  - 102223
PY  - 2024
DA  - 2024/05/01/
SN  - 1566-2535
DO  - https://doi.org/10.1016/j.inffus.2024.102223
UR  - https://www.sciencedirect.com/science/article/pii/S1566253524000010
KW  - Connected autonomous vehicles
KW  - Anomaly diagnosis
KW  - Anomaly detection
KW  - Anomaly interpretation
KW  - Road vehicle safety
AB  - Connected autonomous vehicles (CAVs) are revolutionizing the development of transportation due to their potential to improve transportation performance in many ways, such as enhanced traffic mobility, road compacity, operation safety, and environmental sustainability. Nevertheless, the issue of road vehicle safety in CAVs remains to be fully solved. Data collected from multiple sources provide information about the internal status of the system and the situation of its surroundings, and the occurrence of data anomalies indicates the existence of potential safety risks. Thus, anomaly diagnosis is of major importance to analyze the nature or cause of underlying safety risks and provide insightful information for the subsequent decision-making that ensures safety. Anomaly diagnosis consists of two basic tasks: anomaly detection and anomaly interpretation. In this paper, both of these two tasks are comprehensively surveyed. For anomaly detection, the following four aspects are covered: 1) formalized definition of an anomaly, 2) classification of anomalies, 3) taxonomies of anomaly detection techniques, and 4) review of recent advances for anomaly detection in CAV applications. For anomaly interpretation, related works are investigated in the context of 1) the anomaly detection process, and 2) the tested/monitored system/process, respectively. The novelty particularly lies in the latter, where the interpretation of anomalies combining the analysis of road vehicle safety risks is presented, and related works for anomaly interpretation in CAV applications are reviewed by analyzing 1) functional safety risks, 2) safety of the intended functionality (SOTIF) risks, and 3) cyber security risks, respectively. Finally, open issues, challenges, future directions, and emerging technologies for anomaly diagnosis in CAVs are discussed.
ER  - 

TY  - JOUR
T1  - Cybersecurity-aware log management system for critical water infrastructures
AU  - Dural Balta, Deniz
AU  - Balta Kaç, Seda
AU  - Balta, Musa
AU  - Oğur, Nur Banu
AU  - Eken, Süleyman
JO  - Applied Soft Computing
VL  - 169
SP  - 112613
PY  - 2025
DA  - 2025/01/01/
SN  - 1568-4946
DO  - https://doi.org/10.1016/j.asoc.2024.112613
UR  - https://www.sciencedirect.com/science/article/pii/S1568494624013875
KW  - Critical infrastructure protection
KW  - Security analytics
KW  - Cybersecurity-aware log management
KW  - Big data
KW  - Machine learning
AB  - Cyber threats are increasingly targeting critical water infrastructures, requiring robust cybersecurity measures to ensure the continuous and safe delivery of water services. This paper presents a comprehensive, cybersecurity-aware log management system specifically designed for critical water infrastructures. The system leverages advanced data collection, analysis, and real-time monitoring to effectively detect and mitigate cyber threats. Key features include integration with existing infrastructure, scalability to handle large volumes of log data, and machine learning algorithms for enhanced threat detection and response. Our solution demonstrated significant improvements in threat detection accuracy, response times, and overall system resilience through rigorous testing in real-world scenarios. This paper discusses the design, implementation, and performance evaluation of the proposed log management system, highlighting its potential to strengthen the cybersecurity posture of critical water infrastructures.
ER  - 

TY  - JOUR
T1  - Evaluating spam filters and Stylometric Detection of AI-generated phishing emails
AU  - Opara, Chidimma
AU  - Modesti, Paolo
AU  - Golightly, Lewis
JO  - Expert Systems with Applications
VL  - 276
SP  - 127044
PY  - 2025
DA  - 2025/06/01/
SN  - 0957-4174
DO  - https://doi.org/10.1016/j.eswa.2025.127044
UR  - https://www.sciencedirect.com/science/article/pii/S0957417425006669
KW  - AI-generated phishing email
KW  - Phishing detection
KW  - Stylometric analysis
KW  - Large Language Models (LLMs)
KW  - Machine learning
KW  - Cybersecurity
AB  - The advanced architecture of Large Language Models (LLMs) has revolutionised natural language processing, enabling the creation of text that convincingly mimics legitimate human communication, including phishing emails. As AI-generated phishing emails become increasingly sophisticated, a critical question arises: How effectively can current email systems and detection mechanisms identify these threats? This study addresses this issue by analysing 63 AI-generated phishing emails created using GPT-4o. It evaluates the effectiveness of major email services, Gmail, Outlook, and Yahoo, in filtering these malicious communications. The findings reveal that Gmail and Outlook allowed more AI-generated phishing emails to bypass their filters compared to Yahoo, highlighting vulnerabilities in existing email filtering systems. To mitigate these challenges, we applied 60 stylometric features across four machine learning models: Logistic Regression, Support Vector Machine, Random Forest, and XGBoost. Among these, XGBoost demonstrated superior performance, achieving 96% accuracy and an AUC score of 99%. Key features such as imperative verb count, clause density, and first-person pronoun usage were instrumental to the model’s success. The dataset of AI-generated phishing emails is publicly available on Kaggle to foster further research.
ER  - 

TY  - JOUR
T1  - Malware of Dynamic Behavior and Attack Patterns Using ATT&CK Framework
AU  - Kuo, Jong-Yih
AU  - Wang, Ping-Feng
AU  - Hsieh, Ti-Feng
AU  - Kuo, Cheng-Hsuan
JO  - CMES - Computer Modeling in Engineering and Sciences
VL  - 143
IS  - 3
SP  - 3133
EP  - 3166
PY  - 2025
DA  - 2025/06/30/
SN  - 1526-1492
DO  - https://doi.org/10.32604/cmes.2025.064104
UR  - https://www.sciencedirect.com/science/article/pii/S1526149225001602
KW  - Linux malware
KW  - dynamic analysis
KW  - behavior analysis
KW  - behavioral feature
KW  - ATT&CK
KW  - sandbox
KW  - large language model
KW  - fine-tuning
AB  - In recent years, cyber threats have escalated across diverse sectors, with cybercrime syndicates increasingly exploiting system vulnerabilities. Traditional passive defense mechanisms have proven insufficient, particularly as Linux platforms—historically overlooked in favor of Windows—have emerged as frequent targets. According to Trend Micro, there has been a substantial increase in Linux-targeted malware, with ransomware attacks on Linux surpassing those on macOS. This alarming trend underscores the need for detection strategies specifically designed for Linux environments. To address this challenge, this study proposes a comprehensive malware detection framework tailored for Linux systems, integrating dynamic behavioral analysis with the semantic reasoning capabilities of large language models (LLMs). Malware samples are executed within sandbox environments to extract behavioral features such as system calls and command-line executions. These features are then systematically mapped to the MITRE ATT&CK framework, incorporating its defined data sources, data components, and Tactics, Techniques, and Procedures (TTPs). Two mapping constructs—Conceptual Definition Mapping and TTP Technical Keyword Mapping—are developed from official MITRE documentation. These resources are utilized to fine-tune an LLM, enabling it to semantically interpret complex behavioral patterns and infer associated attack techniques, including those employed by previously unknown malware variants. The resulting detection pipeline effectively bridges raw behavioral data with structured threat intelligence. Experimental evaluations confirm the efficacy of the proposed system, with the fine-tuned Gemma 2B model demonstrating significantly enhanced accuracy in associating behavioral features with ATT&CK-defined techniques. This study contributes a fully integrated Linux-specific detection framework, a novel approach for transforming unstructured behavioral data into actionable intelligence, improved interpretability of malicious behavior, and a scalable training process for future applications of LLMs in cybersecurity.
ER  - 
