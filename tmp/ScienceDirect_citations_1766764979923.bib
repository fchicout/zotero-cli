@article{JIANG2026111940,
title = {Dual similarity enhanced hybrid orthogonal fusion for multimodal named entity recognition},
journal = {Pattern Recognition},
volume = {169},
pages = {111940},
year = {2026},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2025.111940},
url = {https://www.sciencedirect.com/science/article/pii/S0031320325006004},
author = {Chunmao Jiang and Yongpeng Wang and Baoping Xiong},
keywords = {Multimodal named entity recognition, Hybrid feature, Dual similarity, Orthogonal fusion},
abstract = {With the rise of multimodal content on social media, Multimodal Named Entity Recognition (MNER) has become vital. Previous studies have improved text-based recognition by incorporating visual information but often struggle with semantic degradation during cross-modal learning and feature redundancy in fusion. To address these issues, we propose a Dual Similarity Enhanced Hybrid Orthogonal Fusion (DSE-HOF) network for MNER, using a semantic constraint strategy with self-attention and cross-modal attention, which effectively captures semantic relations within and between modalities, producing robust hybrid features. Specifically, we introduce a word-level similarity to filter relevant image regions and a modal-level similarity to control the integration of visual features. Extensive experiments on two widely-used MNER datasets demonstrate the effectiveness of our approach. Our model achieves average improvements of 1.398% and 1.681% in overall F1 scores compared to state-of-the-art baselines on the Twitter-2015 and Twitter-2017 datasets, respectively. Further ablation studies and qualitative analyses validate the contribution of each component in our proposed framework.}
}
@article{SOMU2020114131,
title = {A hybrid model for building energy consumption forecasting using long short term memory networks},
journal = {Applied Energy},
volume = {261},
pages = {114131},
year = {2020},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2019.114131},
url = {https://www.sciencedirect.com/science/article/pii/S0306261919318185},
author = {Nivethitha Somu and Gauthama Raman {M R} and Krithi Ramamritham},
keywords = {Buildings, Energy consumption, Forecasting models, Artificial intelligence, Neural networks, Optimization},
abstract = {Data driven building energy consumption forecasting models play a significant role in enhancing the energy efficiency of the buildings through building energy management, energy operations, and control strategies. The multi-source and heterogeneous energy consumption data necessitates the integration of evolutionary algorithms and data-driven models for better forecast accuracy and robustness. We present eDemand, an energy consumption forecasting model which employs long short term memory networks and improved sine cosine optimization algorithm for accurate and robust building energy consumption forecasting. A novel Haar wavelet based mutation operator was introduced to enhance the divergence nature of sine cosine optimization algorithm towards the global optimal solution. Further, the hyperparameters (learning rate, weight decay, momentum, and number of hidden layers) of the LSTM were optimized using the improved sine cosine optimization algorithm. A case study on the real-time energy consumption data obtained from Kanwal Rekhi building, an academic building at Indian Institute of Technology, Bombay for short, mid, and long-term forecasting. Experiments reveal that the proposed model outperforms the state-of-the-art energy consumption forecast models in terms of mean absolute error, mean absolute percentage error, mean square error, root mean square error, and Theil statistics. It is shown that stable and accurate forecast results are produced by ISCOA-LSTM and hence it can be used as an efficient tool for solving energy consumption forecast problems.}
}
@article{WANG2025104263,
title = {A systematic mapping study of online health communities in the field of Information Science},
journal = {Information Processing & Management},
volume = {62},
number = {6},
pages = {104263},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2025.104263},
url = {https://www.sciencedirect.com/science/article/pii/S0306457325002043},
author = {Xi Wang and Wenjie Pan and Hui Li and Yuan Jia and Zhiya Zuo},
keywords = {Online health communities, Systematic mapping study, Social support, User engagement},
abstract = {Studies on online health communities (OHCs) have proliferated within the field of Information Science over the past decade. OHC studies have seen the adoption of a variety of theoretical frameworks, advanced computational methodologies, empirical models, and online/field experiments, aiming to analyze user behavior and improve platform design. Despite their growing importance, OHCs face significant challenges including diminished socio-emotional communication, poor user retention, and privacy vulnerabilities. A comprehensive systematic mapping study (SMS) is essential to identify recurring patterns across studies, synthesize insights on social support mechanisms, and inform evidence-based strategies for healthcare providers, platform designers, and policymakers to enhance OHCs’ effectiveness. This study presents a SMS synthesizing the focal points, theories, and methods used in this field over the past decade. Based on 218 journal articles from the Information Science community, we highlight the role of OHCs as a vital platform for health information exchange and user engagement, while also identifying prevailing trends and challenges. Underscoring the interdisciplinary essence of OHC research, our main findings emphasize the necessity for methodological improvements and system design enhancements to fully realize the utility of OHCs.}
}
@article{PIZZI2025104615,
title = {VirtualPatch: Distributing Android security patches through Android virtualization},
journal = {Computers & Security},
volume = {157},
pages = {104615},
year = {2025},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2025.104615},
url = {https://www.sciencedirect.com/science/article/pii/S0167404825003049},
author = {Simeone Pizzi and Samuele Doria and Nicholas Miazzo and Eleonora Losiouk},
keywords = {Android security, Application-level virtualization, Security patch deployment, Android hooking techniques, Android user protection},
abstract = {The Android Operating System (OS) is a complex system that might contain vulnerabilities and allow malicious apps to damage the legitimate ones on the same device or steal sensitive user data. Vulnerabilities in the Android OS are fixed through security patches that can only be distributed through an update of the whole OS. Google is responsible for the development of security patches for the official Android platform i.e., the Android Open Source Project (AOSP). However, several other mobile vendors (e.g., Samsung, Xiaomi) sell smartphones running a customized version of AOSP and are responsible for integrating the AOSP security patches into their custom OS. This integration should occur before Google makes a vulnerability and the associated security patch public. Unfortunately, this is not always the case: we have found that the median time that Samsung requires to integrate a security patch is 35 days. This is astonishing and confirms the urgent need for a solution. In this paper, we propose VirtualPatch, a solution that allows the development of security patches and their immediate distribution on any Android device without involving mobile vendors. VirtualPatch creates a virtual environment through the Android virtualization technique and executes the target app with security patches inside it. To evaluate VirtualPatch, we selected 25 Android CVEs. For each of them, we developed the exploit and the security patch through VirtualPatch, and we then proved that the latter could prevent the former. We successfully implemented security patches for all the 25 Android CVEs and measured the additional overhead introduced by VirtualPatch at the startup time and at runtime. Finally, we conducted a user study with 29 participants to evaluate VirtualPatch usability.}
}
@article{SU2025841,
title = {A methodology for estimating the cost of a digital twin},
journal = {Journal of Manufacturing Systems},
volume = {80},
pages = {841-858},
year = {2025},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2025.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S0278612525000937},
author = {Shuo Su and Aydin Nassehi and Adam McClenaghan and Andrew Langridge and Ben Hicks},
keywords = {Digital twin, Cost estimation model, Digital twin data, Activity-based costing},
abstract = {This paper proposes a methodology for estimating the cost of developing digital twins (DTs) in manufacturing processes. It formulates a cost model that identifies main cost elements and presents the estimation process for establishing an acceptable cost reference based on a given set of physical information entities as DT inputs. To achieve this, six core data activities are derived from the ISO 23247 DT reference framework and the Digital Twin Data concept to characterize the functioning of DTs from a data perspective. These activities are data gathering, data interaction, data storage, data processing, data servitization, and data maintenance. The activity-based costing (ABC) method is applied to allocate six resources in the development of DTs (personnel, machine, equipment, material, facility, and service) to these data-intensive activities. The resultant cost structure comprises 40 cost activities, along with associated quantitative metrics. This work presents a case study on developing a DT for estimating the dimensional accuracy in the MEX process, where thermal and acceleration measurements are considered. For the information set with extruder and build plate temperatures as well as X- and Y-axis acceleration, developing a DT is estimated to cost between £4780 (for one temperature signal) and £39,285 (for two temperature signals and two acceleration signals) for two years of service and one year of data archiving. In addition, the cost distribution across four categories (IT infrastructure, resource, data activity, and investment) are analysed. The derived insights can support cost-related analysis, physical information entity selection, budget control, and standard open databases for DT costs.}
}
@article{ABDULLAH2024100218,
title = {The impact of artificial intelligence and Industry 4.0 on transforming accounting and auditing practices},
journal = {Journal of Open Innovation: Technology, Market, and Complexity},
volume = {10},
number = {1},
pages = {100218},
year = {2024},
issn = {2199-8531},
doi = {https://doi.org/10.1016/j.joitmc.2024.100218},
url = {https://www.sciencedirect.com/science/article/pii/S219985312400012X},
author = {Abdulwahid Ahmad Hashed Abdullah and Faozi A. Almaqtari},
keywords = {Artificial Intelligence, Industry 4.0 readiness, Technology acceptance model, Accounting education, Auditing practices, Accounting practices},
abstract = {The main aim is to investigate the impact of artificial intelligence (AI), Industry 4.0 readiness, and Technology Acceptance Model (TAM) variables on various aspects of accounting and auditing operations. To evaluate the associations between the variables, the research design employs a mediation and path approach using SMART PLS. The study employs a convenience sampling method, which is augmented with snowball sampling. The sample size was determined using various techniques, yielding a final sample of 228 respondents. The findings indicate that leveraging AI, big data analytics, cloud computing, and deep learning advancements can improve accounting and auditing practices. AI technologies assist businesses in increasing their efficiency, accuracy, and decision-making capabilities, resulting in improved financial reporting and auditing processes. The study contributes to the theoretical explanation of the influence of AI adoption in accounting and auditing practices in the context of an emerging country, Saudi Arabia. The findings of the study have practical implications for accounting and auditing practitioners, policymakers, and scholars. The findings of this study can assist businesses in efficiently leveraging AI developments to improve their accounting and auditing operations. Policymakers can use the findings to create supporting frameworks and regulations that encourage the adoption and integration of artificial intelligence in the domain. These findings contribute to the existing stock of knowledge on the use of AI in accounting and auditing, as well as providing evidence of its benefits in the context of an emerging country.}
}
@article{SUBRAMANIAN2024,
title = {Patient Health Record Protection Beyond the Health Insurance Portability and Accountability Act: Mixed Methods Study},
journal = {Journal of Medical Internet Research},
volume = {26},
year = {2024},
issn = {1438-8871},
doi = {https://doi.org/10.2196/59674},
url = {https://www.sciencedirect.com/science/article/pii/S1438887124007544},
author = {Hemang Subramanian and Arijit Sengupta and Yilin Xu},
keywords = {security, privacy, security breach, breach report, health care, health care infrastructure, regulatory, law enforcement, Omnibus Rule, qualitative analysis, AI-generated data, artificial intelligence, difference-in-differences, best practice, data privacy, safe practice},
abstract = {Background
The security and privacy of health care information are crucial for maintaining the societal value of health care as a public good. However, governance over electronic health care data has proven inefficient, despite robust enforcement efforts. Both federal (HIPAA [Health Insurance Portability and Accountability Act]) and state regulations, along with the ombudsman rule, have not effectively reduced the frequency or impact of data breaches in the US health care system. While legal frameworks have bolstered data security, recent years have seen a concerning increase in breach incidents. This paper investigates common breach types and proposes best practices derived from the data as potential solutions.
Objective
The primary aim of this study is to analyze health care and hospital breach data, comparing it against HIPAA compliance levels across states (spatial analysis) and the impact of the Omnibus Rule over time (temporal analysis). The goal is to establish guidelines for best practices in handling sensitive information within hospitals and clinical environments.
Methods
The study used data from the Department of Health and Human Services on reported breaches, assessing the severity and impact of each breach type. We then analyzed secondary data to examine whether HIPAA’s storage and retention rule amendments have influenced security and privacy incidents across all 50 states. Finally, we conducted a qualitative analysis of textual data from vulnerability and breach reports to identify actionable best practices for health care settings.
Results
Our findings indicate that hacking or IT incidents have the most significant impact on the number of individuals affected, highlighting this as a primary breach category. The overall difference-in-differences trend reveals no significant reduction in breach rates (P=.50), despite state-level regulations exceeding HIPAA requirements and the introduction of the ombudsman rule. This persistence in breach trends implies that even strengthened protections and additional guidelines have not effectively curbed the rising number of affected individuals. Through qualitative analysis, we identified 15 unique values and associated best practices from industry standards.
Conclusions
Combining quantitative and qualitative insights, we propose the “SecureSphere framework” to enhance data security in health care institutions. This framework presents key security values structured in concentric circles: core values at the center and peripheral values around them. The core values include employee management, policy, procedures, and IT management. Peripheral values encompass the remaining security attributes that support these core elements. This structured approach provides a comprehensive security strategy for protecting patient health information and is designed to help health care organizations develop sustainable practices for data security.}
}
@article{CAO2026108046,
title = {EADRAN: An edge marketplace for federated learning},
journal = {Future Generation Computer Systems},
volume = {175},
pages = {108046},
year = {2026},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2025.108046},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X25003413},
author = {Tien-Dung Cao and Hong-Tri Nguyen and Minh-Tri Nguyen and Tram Truong-Huu and Hong-Linh Truong},
keywords = {ML-as-a-service, Marketplaces, Pricing models, Federated learning, Observability, Explainable quality of training},
abstract = {The proliferation of edge data availability alongside advanced federated and distributed machine learning training techniques calls for new developments of machine learning (ML) with distributed and private edge data providers. Most existing works, however, focus on the development and optimization of federated ML communication and aggregation techniques and under-research the quality of training based on the impact of the quality of data and contributions of distributed data sources from such providers to the building of ML models. In this paper, we introduce an Edge marketplAce for DistRibuted AI/ML traiNing (EADRAN), a comprehensive platform for federated learning (FL) with independent edge data providers. The key distinguishable feature of EADRAN is to enable the explainable quality of training (eQoT) approach based on the quality of data and the contributions of provided data to the target-trained ML models. EADRAN offers end-to-end services for both data providers and ML market consumers, enhancing explainability and incentivizing the active participation of edge data providers. EADRAN shows high adaptability via the integration with notable FL frameworks like Flower to allow consumers to choose their preferred FL methods. EADRAN is developed based on a detailed conceptual architecture, guided by novel principles and requirements for federated machine learning marketplaces. We present the detailed design and implementation of EADRAN and conduct extensive experiments to demonstrate the benefits of eQoT-aware training in EADRAN with heterogeneous data scenarios.}
}
@article{HADIMOGAVI2024100027,
title = {ChatGPT in education: A blessing or a curse? A qualitative study exploring early adopters’ utilization and perceptions},
journal = {Computers in Human Behavior: Artificial Humans},
volume = {2},
number = {1},
pages = {100027},
year = {2024},
issn = {2949-8821},
doi = {https://doi.org/10.1016/j.chbah.2023.100027},
url = {https://www.sciencedirect.com/science/article/pii/S2949882123000270},
author = {Reza {Hadi Mogavi} and Chao Deng and Justin {Juho Kim} and Pengyuan Zhou and Young {D. Kwon} and Ahmed {Hosny Saleh Metwally} and Ahmed Tlili and Simone Bassanelli and Antonio Bucchiarone and Sujit Gujar and Lennart E. Nacke and Pan Hui},
keywords = {Artificial intelligence (AI), Generative AI, ChatGPT, Education, Human-computer interaction (HCI),, Early adopters, Social media, Qualitative research},
abstract = {To foster the development of pedagogically potent and ethically sound AI-integrated learning landscapes, it is pivotal to critically explore the perceptions and experiences of the users immersed in these contexts. In this study, we perform a thorough qualitative content analysis across four key social media platforms. Our goal is to understand the user experience (UX) and views of early adopters of ChatGPT across different educational sectors. The results of our research show that ChatGPT is most commonly used in the domains of higher education, K-12 education, and practical skills training. In social media dialogues, the topics most frequently associated with ChatGPT are productivity, efficiency, and ethics. Early adopters' attitudes towards ChatGPT are multifaceted. On one hand, some users view it as a transformative tool capable of amplifying student self-efficacy and learning motivation. On the other hand, there is a degree of apprehension among concerned users. They worry about a potential overdependence on the AI system, which they fear might encourage superficial learning habits and erode students’ social and critical thinking skills. This dichotomy of opinions underscores the complexity of Human-AI Interaction in educational contexts. Our investigation adds depth to this ongoing discourse, providing crowd-sourced insights for educators and learners who are considering incorporating ChatGPT or similar generative AI tools into their pedagogical strategies.}
}
@article{SENGUPTA2020105596,
title = {A review of deep learning with special emphasis on architectures, applications and recent trends},
journal = {Knowledge-Based Systems},
volume = {194},
pages = {105596},
year = {2020},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2020.105596},
url = {https://www.sciencedirect.com/science/article/pii/S095070512030071X},
author = {Saptarshi Sengupta and Sanchita Basak and Pallabi Saikia and Sayak Paul and Vasilios Tsalavoutis and Frederick Atiah and Vadlamani Ravi and Alan Peters},
keywords = {Deep neural network architectures, Supervised learning, Unsupervised learning, Testing neural networks, Applications of deep learning, Evolutionary computation},
abstract = {Deep learning (DL) has solved a problem that a few years ago was thought to be intractable — the automatic recognition of patterns in spatial and temporal data with an accuracy superior to that of humans. It has solved problems beyond the realm of traditional, hand-crafted machine learning algorithms and captured the imagination of practitioners who are inundated with all types of data. As public awareness of the efficacy of DL increases so does the desire to make use of it. But even for highly trained professionals it can be daunting to approach the rapidly increasing body of knowledge in the field. Where does one start? How does one determine if a particular DL model is applicable to their problem? How does one train and deploy them? With these questions in mind, we present an overview of some of the key DL architectures. We also discuss some new automatic architecture optimization protocols that use multi-agent approaches. Further, since guaranteeing system uptime is critical to many applications, a section dwells on using DL for fault detection and mitigation. This is followed by an exploratory survey of several areas where DL emerged as a game-changer: fraud detection in financial applications, financial time-series forecasting, predictive and prescriptive analytics, medical image processing, power systems research and recommender systems. The thrust of this review is to outline emerging applications of DL and provide a reference to researchers seeking to use DL in their work for pattern recognition with unparalleled learning capacity and the ability to scale with data.}
}
@article{BELK2026103737,
title = {Yes, but…: Technology, netnography, and futures},
journal = {Futures},
volume = {175},
pages = {103737},
year = {2026},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2025.103737},
url = {https://www.sciencedirect.com/science/article/pii/S0016328725001995},
author = {Russell Belk and Mariam Humayun and Mahsan Hajirasouliha},
keywords = {Netnography, Futures, Prediction, Understanding, Visioneering, Hype},
abstract = {The idea of understanding the emergence of hopeful futures with netnography is no doubt a good one. Although netnography is based on the study of a small group of people, it may help understand where larger groups may be going, based on naturally occurring conversations conducted online. Such public and semi-public discourse tends to be polarized, but the more positive visions may indeed offer hope. In the contrarian view offered here, we temper such optimism with a historical view of prognostication in the realm of consumption and everyday life. We find that the practice of predicting the future has become more quantitative, but no more insightful with the rise of the internet and Big Data analytics. Doing in-depth netnography may, however, help understand how trends form and how they may affect the future as much or more than they predict it. We present a new conceptual understanding of the role of hype and visioneering in creating an atmosphere of excitement toward the latest technological innovation and explain why this is important.}
}
@article{DAVID2023111626,
title = {Collaborative Model-Driven Software Engineering — A systematic survey of practices and needs in industry},
journal = {Journal of Systems and Software},
volume = {199},
pages = {111626},
year = {2023},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2023.111626},
url = {https://www.sciencedirect.com/science/article/pii/S0164121223000213},
author = {Istvan David and Kousar Aslam and Ivano Malavolta and Patricia Lago},
keywords = {Model-driven engineering, Collaborative software engineering, Industry survey},
abstract = {The engineering of modern software-intensive systems is carried out in collaboration among stakeholders with specialized expertise. The complexity of such systems often also necessitates employing more rigorous approaches, such as Model-Driven Software Engineering (MDSE). Collaborative MDSE is the combination of the two disciplines, with its specific opportunities and challenges. The rapid expansion and maturation of the field started attracting tool builders from outside of academia. However, available systematic studies on collaborative MDSE focus exclusively on mapping academic research and fail to identify how academic research aligns with industry practices and needs. To address this shortcoming, we have carried out a mixed-method survey on the practices and needs concerning collaborative MDSE. First, we carried out a qualitative survey in two focus group sessions, interviewing seven industry experts. Second, based on the results of the interviews, we constructed a questionnaire and carried out a questionnaire survey with 41 industry expert participants. In this paper, we report the results of our study, investigate the alignment of academic research with the needs of practitioners, and suggest directions on research and development of the supporting techniques of collaborative MDSE.}
}
@article{DEMIR2025101428,
title = {Driving AI-enabled transformation in small and medium tourism enterprises: The strategic and investment roles of decision makers},
journal = {Tourism Management Perspectives},
volume = {59},
pages = {101428},
year = {2025},
issn = {2211-9736},
doi = {https://doi.org/10.1016/j.tmp.2025.101428},
url = {https://www.sciencedirect.com/science/article/pii/S2211973625000935},
author = {Şirvan Şen Demir and Mahmut Demir},
keywords = {Artificial intelligence, Small and medium tourism enterprise, Strategic decision-making, Digital transformation, Technology investment},
abstract = {This study aims to explore how business owners and general managers perceive, evaluate, and implement AI-supported digital transformation in tourism businesses. Drawing on the Technology-Organization-Environment framework, the Diffusion of Innovations Theory, and the Upper Echelons Theory, the research employs a qualitative design based on 34 in-depth interviews with decision-makers in Türkiye's tourism sector. The findings reveal a clear role-based divergence: business owners emphasize long-term strategic value, financial prudence, and innovation leadership, whereas general managers focus on operational feasibility, workforce adaptation, and service quality continuity. These differences highlight how cognitive orientations and managerial responsibilities shape AI adoption pathways. The study advances existing literature by integrating multiple theoretical perspectives to develop a role-differentiated model of strategic change. Practically, it offers actionable insights into how tourism SMEs can align strategic vision with operational implementation to achieve sustainable digital transformation.}
}
@article{KUMAR2024102783,
title = {AI-powered marketing: What, where, and how?},
journal = {International Journal of Information Management},
volume = {77},
pages = {102783},
year = {2024},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2024.102783},
url = {https://www.sciencedirect.com/science/article/pii/S0268401224000318},
author = {V. Kumar and Abdul R. Ashraf and Waqar Nadeem},
keywords = {AI marketing, Marketing strategies, Marketing activities, Consumer behavior and decision making, Research agenda},
abstract = {Artificial intelligence (AI) has become a disruptive force that has revolutionized industries and changed business practices. The integration of AI has brought numerous benefits to various functional areas within organizations, with marketing experiencing a significant positive impact. AI technologies have empowered marketers with advanced tools and insights, fostering unparalleled efficiency, personalization, and strategic campaign decision-making. Despite these advancements, the scholarly focus on AI's transformative effects on marketing is limited. This research investigates how AI is currently applied across different marketing functions and its potential future evolution and impact on marketing processes. In a rapidly evolving world, businesses must navigate complexity, innovate, and sustain competitive advantages. Grounding our analysis in previous AI marketing literature, we adopt the dynamic capability theoretical lens, emphasizing how organizations adapt and prosper in changing environments. This study highlights six key marketing areas where AI promises transformative effects, aiming to illuminate the path for future marketing innovations and strategies, including AI-driven customer insights, measuring marketing performance, automated marketing strategies, ethical implications, enhancing customer experiences, and growth opportunities with AI Implementation. While recognizing AI as a positive disruptive force, we also highlight its limitations, potential threats to privacy and security, as well as ramifications of biases, misuse, and dissemination of misinformation. Finally, the article delineates the gaps in the research and formulates questions aimed at advancing knowledge in AI marketing.}
}
@article{RAMMAL2025101480,
title = {An examination of herding behavior among tech stock investors in ASEAN-6 countries: A BiLSTM machine learning approach},
journal = {Sustainable Futures},
volume = {10},
pages = {101480},
year = {2025},
issn = {2666-1888},
doi = {https://doi.org/10.1016/j.sftr.2025.101480},
url = {https://www.sciencedirect.com/science/article/pii/S266618882501041X},
author = {Hussain Gulzar Rammal and Putu Karina Gayatri and Asfi Manzilati and Silvi Asna Prestianawati and Diva Kurnianingtyas and Nathan Daud and Abdurrahman Hakim and Muhammad Fawwaz},
keywords = {Investment, Herding behavior, Investor, Machine learning},
abstract = {The purpose of this study is to investigate whether herding behavior persists in conventional and sustainable stock indices in ASEAN-5 nations during times of economic uncertainty, such as the COVID-19 pandemic and international political unrest. The study uses augmented regression models and the Cross-Sectional Absolute Deviation (CSAD) approach to identify herding tendencies and asymmetries under different market conditions using daily closing price data from 2019 to 2023. The results show that herding behavior is evident in both conventional and sustainable indices, especially during times of high volatility. Herding, on the other hand, is more noticeable in sustainable indices, indicating that during crises, investors may act collectively motivated by risk aversion and ESG sentiments. It is also confirmed that asymmetric herding exists, with downward markets producing more powerful herding effects than upward ones. These findings demonstrate how behavioral dynamics play a crucial part in determining the performance of sustainable investments. They also imply that irrational investor behavior can still occur in sustainable markets. This necessitates a more thorough incorporation of behavioral finance principles into the planning and oversight of financial products that are associated with ESG. The study has two implications: first, it calls on financial institutions and policymakers to increase investor education and transparency in sustainable finance; second, it suggests that portfolio managers take behavioral risks into account when distributing assets among sustainability-focused investments, especially in lean economic times.}
}
@article{NICUESA2025106145,
title = {AI-driven alternative and online dispute resolution in the European Union: An analysis of the legal framework and a proposed categorization},
journal = {Computer Law & Security Review},
volume = {57},
pages = {106145},
year = {2025},
issn = {2212-473X},
doi = {https://doi.org/10.1016/j.clsr.2025.106145},
url = {https://www.sciencedirect.com/science/article/pii/S2212473X25000185},
author = {Aura Esther Vilalta Nicuesa and Marian Gili Saldaña},
keywords = {A2J, ADR, ODR, AI, AI-DR, Dispute resolution, Ethical standards, Fundamental rights},
abstract = {This paper focuses on the impact of the new EU AI Act in alternative and online dispute resolution. After briefly analysing the state of the art regarding international regulations on artificial intelligence (AI) and the strategy followed in the European Union (EU) in the field of dispute resolution, the research provides a critical discursive overview of the international existing legal guidelines and frameworks for the use of AI in dispute resolution, aiming to identify the different levels of risk addressed by the EU IA Act in this context. The paper also offers forward-looking reflections intended to contribute to the improvement of the current legal framework on AI applied to dispute resolution in the EU. To this end, it identifies various AI tools applicable to the justice sector, highlighting their main advantages and limitations. It then outlines the most relevant hard law and soft law instruments at both international and European levels, with a particular focus on the strategy implemented by the EU leading to the adoption of the current EU AI Act. The study also reviews initiatives carried out by organisations to promote the ethical use of AI in judicial systems and examines the legislative approach adopted by the EU to regulate AI in the field of justice. Finally, the paper proposes a new categorisation of AI-assisted alternative and online dispute resolution mechanisms based on their degree of risk and autonomy.}
}
@article{HOQUE202466,
title = {Resource Consumption Analysis of Distributed Machine Learning for the Security of Future Networks},
journal = {Procedia Computer Science},
volume = {251},
pages = {66-74},
year = {2024},
note = {15th International Conference on Emerging Ubiquitous Systems and Pervasive Networks / 14th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare EUSPN/ICTH 2024},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.11.085},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924033192},
author = {Md Muzammal Hoque and Ijaz Ahmad and Mohammad Tahir},
keywords = {Green AI, machine learning, Security, 6G, distributed security, 6G security},
abstract = {As the network continues to become more complex due to the increased number of devices and ubiquitous connectivity, the trend is shifting from a centralized implementation to decentralization. Similarly, strategies to secure networks are increasingly leaning towards decentralization for its potential to enhance security in future networks with the help of Machine Learning (ML) techniques. In this regard, Distributed Machine Learning (DML) techniques, such as Federated Learning (FL) and Split Learning (SL), are at the forefront of this shift, offering collaborative learning capabilities across network nodes while maintaining data privacy. However, ML requires vast amounts of dedicated computing, memory, bandwidth, and as a consequence, energy resources. Moreover, resource consumption ML techniques used for network security have mostly been overlooked, which presents a glaring challenge for future networks in terms of overall resource utilization. This research emphasizes the importance of understanding the resource consumption patterns of two important DML techniques, i.e., FL and SL, to analyze the consumption of critical resources when deployed for network security. Furthermore, this research draws important insights from a practical comparative analysis of FL and SL in terms of resource consumption patterns and discusses their scope for future network security, such as in 6G, and stirs further research in this area.}
}
@article{WAN2024103609,
title = {Emotion-cognitive reasoning integrated BERT for sentiment analysis of online public opinions on emergencies},
journal = {Information Processing & Management},
volume = {61},
number = {2},
pages = {103609},
year = {2024},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2023.103609},
url = {https://www.sciencedirect.com/science/article/pii/S0306457323003461},
author = {Bingtao Wan and Peng Wu and Chai Kiat Yeo and Gang Li},
keywords = {Emergency, Sentiment analysis, OCC model, BERT, Hybrid approach},
abstract = {Sentiment analysis of online public opinions on emergencies (OPOEs) requires accurate and explainable results to facilitate a better understanding of public sentiment and effective crisis management, but it is challenging due to the complexity and diversity of emotions contained in OPOEs. In this paper, we propose an Emotion-Cognitive Reasoning integrated BERT (ECR-BERT) for sentiment analysis of OPOEs. ECR-BERT combines an emotion model and deep learning to provide reliable auxiliary knowledge to improve BERT. Specifically, we use the emotion model proposed by Ortony, Clore, and Collins (OCC) to build emotion-cognitive rules and perform emotion-cognitive reasoning to discover emotion-cognitive knowledge. To mitigate the impact of knowledge noise, we propose a novel self-adaptive fusion algorithm that provides a selection mechanism for the incorporation of knowledge. In addition, we utilize knowledge-enabled feature representation to efficiently exploit inferred knowledge. Our evaluation on four real-world OPOE datasets shows that ECR-BERT significantly outperforms other BERT-based models, achieving state-of-the-art results with an absolute average accuracy improvement of 0.82%, 1.74%, 0.98%, and 1.37% over BERT, respectively. In addition, ECR-BERT provides a detailed explanation of how sentiment polarity is derived from fine-grained emotion categories. The ablation study demonstrates the effectiveness of each technique. In conclusion, ECR-BERT is an excellent choice for sentiment analysis of OPOEs, providing accurate and explainable results for crisis management.}
}
@article{KOHL202487,
title = {A Knowledge Graph-based Learning Assistance Systems for Industrial Maintenance},
journal = {Procedia CIRP},
volume = {126},
pages = {87-92},
year = {2024},
note = {17th CIRP Conference on Intelligent Computation in Manufacturing Engineering (CIRP ICME ‘23)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2024.08.305},
url = {https://www.sciencedirect.com/science/article/pii/S221282712400876X},
author = {Linus Kohl and Fazel Ansari},
keywords = {Artificial Intelligence, Maintenance, Knowledge Graph, Natural Language Processing, Learning Assistance System},
abstract = {Manufacturing industry is constantly evolving technologically. To ensure that the workforce comply with pace of changing requirements, Learning Assistance Systems (LAS) can provide competency-based support for daily tasks. LAS provide personalized support to blue and white collars through modular and contextualized workflows, information and learning materials. This paper presents a LAS designed and implemented for maintenance personnel in semiconductor manufacturing. The implemented LAS employs a novel statistical learning algorithm to determine the required competencies per task, interlinking them with information from knowledge bases and Manufacturing Execution Systems. This leads to a reduced mean time to repair of 18% in the illustrated use case.}
}
@article{YUAN2023103476,
title = {PVE: A log parsing method based on VAE using embedding vectors},
journal = {Information Processing & Management},
volume = {60},
number = {5},
pages = {103476},
year = {2023},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2023.103476},
url = {https://www.sciencedirect.com/science/article/pii/S0306457323002133},
author = {Wanli Yuan and Shi Ying and Xiaoyu Duan and Hailong Cheng and Yishi Zhao and Jianga Shang},
keywords = {Log parsing, Variational auto-encoder, Data analysis, Software engineering},
abstract = {Log parsing is a critical task that converts unstructured raw logs into structured data for downstream tasks. Existing methods often rely on manual string-matching rules to extract template tokens, leading to lower adaptability on different log datasets. To address this issue, we propose an automated log parsing method, PVE, which leverages Variational Auto-Encoder (VAE) to build a semi-supervised model for categorizing log tokens. Inspired by the observation that log template tokens often consist of words, we choose common words and their combinations to serve as training data to enhance the diversity of structure features of template tokens. Specifically, PVE constructs two types of embedding vectors, the sum embedding and the n-gram embedding, for each word and word combination. The structure features of template tokens can be learned by training VAE on these embeddings. PVE categorizes a token as a template token if it is similar to the training data when log parsing. To improve efficiency, we use the average similarity between token embedding and VAE samples to determine the token type, rather than the reconstruction error. Evaluations on 16 real-world log datasets demonstrate that our method has an average accuracy of 0.878, which outperforms comparison methods in terms of parsing accuracy and adaptability.}
}
@article{TRUONG2021102402,
title = {Privacy preservation in federated learning: An insightful survey from the GDPR perspective},
journal = {Computers & Security},
volume = {110},
pages = {102402},
year = {2021},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2021.102402},
url = {https://www.sciencedirect.com/science/article/pii/S0167404821002261},
author = {Nguyen Truong and Kai Sun and Siyao Wang and Florian Guitton and YiKe Guo},
keywords = {Federated learning, Data protection regulation, GDPR, Personal data, Privacy, Privacy preservation},
abstract = {In recent years, along with the blooming of Machine Learning (ML)-based applications and services, ensuring data privacy and security have become a critical obligation. ML-based service providers not only confront with difficulties in collecting and managing data across heterogeneous sources but also challenges of complying with rigorous data protection regulations such as EU/UK General Data Protection Regulation (GDPR). Furthermore, conventional centralised ML approaches have always come with long-standing privacy risks to personal data leakage, misuse, and abuse. Federated learning (FL) has emerged as a prospective solution that facilitates distributed collaborative learning without disclosing original training data. Unfortunately, retaining data and computation on-device as in FL are not sufficient for privacy-guarantee because model parameters exchanged among participants conceal sensitive information that can be exploited in privacy attacks. Consequently, FL-based systems are not naturally compliant with the GDPR. This article is dedicated to surveying of state-of-the-art privacy-preservation techniques in FL in relations with GDPR requirements. Furthermore, insights into the existing challenges are examined along with the prospective approaches following the GDPR regulatory guidelines that FL-based systems shall implement to fully comply with the GDPR.}
}
@article{SKLENARZ2024648,
title = {Does bigger still mean better? How digital transformation affects the market share–profitability relationship},
journal = {International Journal of Research in Marketing},
volume = {41},
number = {4},
pages = {648-670},
year = {2024},
issn = {0167-8116},
doi = {https://doi.org/10.1016/j.ijresmar.2024.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S0167811624000041},
author = {Felix Anton Sklenarz and Alexander Edeling and Alexander Himme and Julian R.K. Wichmann},
keywords = {Market share, Digital transformation, Profitability, Value creation, Two-sided digital platforms, Digital transformation dictionary},
abstract = {Extensive research has examined the effect of market share on profitability and, in general, has found a significantly positive relationship between the two metrics. However, this article demonstrates that the digital transformation of companies has substantially altered this relationship and its underlying mechanisms. The authors first theoretically develop the different influences of digital transformation on the traditional market share–profitability framework. Subsequently, they estimate a firm–profitability model based on a sample of 6,389 observations from 824 U.S. firms over 25 years that accounts for companies’ degree of digital transformation by text mining their financial statements using a self-developed and validated dictionary. The authors find a significantly negative interaction between the degree of digital transformation of a company and the impact of market share on profitability. However, they also show that this effect is moderated by i) a firm’s digital transformation emphasis (i.e., digital transformation of internal vs. external processes; digital transformation through platformization), ii) a firm’s general strategic emphasis (value appropriation relative to value creation), and iii) a firm’s general market environment (B2C versus B2B). The findings suggest that managers and investors of digital companies should exercise caution when relying on market share as a metric for performance.}
}
@article{BEVILACQUA2026115878,
title = {Strategic leadership at high altitude: Investigating how AI affects the required skills of top managers},
journal = {Journal of Business Research},
volume = {205},
pages = {115878},
year = {2026},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2025.115878},
url = {https://www.sciencedirect.com/science/article/pii/S0148296325007015},
author = {Simone Bevilacqua and Alberto Ferraris and Kurt Matzler and Michal Kuděj},
keywords = {Top managers, Artificial intelligence, Skills, Upper echelons theory, Strategic Leadership},
abstract = {As artificial intelligence (AI) becomes a core driver of organizational digital transformation, several studies emphasize the critical role of top managers. While prior research has examined the skills at the top levels in digital contexts, few studies differentiate the implications of diverse AI technologies for the strategic leadership skills of top managers. Drawing on upper echelons theory (UET), we conducted 23 interviews with senior executives across diverse industries. Our findings identify four interdependent leadership skills: 1) AI open mindset; 2) AI strategic co-thinker; 3) Multi-level connector; 4) Ethics risk management. We propose a multi-level framework that captures the interactive nature of these skills, operating across personal, organizational, and relational dimensions and shaped by top-down and bottom-up dynamics. The study, grounded in UET, contributes to the emerging debate on how AI reshapes top managers’ strategic leadership skills and introduces the enabling role of middle managers in AI transformation.}
}
@article{HAUTAMAKI2025115396,
title = {Fully leveraging AI in B2B sales: Exploring sales managers’ capabilities and organizational knowledge processes},
journal = {Journal of Business Research},
volume = {194},
pages = {115396},
year = {2025},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2025.115396},
url = {https://www.sciencedirect.com/science/article/pii/S014829632500219X},
author = {Pia Hautamäki and Minna Heikinheimo},
keywords = {Artificial intelligence, AI, B2B, Management behaviors, Managerial capabilities, Knowledge process, Sales manager},
abstract = {Studies have demonstrated that artificial intelligence (AI) can enhance sales efficiency in business-to-business (B2B) contexts. However, despite the wide accessibility of AI, its adoption in B2B sales remains limited. Few studies have examined how sales managers support their sales teams in leveraging AI. This is the first study to link the perspectives of AI exploitation to managerial capabilities and organizational knowledge processes in the context of B2B sales. Using a grounded theory approach, we derive empirical insights from interviews with 32 top-level managers in B2B sales organizations. We contribute to the B2B sales literature by presenting a framework illustrating how sales managers’ capabilities—data-based human capital, the social capital associated with creating a knowledge-sharing culture, and a transformative AI-positive mindset—foster AI-integrated knowledge processes at the individual, team, and organizational levels. We also provide practical recommendations for sales managers integrating AI into B2B sales operations.}
}
@article{LI2025244,
title = {How to Build New Productive Forces for Traditional Chinese Medicine Industry: Industrial Perception Intelligence and AI-Based Pharmaceutical Robot},
journal = {Engineering},
volume = {52},
pages = {244-255},
year = {2025},
issn = {2095-8099},
doi = {https://doi.org/10.1016/j.eng.2025.07.027},
url = {https://www.sciencedirect.com/science/article/pii/S2095809925004370},
author = {Zheng Li and Qilong Xue and Yang Yu and Yequan Yan and Jingxuan Zhang and YangYang Su and Chenfei Li and Boli Zhang and Yiyu Cheng},
keywords = {Traditional Chinese medicine industry, Industrial perception intelligence, Multi-objective optimization of extraction process, Pharmaceutical robot, Artificial intelligence for pharmaceutical engineering},
abstract = {Extraction unit operation is the first step in traditional Chinese medicine (TCM) product manufacturing, and it is crucial in determining the quality of the produced medicine. However, due to a lack of effective multimodal monitoring and adjustment strategies, achieving high quality and efficiency remains a challenge. In this work, we proposed an artificial intelligence (AI)-based robot platform for the multi-objective optimization of the extraction process. First, a perception intelligence method for multimodal process monitoring was established to track active ingredient transfer and production changes during the extraction process. Second, a digital twin model was developed to reconstruct the field information, which interacted with real-time monitoring data. Furthermore, the model performed real-time inference to predict future production process states by using the reconstructing information. Finally, according to the predicted process states, the autonomous decision-making robot implemented multi-objective optimization, ensuring efficient process adjustments for global optimization. Experimental and industrial results demonstrated that the platform could effectively infer component transfer dynamics, monitor temperature variations, and identify boiling states, ensuring product quality while reducing energy consumption. This pharmaceutical robot could promote the integration of AI and pharmaceutical engineering, thereby accelerating the iterative development and improvement of China’s pharmaceutical industry.}
}
@article{LIU2022102727,
title = {A collaborative deep learning microservice for backdoor defenses in Industrial IoT networks},
journal = {Ad Hoc Networks},
volume = {124},
pages = {102727},
year = {2022},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2021.102727},
url = {https://www.sciencedirect.com/science/article/pii/S157087052100216X},
author = {Qin Liu and Liqiong Chen and Hongbo Jiang and Jie Wu and Tian Wang and Tao Peng and Guojun Wang},
keywords = {Industrial Internet of Things, Secure microservices, Deep neural networks, Backdoor attacks, Backdoor defenses},
abstract = {Deep Learning shows a broad prospect in providing intelligence microservices to Industrial Internet of Things (IIoT). However, the existence of potential secure vulnerabilities limits the application of deep learning in IIoT. Therefore, how to provide secure deep learning services in IIoT applications becomes an important research topic. Among various attacks on deep neural networks (DNNs), backdoor attacks are generally recognized as the most imperceptible type, where an attacker can upload a poisoned DNN model that misbehaves only when inputs contain specific triggers. Existing defense solutions assume a defender has prior knowledge of backdoor triggers or DNN models, remaining far away from practical and flexible. To this end, this paper proposes a collaborative deep learning microservice, CoDefend, which employs strong intentional perturbation (STRIP) and cycle generative adversarial network (CycleGAN) to defend against backdoored neural networks. Compared with previous work, CoDefend has the advantages of flexibility and practicality. Empirical evaluations validate the high efficacy of CoDefend in providing secure deep learning microservices to IIoT.}
}
@article{TONG2025865,
title = {AI governance on young consumers in higher education: a content analysis of policies for generative AI},
journal = {Young Consumers: Insight and Ideas for Responsible Marketers},
volume = {26},
number = {5},
pages = {865-885},
year = {2025},
issn = {1747-3616},
doi = {https://doi.org/10.1108/YC-10-2024-2303},
url = {https://www.sciencedirect.com/science/article/pii/S1747361625000186},
author = {Ashley Tong and Zahirah Zainol and Teck Siong Chong and Krishnamoorthy Renganathan},
keywords = {Generative artificial intelligence, Governance framework, Academic policy, Higher education, Young consumer},
abstract = {Purpose
As generative artificial intelligence (AI) technologies continue to advance and become more prevalent in higher education, addressing the ethical concerns associated with their use is essential. This study emphasizes the need for robust AI governance as more young consumers increasingly use generative AI for various applications. This paper aims to examine the ethical challenges posed by generative AI and review the AI policies in higher education to regulate young consumers use of generative AI, focusing on the ethical use of AI from foundational principles to sustainable governance.
Design/methodology/approach
Through a content analysis of literature on generative AI policies in higher education published between 2020 and 2024, this research aims to explore a more holistic approach to integrating generative AI into the educational process. The analysis examines academic policies and governance framework from 28 journal papers regarding generative AI tools in higher education. Data were collected from publicly accessible sources, such as Scopus, Emerald Insights, ProQuest, Web of Science and ScienceDirect.
Findings
This study analyses ten elements of the governance framework to identify potential AI governance and policy setting, benefiting stakeholders aiming at enhancing the regulatory framework of generative AI use in higher education. The discussions indicate a generally balanced yet cautious approach to integrating generative AI technology, especially considering ethical issues, inherent limitations and data privacy concerns.
Originality/value
The findings contribute to ongoing discussions to strengthen universities’ responses to new academic challenges posed by the use of generative AI and promote high AI ethical standards across educational sectors.}
}
@article{BROEKHUIZEN2023114196,
title = {AI for managing open innovation: Opportunities, challenges, and a research agenda},
journal = {Journal of Business Research},
volume = {167},
pages = {114196},
year = {2023},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2023.114196},
url = {https://www.sciencedirect.com/science/article/pii/S0148296323005556},
author = {Thijs Broekhuizen and Henri Dekker and Pedro {de Faria} and Sebastian Firk and Dinh Khoi Nguyen and Wolfgang Sofka},
keywords = {, , , },
abstract = {Artificial intelligence (AI) provides ample opportunities for enabling effective knowledge sharing among organizations seeking to foster open innovation. Past research often investigates the capability of AI to perform ‘human’ tasks in structured application fields. Yet, there is a lack of research that systematically analyzes when and how AI can be used for the more complex and unstructured tasks of open innovation (OI). We present a framework for leveraging AI-enabled applications to foster productive OI collaborations. Specifically, we create a 3x3 matrix by aligning the three OI stages (initiation, development, realization) with the three management functions of AI (mapping, coordinating, controlling). This matrix assists in identifying how various AI applications may augment or automate human intelligence, thereby helping to resolve prevailing OI challenges. It provides guidance on how organizations can use AI to establish, execute and govern exchanges across the OI stages. Finally, we lay out an agenda for future research.}
}
@article{AMER2026103969,
title = {A digital twin-based framework for predictive quality assurance and supply chain resilience in the automotive industry},
journal = {Advanced Engineering Informatics},
volume = {69},
pages = {103969},
year = {2026},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103969},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625008626},
author = {Yousef Amer and Amirhesam Soufali and Ashraf Zaghwan},
keywords = {Digital Twin, Predictive Quality Assurance, Automotive Industry, Supply Chain Resilience, Internet of Things, Key Performance Indicators, International Automotive Task Force},
abstract = {The automotive industry faces growing challenges in ensuring supply chain resilience (SCR) and predictive quality assurance (PQA), particularly amid global disruptions. Traditional quality systems often lack the traceability and adaptability needed in this dynamic environment. Addressing this gap, this study proposes a novel digital twin-enabled framework based on a structured seven-phase, five-stage methodology, termed the 7D model. Aligned with international automotive task force (IATF) standards, the framework leverages real-time IoT data and historical metrics to simulate disruptions, monitor key performance indicators (KPIs), and enable data-driven, proactive quality interventions. A case study from a tractor manufacturer illustrates the framework’s applicability in an emerging market context. Despite operating with limited digital infrastructure, the company’s engagement with lean practices demonstrates the feasibility and scalability of the 7D-PQA model. Comparative analysis against conventional problem-solving methods validates the framework’s enhanced capacity for resilience, traceability, and predictive quality. This work advances the field by offering the first IATF-aligned DT framework for PQA in the automotive sector, with broader implications for digital transformation across manufacturing industries.}
}
@article{LIU2025137483,
title = {Enabling self-approaching optimization of Home Energy Management System through multi-agent systems},
journal = {Energy},
volume = {334},
pages = {137483},
year = {2025},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2025.137483},
url = {https://www.sciencedirect.com/science/article/pii/S0360544225031251},
author = {Weifeng Liu and Yu Shen and Jiyuan Tian and Yuhang Meng and Qing Wu and Guangyu He},
keywords = {Home energy management system (HEMS), Multi-agent system, Electricity usage scenarios, Compatibility, Autonomous decentralized system, Self-approaching optimization, Behavior tree},
abstract = {With the diversification of application scenarios in Home Energy Management Systems (HEMS), its autonomous control systems must dynamically adapt to evolving electricity usage demands and operational environment, akin to intelligent robots, to deliver automated, precise, responsive, and sustainable electricity services. Although dedicated solutions for these electricity services perform well under specified application scenarios, differences in focus and implementation prerequisites result in suboptimal system performance in highly fuzzy and temporally variable operational environments. This presents significant challenges for HEMS in delivering automated, precise, responsive, and sustainable electricity services. In this paper, a comprehensive, flexible, and actionable framework, termed the Electricity Usage Scenario (EUS), is first proposed to uniformly describe the models underlying these electricity services. The standard EUS is adopted to support ongoing decision-making for the optimization and control of electrical appliances. Additionally, a compatibility assessment method that incorporates spatiotemporal features is proposed. The degree of compatibility reflects the adaptation between the operation state of electrical appliances, electricity usage demands, and the operational environment, while its slope serves as the trigger for consistency control. Subsequently, a non-hierarchical multi-agent system based on an Autonomous Decentralized System (ADS) is designed. This architecture offers advantages in terms of online fault tolerance, system scalability, and maintainability. Furthermore, the types and functions of specialized agents within the multi-agent system are defined to ensure that HEMS can autonomously provide responsive electricity services. To further tackle the adverse effects caused by temporal variability, a self-approaching optimization consistency control method is introduced, implemented through Behavior Tree (BT), aiming to keep the multi-agent system in a state that persistently approaches optimal performance, thereby sustaining electricity services. Finally, the proposed methodologies are validated through both a real-world office environment and a virtual simulation system based on the discrete event simulation framework SimPy. The results demonstrate that the proposed methods effectively deliver automated, precise, responsive, and sustainable electricity services, affirming their practical feasibility and application potential.}
}
@article{TEO2025106134,
title = {Artificial intelligence, human vulnerability and multi-level resilience},
journal = {Computer Law & Security Review},
volume = {57},
pages = {106134},
year = {2025},
issn = {2212-473X},
doi = {https://doi.org/10.1016/j.clsr.2025.106134},
url = {https://www.sciencedirect.com/science/article/pii/S2212473X25000070},
author = {Sue Anne Teo},
keywords = {Human vulnerability, Algorithmic vulnerability, Artificial intelligence, Multi-level resilience, Vulnerability theory},
abstract = {Artificial intelligence (AI) is increasing being deployed across various sectors in society. While bringing progress and promise to scientific discovery, public administration, healthcare, transportation and human well-being generally, artificial intelligence can also exacerbate existing forms of human vulnerabilities and can introduce new vulnerabilities through the interplay of AI inferences, predictions and content that is generated. This underpins the anxiety of policymakers in terms of managing potential harms and vulnerabilities and the harried landscape of governance and regulatory modalities, including through the European Union’s effort to be the first in the world to comprehensively regulate AI. This article examines the adequacy of the existing theories of human vulnerability in countering the challenges posed by artificial intelligence, including through how vulnerability is theorised and addressed within human rights law and within existing legislative efforts such as the EU AI Act. Vulnerability is an element that informs the contours of groups and populations that are protected, for example under non-discrimination law and privacy law. A critical evaluation notes that while human vulnerability is taken into account in governing and regulating AI systems, the vulnerability lens that informs legal responses is one that is particularistic, static and identifiable. In other words, the law demands that vulnerabilities are known in advance in order for meaningful parameters of protection to be designed around them. The individual, as the subject of legal protection, is also expected to be able to identify the harms suffered and therein seek for accountability. However, AI can displace this straightforward framing and the legal certainty that implicitly underpins how vulnerabilities are dealt with under the law. Through data-driven inferential insights of predictive AI systems and content generation enabled by general purpose AI models, novel forms of dynamic, unforeseeable and emergent forms of vulnerability can arise that cannot be adequately encompassed within existing legal responses. Instead, it requires an expansion of not only the types of legal responses offered but also of vulnerability theory itself and the measures of resilience that should be taken to address the exacerbation of existing vulnerabilities and but also of emergent ones. The article offers a re-theorisation of human vulnerability in the age of AI as one informed by the universalist idea of vulnerability theorised by Martha Fineman. A new conceptual framework is offered, through an expanded understanding that sketches out the human condition in this age as one of ‘algorithmic vulnerability.’ It finds support for this new condition through a vector of convergence from the growing vocabularies of harm, the regulatory direction and drawing from scholarship on emerging vulnerabilities. The article proposes the framework of multi-level resilience to account for existing and emerging vulnerabilities. It offers a typology, examining how resilience towards vulnerabilities can be operationalised at the level of the individual, through technological design and within regulatory initiatives and other measures that promote societal resilience. The article also addresses objections to this new framing, namely in terms of how it seemingly results in a problem with no agency, potentially negating fault ascription and blame. Further, it addresses if the re-conception itself falls into the trap of technological determinism and finally, how the universalist notion of vulnerability can seemingly negate human autonomy that is a key feature of human dignity.}
}
@article{DOKHAC2026103064,
title = {Towards an integrative model of organizational human-AI collaboration: A semi-systematic review of the current state of the art},
journal = {Technology in Society},
volume = {84},
pages = {103064},
year = {2026},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2025.103064},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X25002544},
author = {Lilian Tai {Do Khac} and Michael Leyer},
keywords = {Trustworthy artificial intelligence (AI), Literature review, Human-AI interaction, Ethical AI design, Social and technical Infrastructure, Human and AI Trust},
abstract = {This article emphasizes the role of AI trustworthiness in business operations, aiming to understand which factors must be considered to ensure trust with AI in human-AI collaboration settings. While acknowledging the prevailing emphasis on technical aspects, our research highlights the necessity of a formalized trust model with AI. Through a literature review, we identify foundational principles for designing human-AI systems. Our key contribution lies in proposing a set of sixteen key conceptual elements as testable hypotheses for future studies. These elements are systematically integrated into a unified trust framework, providing a structure to enhance trust in AI systems, thereby fostering more effective human-AI interactions. By clarifying the features of AI that enhance human trust, this framework bridges conceptual gaps in prior literature and provides actionable insights for aligning AI development with organizational and user needs.}
}
@article{LESCHANOWSKY2024108344,
title = {Evaluating privacy, security, and trust perceptions in conversational AI: A systematic review},
journal = {Computers in Human Behavior},
volume = {159},
pages = {108344},
year = {2024},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2024.108344},
url = {https://www.sciencedirect.com/science/article/pii/S0747563224002127},
author = {Anna Leschanowsky and Silas Rech and Birgit Popp and Tom Bäckström},
keywords = {Privacy perception, Security perception, Trust perception, Conversational AI, Systematic review},
abstract = {Conversational AI (CAI) systems which encompass voice- and text-based assistants are on the rise and have been largely integrated into people’s everyday lives. Despite their widespread adoption, users voice concerns regarding privacy, security and trust in these systems. However, the composition of these perceptions, their impact on technology adoption and usage and the relationship between privacy, security and trust perceptions in the CAI context remain open research challenges. This study contributes to the field by conducting a Systematic Literature Review and offers insights into the current state of research on privacy, security and trust perceptions in the context of CAI systems. The review covers application fields and user groups and sheds light on empirical methods and tools used for assessment. Moreover, it provides insights into the reliability and validity of privacy, security and trust scales, as well as extensively investigating the subconstructs of each item as well as additional concepts which are concurrently collected. We point out that the perceptions of trust, privacy and security overlap based on the subconstructs we identified. While the majority of studies investigate one of these concepts, only a few studies were found exploring privacy, security and trust perceptions jointly. Our research aims to inform on directions to develop and use reliable scales for users’ privacy, security and trust perceptions and contribute to the development of trustworthy CAI systems.}
}
@article{GIRAU2024110632,
title = {Definition and implementation of the Cloud Infrastructure for the integration of the Human Digital Twin in the Social Internet of Things},
journal = {Computer Networks},
volume = {251},
pages = {110632},
year = {2024},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2024.110632},
url = {https://www.sciencedirect.com/science/article/pii/S138912862400464X},
author = {Roberto Girau and Matteo Anedda and Roberta Presta and Silvia Corpino and Pietro Ruiu and Mauro Fadda and Chan-Tong Lam and Daniele Giusto},
keywords = {Social IoT, Heterogeneous networks, Digital twins},
abstract = {With the integration of virtualization technologies, the Internet of Things (IoT) is expanding its capabilities and quickly becoming a complex ecosystem of networked devices. The Social Internet of Things (SIoT), where intelligent things include social properties that improve functioning and user engagement, is the result of this progress. The SIoT still has issues with scalability, data management, and user-centric operations, despite tremendous progress. In order to overcome these obstacles, a strong architecture is needed that can handle the enormous number of IoT devices while simultaneously streamlining the user interface. This study provides a unique architecture for the IoT that uses containerization to efficiently deploy and manage services while integrating Virtual Users (VUs) and Social Virtual Objects (SVOs) into a scalable Cloud/Edge infrastructure. These innovative aspects collectively advance previous works presented in literature and focused on novel SIoT architectures and implementations, by addressing key challenges in scalability, efficiency, and automation within the SIoT. The proposed method presents an extensible, modular architecture that lets VUs self-manage IoT services, making user administration easier and improving system security and scalability. Important parts of the design include a host controller for container orchestration, a deployer for automated service deployment, and user clusters for aggregating VUs, SVOs, and apps to provide secured and efficient data sharing. We show through experimental assessment that the architecture can manage high-volume installations and operating needs, exceeding the conventional platform based on Google App Engine in terms of system overhead and deployment timeframes. The obtained results highlight how our suggested architecture, which provides an easy-to-use, scalable, and secure foundation for IoT deployments, has the potential to advance the SIoT landscape.}
}
@article{XIAO2023107274,
title = {BugRadar: Bug localization by knowledge graph link prediction},
journal = {Information and Software Technology},
volume = {162},
pages = {107274},
year = {2023},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2023.107274},
url = {https://www.sciencedirect.com/science/article/pii/S0950584923001283},
author = {Xi Xiao and Renjie Xiao and Qing Li and Jianhui Lv and Shunyan Cui and Qixu Liu},
keywords = {Bug localization, Knowledge graph, Collaborative filtering},
abstract = {Context
: Information Retrieval-based Bug Localization (IRBL) aims to design automatic systems that find buggy files according to bug reports, which can reduce the time consumption to fix bugs for programmers. There has been extensive research on IRBL techniques in recent years. However, these methods cannot make full use of the structure information in bug reports and source files.
Objective
: In this paper, we propose a novel scheme BugRadar. It combines text features and structure features from bug reports and source files for bug localization. Especially, BugRadar leverages a knowledge graph to make use of structure features.
Method
: We originally propose a knowledge graph named TriGraph based on structure features and apply hyperbolic attention embedding to get the link prediction scores. For text features, we propose Partial Text Similarity which improves traditional Text Similarity and Method Level Text Similarity. We also propose Word Collaborative Filtering Score which leverages historical bug reports with more attention on important terms. Finally, we calculate the final suspicious scores based on the structure features, text features, and fixing time information from bug fixing history with a neural network.
Results
: We apply our scheme to four projects (Tomcat, SWT, JDT, and Birt) in a popular dataset and get approving results. BugRadar gets better results than other state-of-the-art methods on three projects out of the four. It achieves a relative improvement of 8.8% in SWT and 9.8% in JDT for Mean Average Precision compared to the previous best scheme KGBugLocator and 11.4% in Birt compared to Adaptive Regression.
Conclusions
: BugRadar can achieve approving performance on large-scale projects with enough historical bug reports. It verifies that knowledge graphs are capable of representing the structure features for bug localization. The novel Partial Text Similarity and Word Collaborative Filtering Score are both effective improvements for using text features.}
}
@article{PADILLARASCON2025104137,
title = {Trustworthy and explainable federated system for extracting descriptive rules in a data streaming environment},
journal = {Results in Engineering},
volume = {25},
pages = {104137},
year = {2025},
issn = {2590-1230},
doi = {https://doi.org/10.1016/j.rineng.2025.104137},
url = {https://www.sciencedirect.com/science/article/pii/S2590123025002257},
author = {M.A. Padilla-Rascón and A.M. García-Vico and C.J. Carmona},
keywords = {Federated rule learning, Trustworthy artificial intelligence, Data streaming, Supervised descriptive rules},
abstract = {A connected world in an information age with dozens of connected devices per person constantly generates continuous data streams. This leads us to the need to generate new intelligent models that discover knowledge in complex paradigms. However, these complex paradigms (capable of generating knowledge in isolated devices and sharing it between them, known as federated learning) must comply with the guidelines of Trustworthy Artificial Intelligence that obtains models with high levels of security, privacy, explainability and traceability. This contribution introduces the Trustworthy and Explainable Federated System based on Supervised Descriptive Rules for Data Streaming (TEFeS-SDR) algorithm, a trustworthy and explainable federated system for extracting descriptive rules in streaming data environments. This model, based on federated learning, emphasizes privacy and security through binary encoding and asymmetric encryption, avoiding the transfer of raw data between devices. Additionally, the system ensures traceability and auditability of the generated rules, providing transparency and trust. Experimental results demonstrate its capability to handle abrupt changes in data streams (concept drift) while maintaining high-quality and homogeneous global models. This work advances the path towards responsible artificial intelligence by combining explainability, security, and efficiency in dynamic environments.}
}
@article{DIAZRODRIGUEZ2023101896,
title = {Connecting the dots in trustworthy Artificial Intelligence: From AI principles, ethics, and key requirements to responsible AI systems and regulation},
journal = {Information Fusion},
volume = {99},
pages = {101896},
year = {2023},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2023.101896},
url = {https://www.sciencedirect.com/science/article/pii/S1566253523002129},
author = {Natalia Díaz-Rodríguez and Javier {Del Ser} and Mark Coeckelbergh and Marcos {López de Prado} and Enrique Herrera-Viedma and Francisco Herrera},
keywords = {Trustworthy AI, AI ethics, Responsible AI systems, AI regulation, Regulatory sandbox},
abstract = {Trustworthy Artificial Intelligence (AI) is based on seven technical requirements sustained over three main pillars that should be met throughout the system’s entire life cycle: it should be (1) lawful, (2) ethical, and (3) robust, both from a technical and a social perspective. However, attaining truly trustworthy AI concerns a wider vision that comprises the trustworthiness of all processes and actors that are part of the system’s life cycle, and considers previous aspects from different lenses. A more holistic vision contemplates four essential axes: the global principles for ethical use and development of AI-based systems, a philosophical take on AI ethics, a risk-based approach to AI regulation, and the mentioned pillars and requirements. The seven requirements (human agency and oversight; robustness and safety; privacy and data governance; transparency; diversity, non-discrimination and fairness; societal and environmental wellbeing; and accountability) are analyzed from a triple perspective: What each requirement for trustworthy AI is, Why it is needed, and How each requirement can be implemented in practice. On the other hand, a practical approach to implement trustworthy AI systems allows defining the concept of responsibility of AI-based systems facing the law, through a given auditing process. Therefore, a responsible AI system is the resulting notion we introduce in this work, and a concept of utmost necessity that can be realized through auditing processes, subject to the challenges posed by the use of regulatory sandboxes. Our multidisciplinary vision of trustworthy AI culminates in a debate on the diverging views published lately about the future of AI. Our reflections in this matter conclude that regulation is a key for reaching a consensus among these views, and that trustworthy and responsible AI systems will be crucial for the present and future of our society.}
}
@article{LABOONE2024100294,
title = {Overview of the future impact of wearables and artificial intelligence in healthcare workflows and technology},
journal = {International Journal of Information Management Data Insights},
volume = {4},
number = {2},
pages = {100294},
year = {2024},
issn = {2667-0968},
doi = {https://doi.org/10.1016/j.jjimei.2024.100294},
url = {https://www.sciencedirect.com/science/article/pii/S2667096824000831},
author = {Perry A. LaBoone and Oge Marques},
keywords = {Wearables, Artificial intelligence (AI), Healthcare},
abstract = {Technological advancements have had a significant impact on healthcare throughout history, leading to improved quality of care and greater efficiency, which ultimately benefits patients. The use of wearables and artificial intelligence (AI) in the healthcare industry has the potential to continue this trend. Wearables and AI enable real-time and continuous monitoring of a patient’s medical health information, which helps physicians detect diseases early and monitor patients during their recovery. However, there are challenges in managing the large amounts of data generated by these technologies and integrating them into existing electronic health records (EHRs). Despite these challenges, the introduction of AI promises to revolutionize the healthcare industry, much like the industrial and digital revolutions of the past. This paper will explore the transformative role of wearables and AI technology in healthcare, assess how it will change fundamental workflows, and highlight how AI solutions will become ubiquitous and expected by patients.}
}
@article{IGE20241847,
title = {Ensemble Filter-Wrapper Text Feature Selection Methods for Text Classification},
journal = {CMES - Computer Modeling in Engineering and Sciences},
volume = {141},
number = {2},
pages = {1847-1865},
year = {2024},
issn = {1526-1492},
doi = {https://doi.org/10.32604/cmes.2024.053373},
url = {https://www.sciencedirect.com/science/article/pii/S1526149224002637},
author = {Oluwaseun Peter Ige and Keng Hoon Gan},
keywords = {Metaheuristic algorithms, text classification, multi-univariate filter feature selection, ensemble filter-wrapper techniques},
abstract = {Feature selection is a crucial technique in text classification for improving the efficiency and effectiveness of classifiers or machine learning techniques by reducing the dataset’s dimensionality. This involves eliminating irrelevant, redundant, and noisy features to streamline the classification process. Various methods, from single feature selection techniques to ensemble filter-wrapper methods, have been used in the literature. Metaheuristic algorithms have become popular due to their ability to handle optimization complexity and the continuous influx of text documents. Feature selection is inherently multi-objective, balancing the enhancement of feature relevance, accuracy, and the reduction of redundant features. This research presents a two-fold objective for feature selection. The first objective is to identify the top-ranked features using an ensemble of three multi-univariate filter methods: Information Gain (Infogain), Chi-Square (Chi2), and Analysis of Variance (ANOVA). This aims to maximize feature relevance while minimizing redundancy. The second objective involves reducing the number of selected features and increasing accuracy through a hybrid approach combining Artificial Bee Colony (ABC) and Genetic Algorithms (GA). This hybrid method operates in a wrapper framework to identify the most informative subset of text features. Support Vector Machine (SVM) was employed as the performance evaluator for the proposed model, tested on two high-dimensional multiclass datasets. The experimental results demonstrated that the ensemble filter combined with the ABC+GA hybrid approach is a promising solution for text feature selection, offering superior performance compared to other existing feature selection algorithms.}
}
@article{NASIRINEJAD2025130,
title = {Implementing Digital Twin for Maintenance 4.0 in SMEs: A Framework for Affordable and Secure Solutions},
journal = {IFAC-PapersOnLine},
volume = {59},
number = {10},
pages = {130-135},
year = {2025},
note = {11th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2025},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2025.09.024},
url = {https://www.sciencedirect.com/science/article/pii/S2405896325007852},
author = {Majid Nasirinejad and Hamid Afshari and Srinivas Sampalli},
keywords = {Industry 4.0, IoT, SME, Smart Maintenance, I4.0, Digital Twin},
abstract = {Industry 4.0 technologies, including digital twin, have the potential to deal with maintenance challenges within SMEs. This paper investigates practical solutions to implement Maintenance 4.0 using digital twin. The proposed framework leverages low-cost sensors, subscription-based software, cloud computing, a digital twin system, and blockchain technology to create an afordable and secure solution. By integrating real-time monitoring, machine learning algorithms, and remote diagnostics, the framework empowers SMEs to optimize their maintenance processes, enhance equipment reliability, and improve overall productivity. The viability of the proposed framework is evaluated using a real case. The results show the advantage of the framework compared to other alternatives in terms of reliability, availability, maintainability, sustainability, and security. The paper concludes by discussing future research directions in this domain.}
}
@article{KONG2025100447,
title = {Developing and validating an artificial intelligence ethical awareness scale for secondary and university students: Cultivating ethical awareness through problem-solving with artificial intelligence tools},
journal = {Computers and Education: Artificial Intelligence},
volume = {9},
pages = {100447},
year = {2025},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2025.100447},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X25000876},
author = {Siu Cheung Kong and Jinyu Zhu},
keywords = {Artificial intelligence literacy, Ethical awareness, Problem-solving, Scale, Secondary and university students},
abstract = {Although there is growing attention to artificial intelligence (AI) ethics education with the rapid surge of AI technology, little research has explored the use of problem-solving to cultivate students' AI ethical awareness. To address this gap, this study developed a scale to multidimensionally measure AI ethical awareness and cultivated students' AI ethical awareness through problem-solving using AI tools within an AI literacy course. A total of 573 Hong Kong secondary and university students participated in a 14-h course. Data was collected through the AI Ethical Awareness Scale (AIEAS) questionnaire and students' self-reflective writings. The AIEAS framework, encompassing three dimensions, was based on the Belmont Report principles of human autonomy, beneficence, and fairness. Confirmatory factor analysis (CFA) supported this three-factor structure, with nine items retained. Multigroup CFA results demonstrated strong measurement invariance across gender, education level, and programming knowledge. All Cronbach's alpha and omega coefficient values exceeded 0.70, indicating satisfactory internal consistency and fair temporal stability. Furthermore, students' ethical awareness showed a significant improvement following the course intervention. This study not only provides a robust measure for researchers and practitioners to assess students' ethical awareness through behavioural engagement but also demonstrates the necessity of implementing an education course for cultivating ethical awareness through integrating experience and reflection.}
}
@article{LIU2025146118,
title = {Transforming the construction industry towards circular supply chain management 5.0: Identifying critical barrier factors through bibliometric and ISM-MICMAC},
journal = {Journal of Cleaner Production},
volume = {520},
pages = {146118},
year = {2025},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2025.146118},
url = {https://www.sciencedirect.com/science/article/pii/S0959652625014684},
author = {Qinghua Liu and Khoo Terh Jing and Chukwuka Christian Ohueri and Mohd Wira {Mohd Shafiei} and Jiao Wang and Hengbing Yin and Xiaoke Li},
keywords = {Circular supply chain management 5.0, Construction industry, Critical barrier factors, Taxonomy, ISM-MICMAC},
abstract = {With the rise of Industry 5.0 (I5.0), the construction industry (CI) is undergoing a transition from traditional linear supply chains to Circular Supply Chain Management 5.0 (CSCM 5.0). However, this transformation encounters several challenges, highlighting the need to identify a taxonomy of Critical Barrier Factors (CBFs) to foster the intelligent and sustainable development of the construction sector. This study employs a systematic literature review (SLR) to identify the Critical Barrier Factors and their taxonomy (political, economic, environmental, social, technological, and organizational) for CSCM5.0 transformation. The Fuzzy Delphi Method (FDM) is applied to eliminate bias in the CBFs and taxonomy, while the Fuzzy Analytic Hierarchy Process (FAHP) is used to assess the importance of the six taxonomies, revealing that governmental policy support plays a dominant role in CSCM 5.0 transformation. Furthermore, the study employs Interpretive Structural Modeling (ISM) to construct the hierarchical structure of the CBFs and integrates MICMAC analysis to explore the interrelationships among these factors. The findings indicate that an inadequate regulatory framework, the absence of policies and regulations, and insufficient economic incentives are the core CBFs to CSCM 5.0 transition, which indirectly validates the accuracy of the FAHP results. This study provides policy recommendations and practical guidance for governments, enterprises, and relevant stakeholders to promote a low-carbon, intelligent, and circular economic model in the CI.}
}
@article{KAMBA2024,
title = {Exploring the Impact of the COVID-19 Pandemic on Twitter in Japan: Qualitative Analysis of Disrupted Plans and Consequences},
journal = {JMIR Infodemiology},
volume = {4},
year = {2024},
issn = {2564-1891},
doi = {https://doi.org/10.2196/49699},
url = {https://www.sciencedirect.com/science/article/pii/S2564189124000100},
author = {Masaru Kamba and Wan Jou She and Kiki Ferawati and Shoko Wakamiya and Eiji Aramaki},
keywords = {COVID-19, natural language processing, NLP, Twitter, disrupted plans, concerns},
abstract = {Background
Despite being a pandemic, the impact of the spread of COVID-19 extends beyond public health, influencing areas such as the economy, education, work style, and social relationships. Research studies that document public opinions and estimate the long-term potential impact after the pandemic can be of value to the field.
Objective
This study aims to uncover and track concerns in Japan throughout the COVID-19 pandemic by analyzing Japanese individuals’ self-disclosure of disruptions to their life plans on social media. This approach offers alternative evidence for identifying concerns that may require further attention for individuals living in Japan.
Methods
We extracted 300,778 tweets using the query phrase Corona-no-sei (“due to COVID-19,” “because of COVID-19,” or “considering COVID-19”), enabling us to identify the activities and life plans disrupted by the pandemic. The correlation between the number of tweets and COVID-19 cases was analyzed, along with an examination of frequently co-occurring words.
Results
The top 20 nouns, verbs, and noun plus verb pairs co-occurring with Corona no-sei were extracted. The top 5 keywords were graduation ceremony, cancel, school, work, and event. The top 5 verbs were disappear, go, rest, can go, and end. Our findings indicate that education emerged as the top concern when the Japanese government announced the first state of emergency. We also observed a sudden surge in anxiety about material shortages such as toilet paper. As the pandemic persisted and more states of emergency were declared, we noticed a shift toward long-term concerns, including careers, social relationships, and education.
Conclusions
Our study incorporated machine learning techniques for disease monitoring through the use of tweet data, allowing the identification of underlying concerns (eg, disrupted education and work conditions) throughout the 3 stages of Japanese government emergency announcements. The comparison with COVID-19 case numbers provides valuable insights into the short- and long-term societal impacts, emphasizing the importance of considering citizens’ perspectives in policy-making and supporting those affected by the pandemic, particularly in the context of Japanese government decision-making.}
}
@article{MOSCA20251959,
title = {Big Data and AI for Smart Maintenance: Literature review on the impact on plants Resilience},
journal = {Procedia Computer Science},
volume = {253},
pages = {1959-1971},
year = {2025},
note = {6th International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.01.258},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925002662},
author = {M. Mosca and R. Mosca and M. Braggio},
keywords = {Industry 4.0, Big Data, Artificial Intelligence, AI, Machine Learning, Deep Learning, Neural Network, Maintenance, Resilience, Literature review},
abstract = {Industry 4.0 (I4.0) is a paradigm that brings great innovation in any field. For this reason, it is mandatory to deeply understand its applications. Due to its characteristics, 4.0 technologies require to be implemented starting with small pilots, able to produce fast and valuable results. Maintenance is a domain in which Industry 4.0 can have a great positive impact, by realizing a true predictive and prescriptive maintenance. In this paper, it is carefully reviewed how Big Data and Artificial Intelligence (AI) can improve maintenance and how they can be integrated to create the Smart Maintenance framework, with a special focus on the Resilience of the systems. The choice of these 2 technologies is due to the current interest in AI and the close relationship of it with Big Data. A deep literature review has been carried out using Scopus database. Records have been categorized associating to each model pros, cons and main applications. A bibliometric analysis has also been carried out. The literature shows a recent surge of interest in AI, especially since the release of models like ChatGPT, but a gap remains in integrating Big Data with AI in maintenance, highlighting the need for further research and practical case studies, while addressing challenges such as computational intensity, data quality, and expertise in AI application. Implementing new technologies like Big Data and AI in maintenance is crucial for improving efficiency and resilience, but it must be done carefully to avoid disruption.}
}
@article{ZHANG2022104740,
title = {Data-driven AI emergency planning in process industry},
journal = {Journal of Loss Prevention in the Process Industries},
volume = {76},
pages = {104740},
year = {2022},
issn = {0950-4230},
doi = {https://doi.org/10.1016/j.jlp.2022.104740},
url = {https://www.sciencedirect.com/science/article/pii/S0950423022000171},
author = {Fengli Zhang and Qianzhe Qiao and Jinjiang Wang and Pinpin Liu},
keywords = {Emergency planning, Artificial intelligence, Safety, Deep learning},
abstract = {Whether it is through crisis mapping or event simulation, Artificial Intelligence (AI) is a pioneering new method of emergency planning. It uses the analysis of information or data from deep learning to predict the evacuation routes, allocate emergency resources reasonably and estimate the location of disaster. To overcome the difficulties of handling many resources emergency information and the variability of production environment, AI provides advanced analytics tools for processing and analyzing big data of emergency management. This paper presents a comprehensive survey of emergency planning technologies based on AI and discusses their applications in making emergency planning robust and efficiency. The development of AI technologies of emergency planning and their advantages over conventional data-driven emergency technologies are firstly discussed. Several representative emergency planning technologies based on deep learning models are compared. Finally, future trends and opportunities related to AI for emergency planning technologies are summarized.}
}
@article{ZHUANG2026104006,
title = {LT-CNN: an integrated deep learning method for enhancing topic recognition in digital healthcare research trend discovering},
journal = {Advanced Engineering Informatics},
volume = {69},
pages = {104006},
year = {2026},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.104006},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625008997},
author = {Wenyi Zhuang and Jianfei Zhu and Yuting Bao and Qi Zeng and Fuqiang Tan and Xi Li and Chenhui Liu and Ching-Hung Lee},
keywords = {Digital healthcare, LDA2vec, TOP2vec, Full text semantic retrieval, Convolutional neural network},
abstract = {Artificial intelligence (AI) serves as a pivotal driver of digital transformation across both traditional and technology-enhanced healthcare domains, with digital healthcare emerging as a core force propelling the digital economy. However, existing digital health research predominantly focuses on Western contexts, leaving Eastern healthcare systems underexplored. Addressing this gap through systematic analysis of emerging innovation patterns in Eastern settings can reveal critical cultural and technological determinants, guiding context-sensitive innovation pathways in global health informatics. Despite the usefulness of AI-based topic modeling techniques like Latent Dirichlet Allocation (LDA), LDA method exhibits several limitations in the topic representation, such as unclear topic references, defects in contextual semantic retrieval, and neglect of local text features, which can result in ambiguous topic words. Meanwhile, the current landscape of digital healthcare research in China remains unclear and is eager to be explored. To overcome these challenges, this study introduces LT-CNN—an innovative integrated deep learning framework which involves six layers of (1) Data preprocessing layer, (2) Vector splicing and input layer, (3) Convolutional layer, (4) BiLSTM layer, (5) Attention mechanism layer, (6) SoftMax output layer. LT-CNN synergistically combines LDA2vec (for joint local–global topic vector mining) with TOP2vec (embedding document-word semantics), with these algorithmic outputs integrated and processed through a CNN-BiLSTM-Attention network. The model’s topic mining capabilities and effectiveness was verified for the exploration on the topic of Chinese digital healthcare research. By leveraging LT-CNN, this work comprehensively examines defining characteristics and emerging future trends within China’s digital healthcare ecosystem which includes (1) Promoting digital transformation in the medical industry, (2) Advancing the optimal allocation of medical resources, (3) Driving the development of the medical industry, (4) Data security and privacy protection. Thusly, LT-CNN establishes a methodological benchmark for domain-specific topic analysis.}
}
@article{HADAVI2024105248,
title = {From BIM to metaverse for AEC industry},
journal = {Automation in Construction},
volume = {160},
pages = {105248},
year = {2024},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2023.105248},
url = {https://www.sciencedirect.com/science/article/pii/S0926580523005083},
author = {Ahmad Hadavi and Sepehr Alizadehsalehi},
keywords = {Metaverse, AEC industry, Building information modeling (BIM), Extended reality (XR), Digital twins, Virtual worlds, Multiuser virtual environments},
abstract = {BIM-to-Metaverse offers benefits to the AEC industry, but the challenge of efficiently and effectively implementing it will be formidable and require several years. The objective of this study is to identify the outsourcing forms of this new technology for construction project stakeholders and to explore its associated implementation challenges. Specifically addressed will be (a) innovations in XR technologies and how to convert BIM models to XR models for visualization in the metaverse, (b) an integrated definition function (IDEF0) model that describes how to prepare the BIM model for metaverse, (c) a use case for utilizing metaverse in the AEC industry and an evaluation of the concerns that must be resolved prior to successful implementation, and (d) an example illustrating the process. The metaverse implementation in design and construction can enhance collaboration, productivity, and quality but implementation cost, data security, and hardware and software interoperability are major challenges in its implementation.}
}
@article{LIU2025127866,
title = {Research on visual operation and maintenance platform of accelerator neutron source driven by digital twins},
journal = {Expert Systems with Applications},
volume = {284},
pages = {127866},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.127866},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425014885},
author = {Shaoqing Liu and Lizhen Liang and Chundong Hu and Yuzhong Qian and Xiancai Meng and Bing Hong and Sheng Yang},
keywords = {Digital twins, Accelerator neutron source, Visual operation and maintenance, Intelligent analysis, Health management},
abstract = {As a key technology connecting physical entities and virtual models, digital twin builds a bridge and bond for the exchange and sharing of virtual and real elements. In view of the complexity of the special equipment structure of accelerator neutron source (ANS), the lack of real-time mapping of the running state of the equipment, and the low degree of simulation, the research and implementation framework of the key technologies of the DT(Digital Twin)-driven visual operation and maintenance platform of the ANS (VOMP-A) was proposed. It includes five parts: digital model construction of the ANS, multi-source heterogeneous data acquisition and monitoring of the ANS, yield prediction and performance optimization of the ANS, fault diagnosis and health assessment of the ANS, and maintenance strategy and health management of the ANS. Through the research on the above key technologies, a whole process operation mechanism is constructed, which includes 3D(Three dimensions) dynamic visual monitoring of ANS, full-process driving of digital twin, multi-parameter optimization of neutron yield, comprehensive analysis of fault prediction and intelligent maintenance strategy, so as to realize the service requirements of reflecting reality by virtual reality, controlling reality by virtual reality, optimizing reality by virtual reality, and prerealizing reality by virtual reality. It can effectively avoid equipment damage caused by accidental failure and further improve the production and operation efficiency of the ANS.}
}
@article{PARK2024102705,
title = {AI vs. human-generated content and accounts on Instagram: User preferences, evaluations, and ethical considerations},
journal = {Technology in Society},
volume = {79},
pages = {102705},
year = {2024},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2024.102705},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X24002537},
author = {Jeongeun Park and Changhoon Oh and Ha Young Kim},
keywords = {Artificial intelligence (AI) generated content, AI artists, Social networking services, User experience, Generative models},
abstract = {As content generated by artificial intelligence (AI) has become more accessible and higher quality, social debates have intensified. Therefore, this study investigates how people perceive and evaluate AI-generated content and accounts compared to human-created accounts and content. We created Instagram accounts representing AI, influencers, and the public and conducted a user study with 43 participants. The participants had difficulty distinguishing AI accounts from human ones. Moreover, there were significant differences in user perceptions concerning the three account types. Participants perceived the AI and influencer accounts as more attractive than the public account, and they rated the quality of AI-generated content as highly as that created by influencers. These findings suggest that the advancement of generative AI could alter the social media landscape and contribute to discussions on the characteristics and ethical problems of AI-generated content.}
}
@article{NIU2024114228,
title = {Enhancing healthcare decision support through explainable AI models for risk prediction},
journal = {Decision Support Systems},
volume = {181},
pages = {114228},
year = {2024},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2024.114228},
url = {https://www.sciencedirect.com/science/article/pii/S0167923624000617},
author = {Shuai Niu and Qing Yin and Jing Ma and Yunya Song and Yida Xu and Liang Bai and Wei Pan and Xian Yang},
keywords = {Explainable AI in healthcare, Healthcare decision support, Disease risk prediction, Modelling longitudinal patient data, Deep neural networks},
abstract = {Electronic health records (EHRs) are a valuable source of information that can aid in understanding a patient’s health condition and making informed healthcare decisions. However, modelling longitudinal EHRs with heterogeneous information is a challenging task. Although recurrent neural networks (RNNs) are frequently utilized in artificial intelligence (AI) models for capturing longitudinal data, their explanatory capabilities are limited. Predictive clustering stands as the most recent advancement within this domain, offering interpretable indications at the cluster level for predicting disease risk. Nonetheless, the challenge of determining the optimal number of clusters has put a brake on the widespread application of predictive clustering for disease risk prediction. In this paper, we introduce a novel non-parametric predictive clustering-based risk prediction model that integrates the Dirichlet Process Mixture Model (DPMM) with predictive clustering via neural networks. To enhance the model’s interpretability, we integrate attention mechanisms that enable the capture of local-level evidence in addition to the cluster-level evidence provided by predictive clustering. The outcome of this research is the development of a multi-level explainable artificial intelligence (AI) model. We evaluated the proposed model on two real-world datasets and demonstrated its effectiveness in capturing longitudinal EHR information for disease risk prediction. Moreover, the model successfully produced interpretable evidence to bolster its predictions.}
}
@article{MANSERPAYNE2024100050,
title = {The search for AI value: The role of complexity in human-AI engagement in the financial industry},
journal = {Computers in Human Behavior: Artificial Humans},
volume = {2},
number = {1},
pages = {100050},
year = {2024},
issn = {2949-8821},
doi = {https://doi.org/10.1016/j.chbah.2024.100050},
url = {https://www.sciencedirect.com/science/article/pii/S2949882124000100},
author = {Elizabeth H. {Manser Payne} and Colleen A. O'Brien},
keywords = {Artificial intelligence, Social presence, Human-AI interaction, Virtual agent, Chatbot, Digital servitization},
abstract = {The banking industry is infusing AI systems into service encounters while dissolving some traditional services. This study aims to empirically test an exploratory framework to identify how human-AI interactions differ when engaged in basic or advanced virtual agent usage contexts. A conceptual framework was developed to examine consumer perceptions of basic and advanced virtual agent usage intentions. Five independent variables of trust in AI, perceived security in AI, perceived AI expertise, comfort in using AI technologies, and need for social presence were explored. Data was collected from 322 respondents and analyzed using multivariate regression. The findings suggest that consumers do not perceive service encounters with virtual agents from a “one size fits all” approach. Consumers perceive different value-in-use perceptions based on the complexity of the usage contexts. Our results suggest that success in advanced virtual agent encounters may require social presence for robust human-AI interaction. Additionally, this study extends the digital servitization and service robot acceptance model (sRAM) literature by evaluating consumer value-in-use perceptions with empirical evidence.}
}
@article{HAO2025101152,
title = {Beyond human-in-the-loop: Sensemaking between artificial intelligence and human intelligence collaboration},
journal = {Sustainable Futures},
volume = {10},
pages = {101152},
year = {2025},
issn = {2666-1888},
doi = {https://doi.org/10.1016/j.sftr.2025.101152},
url = {https://www.sciencedirect.com/science/article/pii/S2666188825007166},
author = {Xinyue Hao and Emrah Demir and Daniel Eyers},
keywords = {AI-human collaboration, Decision-making, Operations and supply chain management (OSCM), Sociotechnical systems, Cognitive mapping},
abstract = {In contemporary operational environments, decision-making is increasingly shaped by the interaction between intuitive, fast-acting System 1 processes and slow, analytical System 2 reasoning. Human intelligence (HI) navigates fluidly between these cognitive modes, enabling adaptive responses to both structured and ambiguous situations. In parallel, artificial intelligence (AI) has rapidly evolved to support tasks typically associated with System 2 reasoning, such as optimization, forecasting, and rule-based analysis, with speed and precision that in certain structured contexts can exceed human capabilities. To investigate how AI and HI collaborate in practice, we conducted 28 in-depth interviews across 9 leading firms recognized as benchmarks in AI adoption within operations and supply chain management (OSCM). These interviews targeted key HI agents, operations managers, data scientists, and algorithm engineers, and were situated within carefully selected, AI-rich scenarios. Using a sensemaking framework and cognitive mapping methodology, we explored how HI interpret and interact with AI across pre-development, deployment, and post-development phases. Our findings reveal that collaboration is a dynamic and co-constitutive process of institutional co-production, structured by epistemic asymmetry, symbolic accountability, and infrastructural interdependence. While AI contributes speed, scale, and pattern recognition in routine, structured environments, human actors provide ethical oversight, contextual judgment, and strategic interpretation, particularly vital in uncertain or ethically charged contexts. Moving beyond static models such as “human-in-the-loop” or “AI-assistance,” this study offers a novel framework that conceptualizes AI and HI collaboration as a sociotechnical system. Theoretically, it bridges fragmented literatures in AI, cognitive science, and institutional theory. Practically, it offers actionable insights for designing collaborative infrastructures that are both ethically aligned and organizationally resilient. As AI ecosystems grow more complex and decentralized, our findings highlight the need for reflexive governance mechanisms to support adaptive, interpretable, and accountable human–machine decision-making.}
}
@article{RAHIM2025101206,
title = {Harnessing generative AI: Reviewing applications, challenges, and solutions for out-of-school children in developing regions},
journal = {Sustainable Futures},
volume = {10},
pages = {101206},
year = {2025},
issn = {2666-1888},
doi = {https://doi.org/10.1016/j.sftr.2025.101206},
url = {https://www.sciencedirect.com/science/article/pii/S2666188825007683},
author = {Sabit Rahim and Gul Sahar and Gul Jabeen and Sabila Khatoon and Dil Angaiz},
keywords = {AI, Generative AI, Out of school children, ChatGPT, AI integration},
abstract = {Out of school children in Gilgit-Baltistan (GB) face significant challenges due to geographical isolation, inadequate infrastructure, harsh weather, lack of schools, cultural barriers, and socio-economic constraints, especially for girls. Generative AI(GAI) has potential to bridge these gaps with adaptive, engaging and aligned curriculum content to support learning specially for out-of-school children. It enables adaptable access to education through visual, text and audio format in remote and underserved mountainous areas. This analysis includes key applications, challenges and solutions of GAI in education for out of school from an initial pool of 90 studies sourced from scholarly databases such as IEEE Xplore, Science Direct, and Google Scholar (different published included). After exhaustive screening, 30 major papers were reviewed to evaluate the potential of GAI in out-of-school children’s education. The findings highlight the significant role of Generative Artificial Intelligence (GAI) in enabling inclusive education by offering tailored content through Learning Management Systems (LMS). A theoretical model is proposed integrating GAI, LMS, adaptive and data-driven methodologies (A&DM), operational and ethical safety framework, implementation strategy, team structure, and financial considerations. Besides offering content, LMS gathers information to analyze individual needs and create appropriate instructional material. Local community facilitators are essential in reinforcing learning, bridging digital divides, and ensuring a supportive education atmosphere. The study addresses strengths, limitations, and suitability associated with GAI integration, such as integrity in assessment, critical thinking, potential disruptions, data reliability, and human interaction. This study shows GAI's potential in access, inclusivity, and engagement through strategic partnerships, adaptive approaches, and targeted efforts.}
}
@article{HUANG2022102967,
title = {Social media mining under the COVID-19 context: Progress, challenges, and opportunities},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {113},
pages = {102967},
year = {2022},
issn = {1569-8432},
doi = {https://doi.org/10.1016/j.jag.2022.102967},
url = {https://www.sciencedirect.com/science/article/pii/S1569843222001601},
author = {Xiao Huang and Siqin Wang and Mengxi Zhang and Tao Hu and Alexander Hohl and Bing She and Xi Gong and Jianxin Li and Xiao Liu and Oliver Gruebner and Regina Liu and Xiao Li and Zhewei Liu and Xinyue Ye and Zhenlong Li},
keywords = {COVID-19, Pandemic, Social media, Big data, Data mining},
abstract = {Social media platforms allow users worldwide to create and share information, forging vast sensing networks that allow information on certain topics to be collected, stored, mined, and analyzed in a rapid manner. During the COVID-19 pandemic, extensive social media mining efforts have been undertaken to tackle COVID-19 challenges from various perspectives. This review summarizes the progress of social media data mining studies in the COVID-19 contexts and categorizes them into six major domains, including early warning and detection, human mobility monitoring, communication and information conveying, public attitudes and emotions, infodemic and misinformation, and hatred and violence. We further document essential features of publicly available COVID-19 related social media data archives that will benefit research communities in conducting replicable and reproducible studies. In addition, we discuss seven challenges in social media analytics associated with their potential impacts on derived COVID-19 findings, followed by our visions for the possible paths forward in regard to social media-based COVID-19 investigations. This review serves as a valuable reference that recaps social media mining efforts in COVID-19 related studies and provides future directions along which the information harnessed from social media can be used to address public health emergencies.}
}
@article{OROSNJAK2025100095,
title = {Fostering cleaner production through the adoption of sustainable maintenance: An umbrella review with a questionnaire-based survey analysis},
journal = {Cleaner Production Letters},
volume = {8},
pages = {100095},
year = {2025},
issn = {2666-7916},
doi = {https://doi.org/10.1016/j.clpl.2025.100095},
url = {https://www.sciencedirect.com/science/article/pii/S2666791625000041},
author = {Marko Orošnjak and Nebojša Brkljač and Kristina Ristić},
keywords = {Cleaner production, Sustainable maintenance, Umbrella review, Gaussian Copula graphical models, Bayesian statistics, Multiple correspondence analysis},
abstract = {The global industrial sector accounts for about 26%, while manufacturing and construction accounts for about 13% of global CO2 emissions, highlighting the need for sustainable operational strategies. Regulatory frameworks (e.g., GreenDeal, RePowerEU), have placed increasing pressure on manufacturing industries to align their economic productivity practices with sustainable business models. In this context, Sustainable Maintenance (SM) emerged as a strategic approach to reduce resource inefficiencies and minimise environmental waste. Hence, the research on SM is important for two reasons. Firstly, the impact of maintenance activities in reducing energy consumption can be considered one of the main determinants for enhancing the sustainability of manufacturing processes. Secondly, integrating SM practices within the context of Industry 4.0 offers a strategic move in achieving cleaner production and availability of manufacturing processes. However, the lack of research on investigating factors hindering the adoption of these practices within manufacturing entities has been reported. Leveraging Umbrella Review, contemporary research on SM prospects has been examined. Instead of focusing solely on the barriers and enablers, the study uses these factors to describe the existing body of SM research by performing network analysis. Secondly, given that digitalization is a barrier and an enabler, a questionnaire-based survey instrument has been developed. The data obtained from the survey is subjected to statistical testing using Bayes inferential statistics and Multiple Correspondence Analysis. The findings suggest strong to extreme evidence (BF10 > 100) in favour of the existence of a correlation between digitalization (and technology) and maintenance sustainability aspects.}
}
@article{YAN2025100903,
title = {Human-centric artificial intelligence towards Industry 5.0: retrospect and prospect},
journal = {Journal of Industrial Information Integration},
volume = {47},
pages = {100903},
year = {2025},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2025.100903},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X25001268},
author = {Jiahe Yan and Zean Liu and Jiewu Leng and J.Leon Zhao and Chong Chen and Ding Zhang and Yong Tao and Yiwei Wang and Tingyu Liu and Chao Zhang and Yifei Tong and Dimitris Mourtzis and Lihui Wang},
keywords = {Human-centric AI, Human-AI collaboration, Human-centric, Industry 5.0, Embodied AI},
abstract = {The technology-driven Industry 4.0 paradigm is in a prosperous stage. Meanwhile, the industry is shifting towards a more human-centric, sustainable, and resilient paradigm, which is envisioned as a value-oriented Industry 5.0. Embodied Artificial Intelligence (AI) has shown promising benefits, but challenges persist in the proper orchestration between AI and human beings. Human-Centric Artificial Intelligence (HCAI) emphasizes that AI systems should enhance and complement human abilities rather than replace humans. It focuses on the interaction between humans and AI, aims to improve human well-being, and ensures that AI technologies are consistent with human values and needs. HCAI prioritizes user experience and ethical considerations by following three principles: being inspired by human intelligence, guided by human impact, and augmenting human capabilities. This paper examines the growing trend of deep integration between AI and human intelligence in industries, emphasizing that AI development necessitates the interdependence of technology, people, and ethics to create reliable, safe, and trustworthy systems. This paper conducts a detailed analysis of the evolution stages and modes of human-AI collaboration in industry. Based on an in-depth examination of enablers of HCAI models in industry, this paper examines HCAI applications for the product lifecycle management. Social barriers, technology challenges, and future research directions of HCAI are underscored, respectively. We believe that our effort lays a foundation for unlocking the power of HCAI during the transition from Industry 4.0 to Industry 5.0.}
}
@article{SILVA2024177,
title = {Bibliographic review of AI applied to project management and its analysis in the context of the metalworking industry},
journal = {Procedia CIRP},
volume = {130},
pages = {177-187},
year = {2024},
note = {57th CIRP Conference on Manufacturing Systems 2024 (CMS 2024)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2024.10.073},
url = {https://www.sciencedirect.com/science/article/pii/S2212827124012265},
author = {José Silva and Paulo Ávila and João Matias and Luíz Faria and João Bastos and Luís Ferreira and Hélio Castro},
keywords = {Project Management, Schedule, Planning, Forecast, Artificial Intelligence, General or Strong AI, Narrow or Weak AI, Machine Learning, Deep Learning, Metalworking},
abstract = {Competition between companies, especially in the metalworking industry, has driven the continuous search for strategies that aim to reduce costs, improve product quality, and shorten delivery times. However, a persistent challenge that these industries face is delivery delays, caused by a number of complex factors, including supply chain issues, project management difficulties, technical and work issues. To address this issue, several approaches and methodologies have been adopted. On the one hand, agile management tools such as Scrum, Prince2 Agile, Lean Project Management, Agile Project Management and DSDM (Dynamic Systems Development Method) have been implemented to increase the flexibility and adaptability of production processes. On the other hand, classic analytical solutions, such as the Gantt/Waterfall Method, PMBOK Guide, Spiral Model, Prince2 and the Critical Path Method (CPM) and/or the PERT method, have been applied to ensure a clearer and more structured view of projects. In addition to these traditional approaches, the use of Artificial Intelligence (AI) has emerged as a promising tool in project management. A detailed analysis of articles reveals a marked preference for Machine Learning (ML), with particular emphasis on Artificial Neural Networks (ANNs). These AI techniques have been successfully applied in a variety of projects, especially in civil construction, helping to predict and mitigate delays, improve operational efficiency and optimize resource allocation. Despite the progress made, research on the use of AI in project management still faces significant challenges, as reflected in the scarcity of available studies on the topic in project management in the metalworking industry.}
}
@article{PASCUAL2024103823,
title = {Hunter: Tracing anycast communications to uncover cross-border personal data transfers},
journal = {Computers & Security},
volume = {141},
pages = {103823},
year = {2024},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2024.103823},
url = {https://www.sciencedirect.com/science/article/pii/S016740482400124X},
author = {Hugo Pascual and Jose M. {del Alamo} and David Rodriguez and Juan C. Dueñas},
keywords = {Personal data, Data protection, International transfers, Traceability, IP geolocation, Anycast, Transparency, Privacy, GDPR, Compliance},
abstract = {Cross-border personal data transfers are heavily regulated worldwide, with data protection authorities imposing huge fines on organizations that fail to meet their strict compliance requirements. However, network-level optimizations such as anycast addresses were not designed with personal data in mind, and their use may unwittingly divert personal data out of a legal boundary. This paper describes Hunter, an automated method to trace anycast communications and identify those threatening data protection compliance. We have applied Hunter in the wild to a set of Android apps to discover that all apps observed sending personal data to anycast addresses eventually carry out international transfers but fail to disclose them in their privacy policies. Our findings suggest that using anycast addresses to transmit personal data generally results in data protection compliance issues.}
}
@article{WANG2025112919,
title = {Safety assessment of intelligent vehicles considering drivers’ risk perception information under interval 2-tuple q-rung Orthopair Fuzzy Sets},
journal = {Applied Soft Computing},
volume = {175},
pages = {112919},
year = {2025},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2025.112919},
url = {https://www.sciencedirect.com/science/article/pii/S1568494625002303},
author = {Danqi Wang and Wengang Deng and Lin Hu and Zhongwei Huang and Yikang Lu and Honghao Zhang},
keywords = {Safety assessment, Intelligent vehicles, Risk perception, Decision-making, Interval 2-tuple q-rung Orthopair fuzzy sets},
abstract = {The safety assessment of intelligent vehicles as an important part of the intelligent vehicle industry is used to assess the safety of vehicles on the road. Drivers' risk perception affects their driving behavior, so taking it into account in safety assessments can more accurately assess the safety of intelligent vehicles. Decision-makers often have difficulty choosing the best option from multiple indicators in a safety assessment. The primary objective of this paper is to assess the safety of intelligent vehicles, considering drivers' risk perception in a fuzzy environment. A multi-level evaluation system for the safety assessment of intelligent vehicles is developed, covering functional safety, active and passive vehicle safety, netlink information reliability, and driver risk perception. A hybrid decision-making methodology under Interval 2-tuple q-rung Orthopair Fuzzy Sets is proposed for safety assessments of intelligent vehicles. Subsequently, an empirical application of four safe driving schemes demonstrates the validity and practicality of this integrated methodology. Comparative analysis, sensitivity analysis, and discussions are performed. The results prove that this approach provides an accurate and effective tool for the safety assessment of intelligent vehicles.}
}
@article{HAGHIGHAT2025101600,
title = {An explainable big transfer learning approach for IoT-based safety management in smart factories},
journal = {Internet of Things},
volume = {31},
pages = {101600},
year = {2025},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2025.101600},
url = {https://www.sciencedirect.com/science/article/pii/S2542660525001131},
author = {Zahra Hamoony Haghighat and Anik Islam and Hadis Karimipour and Behnam Miripour Fard},
keywords = {Big transfer learning, Explainable AI, Internet of things, Smart factory},
abstract = {The integration of the Internet of Things (IoT) in smart factories enhances management through real-time monitoring and data analytics, while Artificial Intelligence (AI) automates processes and boosts efficiency. However, AI systems require vast amounts of data and substantial training time, facing challenges such as domain discrepancies, limited labeled data, negative transfer, sample selection bias, and computational complexity. Additionally, the opaque nature of AI models raises transparency issues, making it difficult for human operators to trust and interpret AI decisions. To address these challenges, this paper proposes an IoT-based safety management scheme for smart factories, utilizing advanced technologies to enhance safety and operational efficiency. The proposed approach integrates robust deep learning (DL) models developed through big transfer learning (BiTL) and is augmented with explainable AI (XAI) to ensure transparency and reliability in safety management. The major contributions of this work include designing a comprehensive IoT-based safety framework, conducting a detailed case study to optimize DL model performance using BiTL, and establishing an experimental environment for thorough validation. The findings demonstrate that the proposed system not only meets but also exceeds the performance of existing safety management solutions, offering a transparent, trustworthy, and highly effective AI-driven safety management system for modern smart factories.}
}
@article{ZHANG2026108262,
title = {AdaptiveWordBug: Generating adversarial texts with an adaptive scoring strategy against deep learning classifiers},
journal = {Neural Networks},
volume = {195},
pages = {108262},
year = {2026},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2025.108262},
url = {https://www.sciencedirect.com/science/article/pii/S0893608025011438},
author = {Yunting Zhang and Lin Ye and Baisong Li and Hongli Zhang},
keywords = {Deep neural network, Adversarial example, Textual adversarial attack, Text classification, Scoring method},
abstract = {Deep learning models demonstrate vulnerability to textual adversarial attacks. Research on adversarial text generation methods contributes to the subsequent design of corresponding countermeasures. Current word-level adversarial text generation methods are typically designed in the framework based on word importance. In this framework, we need to score the importance of each word in a text with a scoring method and subsequently perturb these words in descending order of importance. However, current approaches typically employ a single model-dependent method to score the word importance during the scoring process. This scoring strategy often struggles to select important words comprehensively and accurately and may even fail when faced with some texts. To address this issue, we propose a black-box adversarial text generation method for text classification tasks in the framework based on word importance, named AdaptiveWordBug. AdaptiveWordBug introduces a new scoring strategy, Adaptive Scoring Strategy (ASS), which combines three model-dependent scoring approaches and one model-independent approach. Simultaneously, an adaptive parameter is assigned to each scoring method. Each parameter can be automatically adjusted for different texts. This scoring strategy has two advantages. On the one hand, it can comprehensively and accurately identify important words in any text, greatly enhancing the effectiveness of the generated adversarial texts. On the other hand, each scoring method in this strategy can be easily integrated or removed as a component. This allows simple adjustment of the scoring strategy for different target models, resulting in good suitability for various target models. In experiments conducted on Chinese text classification datasets, we employ the proposed AdaptiveWordBug to attack Chinese BERT and ChatGPT. The results demonstrate that, compared to baseline methods, AdaptiveWordBug exhibits superior attack effectiveness.}
}
@article{SHI2025104662,
title = {Revealing the built environment impacts on truck emissions using interpretable machine learning},
journal = {Transportation Research Part D: Transport and Environment},
volume = {141},
pages = {104662},
year = {2025},
issn = {1361-9209},
doi = {https://doi.org/10.1016/j.trd.2025.104662},
url = {https://www.sciencedirect.com/science/article/pii/S1361920925000720},
author = {Tongtong Shi and Meiting Tu and Ye Li and Haobing Liu and Dominique Gruyer},
keywords = {Urban freight transport, Truck emissions, Built environment, Nonlinear effects, Interpretable machine learning},
abstract = {Understanding the factors influencing truck emissions remains critical for sustainable urban freight transport development. However, ignoring spatiotemporal and policy heterogeneity may lead to inaccurate predictions for specific regions and misinterpretation of outcomes. This study develops a comprehensive framework to analyze the nonlinear effects of the built environment on heavy-duty diesel truck emissions, utilizing large-scale GPS data from Shanghai, China. We introduce an interpretable predictive model that integrates random effects with a light gradient boosting machine to account for spatiotemporal and policy influences. The results show that proposed model outperforms baseline by 15 %–20 %, with an improvement exceeding 17 % in the more complex tasks of localized predictions in central urban areas. Land use and road design factors contribute 72.26 % to truck emissions, with industrial land density as the primary driver. Furthermore, the relationship between these factors and pollution emissions exhibits pronounced non-linearity, with threshold effects that vary under various policy restrictions.}
}
@article{ALI2026124371,
title = {A two-phase TOE framework integrating SEM and ANN for evaluating cloud computing adoption in software testing},
journal = {Technological Forecasting and Social Change},
volume = {222},
pages = {124371},
year = {2026},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2025.124371},
url = {https://www.sciencedirect.com/science/article/pii/S0040162525004020},
author = {Sikandar Ali and Fazli Wahid and Irshad Ahmed Abbasi and Fahad Algarni and Saud S. Alotaibi and Ahmed Alkhayyat},
keywords = {Cloud-based software testing, Cloud computing adoption, Structural equation modeling, Artificial neural network, Empirical survey, SEM-ANN},
abstract = {Context:
Software applications are increasingly complex, distributed, and component-based, making software testing more demanding-especially as large-scale projects generate thousands of test cases. Cloud computing adoption (CCA) has become essential in such scenarios to ensure scalability, efficiency, and resource optimization.
Problem Statement:
However, existing research on cloud-based testing (CBT) often relies on linear methods, failing to capture nonlinear adoption dynamics. Hybrid models like structural equation modeling (SEM)-artificial neural networks (ANNs) and empirically validated Technology–Organization–Environment (TOE) frameworks are underutilized. Cross-industry variations, evolving adoption trends, and key factors like thresholds and moderating variables remain largely unexplored, creating significant gaps in understanding CBT adoption behavior.
Objective:
The aim of the study is to assess CCA for CBT by introducing a novel two-phase TOE framework combining SEM and ANN.
Methods:
A Systematic Literature Review (SLR) of 136 papers was conducted to identify seventy motivating factors (MFs) related to CBT. These MFs were then categorized into ten predictors, and based on these predictors ten hypotheses were developed. A two-phase model based on SEM and ANN was developed to test the hypothesis.
Results:
Out of ten hypotheses, eight were supported. Five predictors i.e. perceived degree of cloud resource utilization, perceived economic benefits of CC adoption, perceived level of trust in cloud adoption, perceived external stimuli and feasibility planning and risk analysis positively influenced CBT adoption. Three predictors: perceived business concerns, perceived effort expectancy, and organizational competence and capacity had a negative effects. Two predictors, perceived performance expectancy and organizational dynamics and business drives, showed no significant impact.
Novelty:
Some studies have been conducted to examine the MFs affecting CCA but they are not specifically conducted for software testing. Moreover, no attempt was made to explore their multifaceted linear and non linear affect on CCA.
Conclusion:
This study suggests that software testing should be conducted in the cloud environment.}
}
@article{GIORDANO2024123389,
title = {The impact of ChatGPT on human skills: A quantitative study on twitter data},
journal = {Technological Forecasting and Social Change},
volume = {203},
pages = {123389},
year = {2024},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2024.123389},
url = {https://www.sciencedirect.com/science/article/pii/S0040162524001859},
author = {Vito Giordano and Irene Spada and Filippo Chiarello and Gualtiero Fantoni},
keywords = {ChatGPT, Generative Artificial Intelligence, Natural Language Processing, Skills, ESCO},
abstract = {The novel generative Artificial Intelligence (AI) developed by OpenAI, i.e., ChatGPT, rised a great interest in both scientific and business contexts. This new wave of technological advancement typically produces deep transformation in the workplace, requiring new skills. However, none of the studies in literature provide quantitative analysis and measures on the impact of ChatGPT on human skills. To address this gap, we collected a database of 616,073 tweets about ChatGPT, and used Natural Language Processing techniques to identify the tasks users requested ChatGPT to perform, and the sentiment related to these tasks. Then, we compared these tasks with a standard taxonomy of skills (i.e., ESCO) using BERT. The results of the study underline that ChatGPT impacts 185 different skills. Moreover, we proposed a model to represent the interaction of the user and ChatGPT, useful to define four skills which are emerging for using this new technology.}
}
@article{SULIS2023107525,
title = {A survey on agents applications in healthcare: Opportunities, challenges and trends},
journal = {Computer Methods and Programs in Biomedicine},
volume = {236},
pages = {107525},
year = {2023},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2023.107525},
url = {https://www.sciencedirect.com/science/article/pii/S0169260723001906},
author = {Emilio Sulis and Stefano Mariani and Sara Montagna},
keywords = {Healthcare, Agent-based research, Literature survey, Network analysis},
abstract = {Background and Objective: The agent abstraction is a powerful one, developed decades ago to represent crucial aspects of artificial intelligence research. The meaning has transformed over the years and now there are different nuances across research communities. At its core, an agent is an autonomous computational entity capable of sensing, acting, and capturing interactions with other agents and its environment. This review examines how agent-based techniques have been implemented and evaluated in a specific and very important domain, i.e. healthcare research. Methods: We survey key areas of agent-based research in healthcare, e.g. individual and collective behaviours, communicable and non-communicable diseases, and social epidemiology. We propose a systematic search and critical review of relevant recent works, introduced by an exploratory network analysis. Results: Network analysis enables to devise out 5 main research clusters, the most active authors, and 4 main research topics. Conclusions: Our findings support discussion of some future directions for increasing the value of agent-based approaches in healthcare.}
}
@article{BOUZIDI2025109620,
title = {Managing emergency crises using secure information through educational awareness: COVID-19 case study},
journal = {Computers in Biology and Medicine},
volume = {186},
pages = {109620},
year = {2025},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.109620},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524017050},
author = {Zair Bouzidi and Abdelmalek Boudries},
keywords = {CNN-LSTM-based hybrid model, Context & Sentiment, Educational awareness, Extended Bidirectional Encoder Representations from Transformers (Bert), False information, Forecasting sanitary crises, Monitoring health environment},
abstract = {Social networks are increasingly taking over daily life, creating a volume of unsecured data and making it very difficult to capture safe data, especially in times of crisis. This study aims to use a Convolutional Neural Network (CNN)-Long Short-Term Memory (LSTM)-based hybrid model for health monitoring and health crisis forecasting. It consists of efficiently retrieving safe content from multiple social media sources. Educational awareness is a fairly important tool and a constant reminder to do everything to avoid fake news. The hybrid model captures safe and meaningful features from multiple social media sources. This research study enables retrieval of qualitative and secure content and mainly effective security against fake news. The results are compared to other approaches thanks to a publicly available dataset, which shows a very satisfactory performance with a precision of 63.74%, an accuracy of 59.33%, an F1-score of 71.66% and Matthews Correlation Coefficient (MCC) with 56.61%. This study allows integrating social media technologies, and artificial intelligence to avoid fake news. The training is combined with educational awareness to always carefully retrieve safe pattern information from multiple social media sources while improving the CNN-LSTM-based alert model. Finally, the hybrid model is evaluated on the Coronavirus Disease 2019 (COVID-19) health crisis to obtain promising results compared to other approaches. This comparison shows extremely positive educational effects on reducing health crisis alerts in sustainability.}
}
@article{BAI2025124260,
title = {How does digital intelligence drive the SRDI development of SMEs? Evidence from Chinese-style niche enterprises},
journal = {Technological Forecasting and Social Change},
volume = {219},
pages = {124260},
year = {2025},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2025.124260},
url = {https://www.sciencedirect.com/science/article/pii/S0040162525002914},
author = {Jingkun Bai and Guimin Qu},
keywords = {Digital intelligence, SRDI, SMEs, Text analysis, Niche},
abstract = {With the advent of the digital intelligence (DI) era, DI has become an important strategic opportunity for SMEs to realize the development of specialization, refinement, distinctiveness or innovation (SRDI). However, there is still a significant gap in the research on the specific mechanism of how DI drives the SRDI development of SMEs. Taking 667 Chinese listed SRDI SMEs from 2012 to 2022 as samples, the text analysis method is used to construct the DI and SRDI indicators and explore the mechanism of DI drives the SRDI development of SMEs. The research finds that DI can drive the SRDI development of SMEs; Mechanism analysis reveals that DI through four mechanisms: vertical division of labor, quality management, product competitiveness, and R&D structure, propelling SMEs towards specialization, refinement, distinctiveness and innovation respectively; Heterogeneity analysis shows that for little giant and SOE, the driving effect of DI is more pronounced, and its mechanism comes from the promoting effect of open innovation and political relevance. This study provides a new perspective for understanding the relationship between DI and SRDI, which has strategic significance for policy makers and entrepreneurs to promote the development of SRDI in SMEs.}
}
@article{TAMAK2025100999,
title = {Exploring the relationships between formalisation and validation tools in sustainability assessment models: Insights from formal concept analysis},
journal = {Journal of Industrial Information Integration},
volume = {48},
pages = {100999},
year = {2025},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2025.100999},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X25002225},
author = {Sundeep Tamak and Yasamin Eslami and Nicolás Leutwyler and Catherine {Da Cunha}},
keywords = {Sustainability, Manufacturing, Formal concept analysis, Formalisation and validation, Knowledge extraction},
abstract = {Sustainability has emerged as a critical concern for manufacturing organisations due to increasing resource scarcity, alongside many other environmental and social concerns. Sustainability assessment models (SAM) play a vital role in evaluating and improving the environmental, economic, and social impacts of manufacturing organisations. To develop a robust sustainability assessment model, it is important to understand how these models are formalised and validated. Consequently, discovering the relationships, whether implicit or explicit, among the formalisation and validation tools can be of value. To this end, the present work uses Formal Concept Analysis, as a clustering tool, to uncover the hidden relationships among several SAM formalisation and validation tools. The findings, in terms of association rules, reveal common pairings of formalisation and validation tools. In addition, a Decision Support System (DSS) has been developed to further assist the researchers in the sustainability assessment field to identify complementary formalisation and validation tools. The DSS takes a formalisation tool as input and, leveraging the derived association rules, provides ranked recommendations for additional complementary formalisation and validation tools. This research contributes to the existing literature by bridging the gap in understanding the interactions among SAM formalisation and validation tools, ultimately leading to more reliable and effective sustainability assessments in manufacturing.}
}
@article{MANTELERO2024106020,
title = {The Fundamental Rights Impact Assessment (FRIA) in the AI Act: Roots, legal obligations and key elements for a model template},
journal = {Computer Law & Security Review},
volume = {54},
pages = {106020},
year = {2024},
issn = {2212-473X},
doi = {https://doi.org/10.1016/j.clsr.2024.106020},
url = {https://www.sciencedirect.com/science/article/pii/S0267364924000864},
author = {Alessandro Mantelero},
keywords = {AI Act, Fundamental rights impact assessment, FRIA, Fundamental Rights, AI},
abstract = {What is the context which gave rise to the obligation to carry out a Fundamental Rights Impact Assessment (FRIA) in the AI Act? How has assessment of the impact on fundamental rights been framed by the EU legislator in the AI Act? What methodological criteria should be followed in developing the FRIA? These are the three main research questions that this article aims to address, through both legal analysis of the relevant provisions of the AI Act and discussion of various possible models for assessment of the impact of AI on fundamental rights. The overall objective of this article is to fill existing gaps in the theoretical and methodological elaboration of the FRIA, as outlined in the AI Act. In order to facilitate the future work of EU and national bodies and AI operators in placing this key tool for human-centric and trustworthy AI at the heart of the EU approach to AI design and development, this article outlines the main building blocks of a model template for the FRIA. While this proposal is consistent with the rationale and scope of the AI Act, it is also applicable beyond the cases listed in Article 27 and can serve as a blueprint for other national and international regulatory initiatives to ensure that AI is fully consistent with human rights.}
}
@article{HAIDEGGER2023422,
title = {Robotics: Enabler and inhibitor of the Sustainable Development Goals},
journal = {Sustainable Production and Consumption},
volume = {43},
pages = {422-434},
year = {2023},
issn = {2352-5509},
doi = {https://doi.org/10.1016/j.spc.2023.11.011},
url = {https://www.sciencedirect.com/science/article/pii/S2352550923002634},
author = {T. Haidegger and V. Mai and C.M. Mörch and D.O. Boesl and A. Jacobs and B. {Rao R} and A. Khamis and L. Lach and B. Vanderborght},
keywords = {Sustainable robotics, UN Sustainable Development Goals (SDGs), Industry 4.0, Social responsibility, Smart production},
abstract = {Robotics has the power to help our society in managing many current and foreseeable challenges, and contribute to a responsible future, as formally structured in the United Nations' Sustainable Development Goals (SDGs) initiative. Prior work has already investigated the impact of Artificial Intelligence (AI) on the SDGs, using a systematic consensus-based expert elicitation process. However, the existing literature has not focused on the intricacies of robotics and the unique dynamics this domain presents regarding the SDGs. In this vein, this work adapts an established approach, to focus on and dive deeper into the field of robotics and social responsibility. We introduce a multidisciplinary analysis of both the enabling and disabling roles of robotics, in achieving the SDG-presented, major economic, social and environmental priorities. The United Nation's 17 SDG and the 169 Targets, were individually examined within the context of state-of-the-art robotics already documented in scientific literature. The significance and the quality-of-evidence of enabling/inhibiting impacts, were assessed by an international panel of experts, to quantify the positive or negative effect of the applied robotic systems. Results from this study indicate that robotics has the potential to enable 46 % of the Targets, particularly for the industry and environment-related SDGs, forecasting a huge impact on our production systems and thus on our entire society. Inversely, robotics could inhibit 19 % of the SDG Targets, mainly through exacerbation of inequalities and tensions in the SDGs. The objective of this paper is to assess and grade the current impact of the robotics megatrend on the SDGs, provide comparable data, and encourage the robotics community to work on these targets, in a unified way and eventually improve the quality of the related outcomes.}
}
@article{HEMBAGEEKIYANAGE2020106834,
title = {Architectures and algorithms of an autonomous small-scale drilling agent},
journal = {Journal of Petroleum Science and Engineering},
volume = {188},
pages = {106834},
year = {2020},
issn = {0920-4105},
doi = {https://doi.org/10.1016/j.petrol.2019.106834},
url = {https://www.sciencedirect.com/science/article/pii/S0920410519312537},
author = {Suranga Chaminda {Hemba Geekiyanage} and Erik A. Loeken and Dan Sui},
keywords = {Drilling systems automation, Autonomous agent, Applied artificial intelligence, ROP optimization, Laboratory rig},
abstract = {This paper describes the core architectures and algorithms of an autonomous small-scale drilling agent. The agent operates in a laboratory rig, demonstrating drilling scenarios with limited or even no human intervention. The work illustrates its performance through self-coordinating state transition, Rate of Penetration (ROP, drilling speed) optimization capability, formation classification, and drilling incidents management. The agent is an original rule-based system, and its control architecture utilizes finite states automation. The novel ROP optimization strategy employs a gradient search in Weight on Bit (WOB)-rotational speed (RPM, Revolutions per Minute) control parameter space. It generates an increasing ROP trend with time and requires re-iteration at abrupt formation changes. Several drilling incidents are managed using ‘if-then’ logic-based activity decomposition. A key learning outcome from the study is the comprehension of the requirement of standard software architecture and Applications Programming Interfaces (API) for continuous research and development of the agent. Such interfaces enhance interoperability between systems and stimulate innovative thinking among independent developers to produce a better-faster set of algorithms. Laboratory testing and evaluation is an essential part of promoting the adaptation of digital technologies for drilling automation. Such studies are a useful, safe, and cost-effective solution for testing, integrating and improving hardware, software, and data management before expensive full-scale testing and integration.}
}
@article{WANG2024333,
title = {Blockchain sharding scheme based on generative AI and DRL: Applied to building internet of things},
journal = {Internet of Things and Cyber-Physical Systems},
volume = {4},
pages = {333-349},
year = {2024},
issn = {2667-3452},
doi = {https://doi.org/10.1016/j.iotcps.2024.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S2667345224000117},
author = {Jinlong Wang and Yixin Li and Yunting Wu and Wenhu Zheng and Shangzhuo Zhou and Xiaoyun Xiong},
keywords = {Blockchain, Sharding technology, Generative ai, Building internet of things, Deep reinforcement learning},
abstract = {When applying blockchain sharding technology in the building Internet of Things (IoT) domain to enhance the throughput performance of the blockchain, cross-shard transactions triggered by device collaborative tasks have increasingly become a prominent issue. Existing solutions base their shard division on historical transaction moments, using the outcomes for future transaction processing. However, since the historical interaction characteristics do not accurately reflect the interaction details within specific fine-grained time periods, this leads to poor system performance. Additionally, the parameter configuration in blockchain sharding systems is mostly based on arbitrary or default settings, which also results in unstable system performance. To address these two challenges, this paper proposes a blockchain sharding scheme called AI-Shard. Firstly, the system includes a module, G-AI, that utilizes generative AI to predict future node interaction relationships, enabling more proactive and adaptive shard division based on the predicted interaction matrix. Secondly, the system integrates a reinforcement learning module, DL-AI, specifically tailored for configuring parameters of the blockchain sharding system, such as the number of shards, block size, and block interval, to automatically optimize them, aiming to further enhance the system's throughput. Experimental results show that AI-Shard can reduce the proportion of cross-shard transactions and improve the system's throughput.}
}
@article{HAN2025112411,
title = {BIS-model: An explicit SysML-based modeling and analysis framework for smart building systems},
journal = {Journal of Building Engineering},
volume = {104},
pages = {112411},
year = {2025},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2025.112411},
url = {https://www.sciencedirect.com/science/article/pii/S2352710225006485},
author = {Deshuai Han and Guanglian Ma and Yanping Cai and Xuewei Zhang},
keywords = {Smart building, System modeling, Behavior analysis, SysML, BIS-Model},
abstract = {Smart building systems have recently achieved rapid development. However, a general software design framework for these systems is still lacking, due to their complexity in scale, structural intricacies, and multidisciplinary integration. To address this challenge, a framework called BuIlding Systems Modeling (BIS-Model) is proposed to enable systematic modeling, design, and behavior analysis of smart building systems. By extending SysML, BIS-Model introduces three types of modeling views: (1) a structure view called BIS-Block Definition Diagram that describes the complex system compositions, structure characteristics, and device properties of smart building systems; (2) a connection view called BIS-Internal Block Diagram that specifies device connections and interface properties of smart building systems; and (3) a behavior view called BIS-Sequence Diagram that specifies and analyzes the dynamic behaviors of smart building systems. Moreover, a support tool and roadmap are presented to integrate BIS-Model into each phase of smart building software development. The BIS-Model framework was empirically validated through a study using a real-world smart building system. The results demonstrate that BIS-Model enhances model quality by 45.17 % and reduces modeling time by 43.09 %, compared to the standard SysML, thereby significantly improving the design effectiveness and efficiency of smart building systems.}
}
@article{GUO2025131347,
title = {SMANet: Sequence-enhanced multi-head attention network for robust neural semantic learning in noisy computational environments},
journal = {Neurocomputing},
volume = {655},
pages = {131347},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2025.131347},
url = {https://www.sciencedirect.com/science/article/pii/S0925231225020193},
author = {Jia Guo and Xinyu Jia and Jinqi Zhu and Xiang Li and Yang Liu and Weijia Feng and Wanli Xue},
keywords = {Semantic communication, Semantic text transmission, Multi-head attention network},
abstract = {Traditional communication systems often fail to efficiently transmit meaningful information in noisy and dynamic environments, prompting the adoption of neural network architectures in semantic communication to prioritize semantic content over raw data. Existing neural models face persistent challenges in mitigating high noise interference, capturing long-range dependencies in sequences, and preserving semantic fidelity under varying conditions. This paper proposes SMANet, sequence-enhanced multi-head attention network for robust neural semantic learning in noisy computational environments. SMANet integrates multi-head attention mechanisms with a Dilated Normalization Block (DNB)—a specialized neural module for extracting local temporal features and global semantic representations—to enhance sequence processing capabilities, alleviate gradient vanishing/explosion issues during training, and improve network stability. At the transmitter, a neural semantic encoder employs dilated convolutions and normalization for robust feature extraction, paired with a channel encoder to achieve noise resilience; at the receiver, neural decoders precisely reconstruct semantics, facilitating applications in machine learning-driven cognitive systems. Experimental evaluations on AWGN and Rayleigh fading channels demonstrate SMANet’s superior performance, outperforming DeepSC by 23 % in BLEU scores, achieving a sentence similarity of 0.91 at SNR=18 dB, and maintaining 85 % semantic fidelity at SNR < 6 dB, highlighting its potential for neurocomputing in resource-constrained networks.}
}
@article{DONG2023107290,
title = {DeKeDVer: A deep learning-based multi-type software vulnerability classification framework using vulnerability description and source code},
journal = {Information and Software Technology},
volume = {163},
pages = {107290},
year = {2023},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2023.107290},
url = {https://www.sciencedirect.com/science/article/pii/S0950584923001441},
author = {Yukun Dong and Yeer Tang and Xiaotong Cheng and Yufei Yang},
keywords = {Multi-type vulnerability classification, Vulnerability description, Source code, Text Recurrent Convolutional Neural Network, Relational graph attention network},
abstract = {Context:
Software vulnerabilities have confused software developers for a long time. Vulnerability classification is thus crucial, through which we can know the specific type of vulnerability and then conduct targeted repair. Stack of papers have looked into deep learning-based multi-type vulnerability classification, among which most are based on vulnerability descriptions and some are based on source code. While vulnerability descriptions can sometimes mislead vulnerability classification and source code-based approaches have been rarely explored in multi-type vulnerability classification.
Objective:
We design DeKeDVer (Vulnerability Descriptions and Key Domain based Vulnerability Classifier) with two objectives: (i) to extract more useful information from vulnerability descriptions; (ii) to better utilize the information source code can reflect.
Method:
In this work, we propose a multi-type vulnerability classifier which combine vulnerability descriptions and source code together. We process vulnerability descriptions and source code of each project separately. For the vulnerability description of a sample, we preprocess it using a specified way we design based on our observations on numerous descriptions and then select text features. After that, Text Recurrent Convolutional Neural Network (TextRCNN) is applied to learn text information. For source code, we leverage its Code Property Graph (CPG) and extract key domain from it which are then embedded. Acquired feature vectors are then fed into Relational Graph Attention Network (RGAT). Result vectors gained from TextRCNN and RGAT are combined together as the feature vector of the current sample. A Multi-Layer Perceptron (MLP) layer is further added to undertake classification.
Results:
We conduct our experiments on C/C++ projects from NVD. Experimental results show that our work achieves 84.49% in weighted F1-measure which proves our work to be more effective.
Conclusion:
Our work utilizes information reflected both from vulnerability descriptions and source code to facilitate vulnerability classification and achieves higher weighted F1-measure than existing vulnerability classification tools.}
}
@article{COLLAO202596,
title = {An Agentic AI-based Architecture for Digital Twins Specialized in Predictive Maintenance: Application to Ball Mills},
journal = {IFAC-PapersOnLine},
volume = {59},
number = {32},
pages = {96-101},
year = {2025},
note = {20th IFAC Symposium on Optimization and Automation in Mining, Minerals and Metal Processing - MMM 2025},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2025.12.403},
url = {https://www.sciencedirect.com/science/article/pii/S2405896325031209},
author = {Claudio Collao and Humza Akhtar and Carlos Toro and Carlos Ocampo-Martínez and Raphael Schor and Rami Pinto Prieto},
keywords = {Agentic AI, Architecture, Digital Twins, Predictive Maintenance, Ball Mills},
abstract = {Mining operations are increasingly confronted with a multitude of challenges, including price volatility, declining ore grades, and escalating energy costs. These challenges are exacerbated by variations in mineral hardness, which contribute to accelerated wear on critical equipment. Among this machinery, ball mills are particularly susceptible to wear and component failures, leading to unplanned maintenance, costly downtime, and disruptions in production. This research seeks to enhance the capabilities of predictive maintenance (PdM), with a concentrated emphasis on Anomalous Behavior Detection (ABD) and Digital Twin (DT) technologies, specifically tailored for ball mill applications within a multi-agent AI system (MAS) framework. We present a novel architectural design that synergizes DT and ABD through a semi-autonomous multi-agent AI system comprising two primary agents: the PdM agent and the Quality Assurance Agent. The primary function of the PdM agent is to identify anomalous behavior, while the Quality Assurance Agent is tasked with assessing the implications of parameter modifications on mill efficiency. Furthermore, we described the principal challenges related to data quality, system integration, and real-time responsiveness that must be systematically addressed to facilitate successful implementations in the future.}
}
@article{CHAKRABORTY2023126680,
title = {Information retrieval algorithms and neural ranking models to detect previously fact-checked information},
journal = {Neurocomputing},
volume = {557},
pages = {126680},
year = {2023},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2023.126680},
url = {https://www.sciencedirect.com/science/article/pii/S0925231223008032},
author = {Tanmoy Chakraborty and Valerio {La Gatta} and Vincenzo Moscato and Giancarlo Sperlì},
keywords = {Fact-checking, Claim retrieval, Learning-to-rank, Semantic matching},
abstract = {Although in the last decade several fact-checking organizations have emerged to verify misinformation, fake news has continued to proliferate, especially through social media platforms. Even though adopting improved detection strategies is of utmost importance, the fact-checking process could be optimized by verifying whether a claim has been previously fact-checked. Despite some ad-hoc information retrieval approaches having been recently proposed, the utility of modern (neural) retrieval systems have not been investigated yet. In this paper, we consider the standard two-phases retriever-reranker architecture and benchmark different state-of-the-art techniques from the information retrieval and Q&A literature. We design several experiments on a real-world Twitter dataset to analyze the efficiency and the effectiveness of the benchmark approaches. Our results show that combining standard and neural approaches is the most promising research direction to improve retrievers performance and that complex (neural) rerankers might still be efficient in practice since there is no need to process a high number of documents to improve ranking performance.}
}
@article{WANG2024512,
title = {A data and knowledge driven autonomous intelligent manufacturing system for intelligent factories},
journal = {Journal of Manufacturing Systems},
volume = {74},
pages = {512-526},
year = {2024},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2024.04.011},
url = {https://www.sciencedirect.com/science/article/pii/S0278612524000785},
author = {Han Wang and Chenze Wang and Qing Liu and Xiaohan Zhang and Min Liu and Yumin Ma and Feng Yan and Weiming Shen},
keywords = {Non-ferrous metal industry, Autonomous intelligent manufacturing system, Industrial-GPT},
abstract = {The non-ferrous metal industry is encountering several challenges, including production efficiency, manufacturing information fragmentation, and human health problems, which highlights the importance of implementing autonomous intelligent manufacturing systems (AIMS). Recently, the foundation model like GPT-4, has garnered attentions due to its exceptional capabilities and proficiency in diverse domains and tasks, facilitating the realization of AIMS. However, the existing foundation models can only address basic general-purpose tasks and are difficult to use for industrial applications. In this paper, we propose a data and knowledge driven AIMS with industrial-generative pretrained Transformer (Industrial-GPT) for intelligent factories. The paradigms and architecture of autonomous intelligent factories are firstly defined. Then, we explore the mechanism with knowledge graph, digital twin, and Industrial-GPT, including multi-level autonomous perception, cross layer and domain cognition, and event-driven collaborative decision-making. Finally, the detailed case study is based on the cooperation with a zinc smelting intelligent factory to achieve networked collaborative manufacturing, and explores the theory and realization mechanism of AIMS on a small scale. We explore the experimental analyses, evaluation mechanisms and platform applications of AIMS at the workshop level. We believe this will help to realize larger scale AIMS in the future.}
}
@article{ORTALDA2025106182,
title = {Anticipating compliance. An exploration of foresight initiatives in data protection},
journal = {Computer Law & Security Review},
volume = {59},
pages = {106182},
year = {2025},
issn = {2212-473X},
doi = {https://doi.org/10.1016/j.clsr.2025.106182},
url = {https://www.sciencedirect.com/science/article/pii/S2212473X25000550},
author = {Alessandro Ortalda and Stefano Leucci and Gabriele Rizzo},
keywords = {Foresight, Anticipation, Data protection, GDPR, Data protection authorities, TechSonar, EDPS, CNIL, LINC, scenario analysis},
abstract = {The pace of technological progress has been increasing in recent years. As novel technologies arise or existing ones further develop, it becomes increasingly challenging to balance leveraging these advancements and safeguarding personal data. By relying on firsthand accounts of professionals in the field, the paper identifies how these challenges, which appear to be applicable to data controllers and Data Protection Authorities, are substantially connected with ensuring a sound interpretation of the law through time. The paper examines the leading foresight and anticipation techniques and explores their possible data protection applications by reviewing existing initiatives that attempt to implement foresight in the context of data protection. Section 2 delves into the evolving regulatory landscape, emphasising the need for a foresight-based approach to tackle the complexities arising from data-intensive technologies and the changing European regulatory framework. Section 3 introduces foresight as a discipline, its history and evolution, and leading techniques. Section 4 presents practical examples of foresight in data protection, detailing initiatives by the authors and other actors in the data protection space. In conclusion, the paper underscores the initial consensus on the benefits of anticipatory approaches in addressing current data protection challenges. Anticipation techniques, as a flexible concept, can be tailored to meet the needs of various stakeholders, fostering a collaborative and practical approach to data protection. However, a gap in consolidated methodologies persists, necessitating further research to design and implement practical foresight approaches.}
}
@article{ZHOU2026104754,
title = {Fuzz4Cuda: Fuzzing your NVIDIA GPU libraries through debug interface},
journal = {Computers & Security},
volume = {161},
pages = {104754},
year = {2026},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2025.104754},
url = {https://www.sciencedirect.com/science/article/pii/S0167404825004432},
author = {Yuhao Zhou and Peng Jia and Jiayong Liu and Ximing Fan},
keywords = {Fuzz testing, GPU library, Debug interface, Streaming assembler analysis},
abstract = {The programming security of Compute Unified Device Architecture (CUDA), NVIDIA’s parallel computing platform and programming model for Graphics Processing Unit, has always been a significant concern. On the host-side, fuzzing has been remarkably successful at uncovering various software bugs and vulnerabilities, with hundreds of flaws discovered annually through different fuzzing tools. However, existing fuzzing tools typically operate on general-purpose CPU architectures and embedded systems. As an independent processing unit, the GPU does not support tools like American Fuzzy Lop for collecting instrumentation and code coverage information. Consequently, grey-box fuzzing for closed-source graphics and driver libraries has remained an unaddressed challenge. This research introduces Fuzz4Cuda, CUDA-focused GPU fuzzing framework specifically designed for GPU libraries. To enhance device-side coverage collection, Fuzz4Cuda achieved this by runtime analysis of CUDA Streaming Assembler. Furthermore, the framework could dynamically adjust the number of breakpoints to optimize test case execution speed, thereby accelerating the overall time to discover program crash inputs. The development of Fuzz4Cuda has moved GPU library fuzzing ahead, aiming to improve the security of the GPU programming environment. Over a month-long real-world fuzzing campaign aimed at vulnerability discovery, our evaluation of the CUDA Toolkit uncovered five real-world bugs, four of which have been assigned Common Vulnerabilities and Exposures (CVE) IDs.}
}
@article{IMAM2022103170,
title = {OCR post-correction for detecting adversarial text images},
journal = {Journal of Information Security and Applications},
volume = {66},
pages = {103170},
year = {2022},
issn = {2214-2126},
doi = {https://doi.org/10.1016/j.jisa.2022.103170},
url = {https://www.sciencedirect.com/science/article/pii/S2214212622000552},
author = {Niddal H. Imam and Vassilios G. Vassilakis and Dimitris Kolovos},
keywords = {Deep learning, Spam image, OCR, Text recognition, Text classification, Adversarial text attack},
abstract = {The amount of images with embedded text shared on Online Social Networks (OSNs), such as Twitter or Facebook has been growing in recent years. It is becoming important to analyse the images uploaded into these platforms, as adversaries may spread images with toxic content or misinformation (i.e. spam). Optical character recognition (OCR) systems have been used to detect images with malicious content, where the embedded text gets extracted and classified using machine learning algorithms. However, most existing OCR-based systems are adversary-agnostic models, in which the extracted text from an image is not checked by humans before the classification. Consequently, these fully automated models become vulnerable to minor modifications of images’ pixels or textual content (e.g., character-level perturbations), which do not affect human understanding, but could cause the OCR systems to misrecognise the embedded text. In this paper, we propose an OCR post-correction algorithm to improve the robustness of OCR-based systems against images with perturbed embedded texts. Experimental results showed that our proposed algorithm improves the robustness of three state-of-the-art OCR models with at least 10% against adversarial text images, and it outperforms five spellcheckers in correcting adversarial text. Also, we evaluated the perceptibility of our adversarial images, and this study showed that 91% of the participants were able to correctly recognise the adversarial text images. Additionally, we developed an adversary-aware OCR-based system for detecting adversarial text images using the proposed algorithm, and our evaluation results showed considerable improvement in the performance of an OCR-based system.}
}
@article{GRUBLE2024156,
title = {Combined Geometric and Kinetic Data Model in Model-Based Systems Engineering of Robotic Cells},
journal = {Procedia CIRP},
volume = {128},
pages = {156-161},
year = {2024},
note = {34th CIRP Design Conference},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2024.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S2212827124006802},
author = {Tobias Grüble and Ralf Stetter and Timo Schuchter and Markus Till and Stephan Rudolph},
keywords = {model based systems engineering, behavior modelling, graph-based design languages},
abstract = {The main intention of the presented research is the development of an engineering framework that allows the automated generation of a combined geometric and kinetic digital twin of a robotic cell. The engineering framework is based on graph-based design languages and an appropriate compiler for their translation in order to realize a novel form of a machine-executable V-model for Model-Based Systems Engineering (MBSE). In this novel machine-executable MBSE process, the primary process management objects are no longer some documents but abstract process descriptions that can be automatically compiled into concrete product models. For such a holistic MBSE process, the models have to be enriched with physical behavior and performance information. The method is illustrated with a use-case of the engineering process of robotic cells. The starting point of the automated synthesis of the robotic cell with its resources is the abstract modelling by means of object-oriented programming. The use-case illustrates a pick-and-place operation that requires a specific geometry of a monolithic gripper. The gripper geometry synthesis is based on design automation with topology optimization, which is also realized inside the executable V-model. A large number of synthesis results from the geometry generating processes can be included in automated kinetic simulations, thus generating digital twins for a large solution spectrum to achieve an overall holistic optimization. Further, a digital twin created from geometrical data and kinetic simulation enables process monitoring based on physical values (joint forces, pressure, etc.) which then can be used to predict failure or evaluate new/optimized motion sequences.}
}
@article{GAZEHI2025126518,
title = {Classification of a nanocomposite using a combination between Recurrent Neural Network based on Transformer and Bayesian Network for testing the conductivity property},
journal = {Expert Systems with Applications},
volume = {270},
pages = {126518},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.126518},
url = {https://www.sciencedirect.com/science/article/pii/S095741742500140X},
author = {Wejden Gazehi and Rania Loukil and Mongi Besbes},
keywords = {Probability distribution, Classification, Nanoparticules, BN, RNN, Transformer},
abstract = {To enhance trust and adoption of machine learning models, particularly neural Networks, it is crucial to develop methods that allow quantifying and reporting epistemic uncertainties and the random uncertainties which lead to the non accuracy of RNN. To transcend these boundaries, we suggest to use Bayesian techniques, where probability distributions are employed to represent not only the model predictions but also the uncertainty associated with these predictions. Our study encapsulates the use of Bayesian Networks, RNNs based on the transformer and propose an hybrid approach for improving the classification of nanocomposites for evaluating the conductivity. In fact, it presents a comprehensive approach to classify nanocomposites using advanced Deep Learning techniques. Initially, a Bayesian Network (BN) model is employed to analyze and classify the nanocomposites, providing probabilistic insights into their conductive behavior based on the distribution and interaction of nanoparticles. Subsequently, a Recurrent Neural Network (RNN) based on the Transformer architecture is used to enhance the classification accuracy by capturing the sequential dependencies and complex patterns in the data. Finally, we propose a hybrid model that combines the strengths of both Bayesian Networks and RNNs. This integrated approach leverages the probabilistic reasoning of BNs and the Deep Learning capabilities of RNNs to achieve a more robust and accurate classification framework. The experimental results based on metric values indicate that the hybrid model significantly outperforms individual models, providing a powerful tool for the predictive classification of nanocomposites’ conductivity.}
}
@article{JAUNG2024128186,
title = {The need for human-centered design for AI robots in urban parks and forests},
journal = {Urban Forestry & Urban Greening},
volume = {91},
pages = {128186},
year = {2024},
issn = {1618-8667},
doi = {https://doi.org/10.1016/j.ufug.2023.128186},
url = {https://www.sciencedirect.com/science/article/pii/S1618866723003576},
author = {Wanggi Jaung},
keywords = {Urban green space, Big data analysis, Cultural ecosystem services, Human-centered AI, Human-computer interaction},
abstract = {The beneficial effects of urban parks and forests on the well-being of city dwellers have gained widespread recognition. In recent years, artificial intelligence (AI) robots have been employed in urban parks for various purposes, such as 24-hour patrol, COVID-19 physical distancing, and assisting visitors. While these robots offer significant benefits, visitors may harbor a negative perception towards autonomous robots, which could potentially undermine the recreational experiences in urban parks and forests. To test such impacts of AI robots, this study examines public perceptions of AI robots in urban parks. An extensive dataset of 36,520 comments from YouTube videos showcasing these robots was collected and analyzed using sentiment analysis, binomial logistic regressions, and topic modeling. The findings revealed that negative comments were widespread, encompassing their fear of AI robots controlling and harming people. These findings underscore the importance of adopting a human-centered approach in the design and implementation of AI robots in urban parks.}
}
@article{BURATTINI2025101560,
title = {Distributing intelligent functionalities in the Internet of Things with agents and Digital Twins},
journal = {Internet of Things},
volume = {31},
pages = {101560},
year = {2025},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2025.101560},
url = {https://www.sciencedirect.com/science/article/pii/S2542660525000733},
author = {Samuele Burattini and Stefano Mariani and Sara Montagna and Marco Picone and Alessandro Ricci},
keywords = {Distributed intelligence, Autonomous agents, Digital Twins, Internet of Things, Software engineering},
abstract = {Autonomous Agents (AAs) and Digital Twins (DTs) are two widely used abstractions in the literature about the engineering of “intelligent” Internet of Things (IoT) systems and applications. However, their role can be considered partially overlapping given the fragmented landscape of approaches emerging from the literature. There, in fact, either AAs or DTs, or their combination, are sometimes used for achieving the same goals. In this paper, we attempt to clarify similarities and differences of these abstractions and argue that the choice to use AAs or DTs (or an integration of the two) should stem from a principled analysis of the IoT system requirements. That is, by matching the desired intelligent functionalities with the properties of the two abstractions to find the most appropriate one. Accordingly, we (i) analyse the state-of-the-art approaches to identify how AAs and DTs are currently used to encapsulate and distribute intelligent functionalities across IoT system components; (ii) propose a set of principles to assist designers in choosing the most suitable abstraction for a given functionality; and (iii) discuss exemplary architectures that may arise from applying such principles. To conceptually validate our contribution, we analyse the practical case of an intelligent manufacturing system and show how following the outlined principles leads to interesting properties in the final system design.}
}
@article{KUMAR2022100780,
title = {Machine learning for energy-resource allocation, workflow scheduling and live migration in cloud computing: State-of-the-art survey},
journal = {Sustainable Computing: Informatics and Systems},
volume = {36},
pages = {100780},
year = {2022},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2022.100780},
url = {https://www.sciencedirect.com/science/article/pii/S2210537922001111},
author = {Yogesh Kumar and Surabhi Kaul and Yu-Chen Hu},
keywords = {Machine learning, Cloud computing, Energy optimization, Load allocation, Workflow scheduling, Live migration},
abstract = {Machine learning and artificial intelligence techniques have been proven helpful when pragmatic to a wide range of complex problems and areas such as energy optimization, workflow scheduling, video gaming, and cloud computing. When machine learning and cloud computing algorithms are combined, they help achieve better outcomes by providing the improved performance of cloud data centers compared to solutions currently employed by various researchers. It is also helpful for migrating the virtual machines based on the current traffic condition and fluctuation due to network congestion and bandwidth availability. The survey aims to present the improvement in dynamic load allocation, task scheduling, energy optimization, live migration, mobile cloud computing, and security on the cloud using machine learning classification. Machine learning algorithms are prevailing analytical approaches that allow machines to identify patterns and simplify the human learning process. The flow of the paper consists of an introduction part, motivation, and background study, including a framework for cloud-machine learning integration, best practices of introducing machine learning in cloud computing, and the objective of the work. The paper also highlights the machine learning-based cloud services and the role of artificial intelligence in different cloud computing platforms. This comprehensive study provides mindfulness and valuable facilities to the researchers by giving thorough studies about various machine learning algorithms and their applicability in cloud computing.}
}
@article{YAN2022104059,
title = {PhenoRerank: A re-ranking model for phenotypic concept recognition pre-trained on human phenotype ontology},
journal = {Journal of Biomedical Informatics},
volume = {129},
pages = {104059},
year = {2022},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2022.104059},
url = {https://www.sciencedirect.com/science/article/pii/S1532046422000752},
author = {Shankai Yan and Ling Luo and Po-Ting Lai and Daniel Veltri and Andrew J. Oler and Sandhya Xirasagar and Rajarshi Ghosh and Morgan Similuk and Peter N. Robinson and Zhiyong Lu},
keywords = {Phenotypic concept recognition, Natural language processing, Machine learning, Human phenotype ontology},
abstract = {The study aims at developing a neural network model to improve the performance of Human Phenotype Ontology (HPO) concept recognition tools. We used the terms, definitions, and comments about the phenotypic concepts in the HPO database to train our model. The document to be analyzed is first split into sentences and annotated with a base method to generate candidate concepts. The sentences, along with the candidate concepts, are then fed into the pre-trained model for re-ranking. Our model comprises the pre-trained BlueBERT and a feature selection module, followed by a contrastive loss. We re-ranked the results generated by three robust HPO annotation tools and compared the performance against most of the existing approaches. The experimental results show that our model can improve the performance of the existing methods. Significantly, it boosted 3.0% and 5.6% in F1 score on the two evaluated datasets compared with the base methods. It removed more than 80% of the false positives predicted by the base methods, resulting in up to 18% improvement in precision. Our model utilizes the descriptive data in the ontology and the contextual information in the sentences for re-ranking. The results indicate that the additional information and the re-ranking model can significantly enhance the precision of HPO concept recognition compared with the base method.}
}
@article{LIU2021102473,
title = {SELF: A method of searching for library functions in stripped binary code},
journal = {Computers & Security},
volume = {111},
pages = {102473},
year = {2021},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2021.102473},
url = {https://www.sciencedirect.com/science/article/pii/S0167404821002972},
author = {Xueqian Liu and Shoufeng Cao and Zhenzhong Cao and Qu Gao and Lin Wan and Fengyu Wang},
keywords = {Software reverse engineering, Binary code analysis, Library function, Semantic feature, Bigram, Co-occurrence matrix, Convolutional autoencoder},
abstract = {During software development, numerous third-party library functions are often reused. Accurately recognizing library functions reused in software is of great significance for some security scenarios, such as the detection of known vulnerabilities and reverse analyses of malware. An optional method for recognizing library functions is matching the functions in the library to those in the target software. However, due to the diversity of function library versions, compilers, build options, etc., there are differences between the two corresponding functions. Recognizing library functions used in target software precisely is still a challenging task. In this paper, we propose a novel method named SELF (SEarch for Library Functions) to recognize library functions used in target software. In SELF, the function is represented with a co-occurrence matrix and encoded by a convolutional auto-encoder (CAE). Then, the similarity between two functions is detected using the generated bottleneck features. This scheme focuses on the discriminative semantic features; thus, this method can not only distinguish different functions but also tolerate the subtle differences between two pairing functions, which is specifically required for library function recognition. We collected 451 software projects, including approximately 3 million functions, to train and evaluate SELF. The experimental results show that SELF performs well in both Recall@1 and Recall@5. Especially when the library version gap is large, SELF significantly outperforms classic BINDIFF. In addition, SELF shows good computational efficiency.}
}
@article{MAHAJAN2025100788,
title = {Enhancing personalization in IoT-based health monitoring via generative AI and transfer learning},
journal = {Egyptian Informatics Journal},
volume = {32},
pages = {100788},
year = {2025},
issn = {1110-8665},
doi = {https://doi.org/10.1016/j.eij.2025.100788},
url = {https://www.sciencedirect.com/science/article/pii/S1110866525001811},
author = {Rupali Atul Mahajan and Rajesh Dey and Mudassir Khan and Mazliham Mohd Su’ud and Muhammad Mansoor Alam and Pratibha Jadhav},
keywords = {IoT-based health monitoring, Personalized health monitoring, Generative AI, Transfer learning, Machine learning algorithms},
abstract = {Owing to the rapid expansion of Internet of Things (IoT) devices, the health care sector is responsible for immense amounts of real-time data, which provides an impetus for custom health metrics. In this context, the current research seeks to fill this gap by proposing a groundbreaking system that employs generative AI technologies and transfer learning in the field of IoT-based health monitoring. Before examining the IoT health data, we must remove any potential discrepancies and errors through data cleaning. An adaptive filter referred to as the delayed error normalized LMS (DENLMS) is a highly sophisticated method that essentially contributes to increasing the precision and accuracy of these particular data. By applying analysis in the frequency domain to the data, we were able to extract features via the fast Fourier transform (FFT) and subsequently review sessions that contained, for example, heart rate variability or respiratory signals over time. The process of developing a generative AI model for personal health monitoring involves selecting suitable models, such as generative adversarial networks (GANs) or variational autoencoders (VAEs), owing to their ability to generate and simulate health data patterns effectively. To facilitate functional data analysis, the system design integrates machine learning techniques with generative models for patient data from various IoT devices. Importantly, the accuracy rate of this technique is 95.6%, the precision rate is 96.4%, the recall rate is 94.7%, and the F1 score is 95.5%. These metrics surpass those of most other techniques described in this study, demonstrating the superior performance of this research technique over other generic algorithms and its implementation with Python software. Future research could also focus on addressing the seemingly trivial challenge of enhancing model adaptability and scalability to meet individual health requirements and integrate multiple data sources.}
}
@article{DAKAKNI2023100179,
title = {Artificial intelligence in the L2 classroom: Implications and challenges on ethics and equity in higher education: A 21st century Pandora's box},
journal = {Computers and Education: Artificial Intelligence},
volume = {5},
pages = {100179},
year = {2023},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2023.100179},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X23000589},
author = {Deema Dakakni and Nehme Safa},
keywords = {Artificial intelligence, Ethical uses of AI, L2 classrooms, Digital technology and L2 learning, Chatbot GPT and ethics, Writing mills},
abstract = {The purpose of this research was to investigate attitudes of both students and teachers concerning Artificial Intelligence (AI) tools in the L2 classroom. The study was a descriptive, qualitative, mixedmethods case study whose data were taken from a purposive, convenient sample at a private, English-speaking university during the Summer Semester 2023 in Beirut, Lebanon. Data collection primarily involved an online survey on Google forms which was given to a sample of 49 students taking a research-based English 202 course of which 46 were completed. Afterwards, six English teachers and six students were chosen based on their voluntary will to participate in individual interviews for the former and semi-structured focus group interviews for the latter. The findings revealed that approximately 85% of students did indeed use AI unethically to get ideas for their assignments, assist them in their projects' “blue-prints” or do their assignments/projects altogether. The findings also revealed that a “love/hate” relationship seemed to dictate students' relationships with AI, where students did indeed make use of AI but were distrusting of it for privacy and equity concerns. Finally, findings also revealed that most of the interviewed instructors' readiness to undergo training for AI was more to monitor students' potential misuse of it. The article purposes a suggestive revamping of course learning objectives due to students' inclinations to misuse AI to do their coursework with 89.4% of students willing to use AI to complete their coursework should university punitive measures be removed; furthermore, the article equally proposes future research investigating the impact and use of AI in the higher educational classroom on student performance and that it be used with a “grain of salt” as it may unleash a Pandora's box of future generations graduating without the necessary know-how in delicate professions of medicine, nursing, engineering, architecture among others.}
}
@article{AKBARIGHATAR2023100193,
title = {A sociotechnical perspective for responsible AI maturity models: Findings from a mixed-method literature review},
journal = {International Journal of Information Management Data Insights},
volume = {3},
number = {2},
pages = {100193},
year = {2023},
issn = {2667-0968},
doi = {https://doi.org/10.1016/j.jjimei.2023.100193},
url = {https://www.sciencedirect.com/science/article/pii/S266709682300040X},
author = {Pouria Akbarighatar and Ilias Pappas and Polyxeni Vassilakopoulou},
keywords = {Artificial intelligence, Maturity model, Responsible AI capabilities, Topic modeling, Sociotechnical},
abstract = {As artificial intelligence (AI) is increasingly used in various industries, it becomes crucial for organizations to enhance their capabilities and maturity in adopting AI responsibly. This paper employs a mixed-method approach that combines topic modeling with manual content analysis to provide a comprehensive review of the literature on AI maturity and readiness. The review encompasses an extensive corpus of 1451 papers, identifying the main themes and topics within this body of literature. Based on these findings, a subset of papers was selected and further analyzed to identify AI capabilities utilizing a sociotechnical lens. This further analysis led to the identification of foundational and responsible AI (RAI) capabilities. These capabilities have been integrated in a sociotechnical framework of capabilities for AI maturity models providing valuable insights for organizations and AI service providers and a basis for further research.}
}
@article{LU2024103753,
title = {Investigating OTA employees’ double-edged perceptions of ChatGPT: The moderating role of organizational support},
journal = {International Journal of Hospitality Management},
volume = {120},
pages = {103753},
year = {2024},
issn = {0278-4319},
doi = {https://doi.org/10.1016/j.ijhm.2024.103753},
url = {https://www.sciencedirect.com/science/article/pii/S0278431924000653},
author = {Lan Lu and Jinlin Zhao and Haoran Chen},
keywords = {Perceived benefits, Perceived risk, Job insecurity, Turnover intention, Organizational support},
abstract = {Based on the conservation of resources theory, this study investigated the relationships between the online travel agency employees' perceived benefits and risks of ChatGPT, job insecurity, and turnover intention. Additionally, we also examined the mediating role of job insecurity and the moderating role of organizational support. Using data from a sample of 432 United States OTA employees, the findings demonstrated that the perceived benefits and risks of ChatGPT significantly affected perceived job insecurity. Moreover, the perceived benefits and risks of ChatGPT indirectly influenced turnover intention through the intermediary variable of perceived job insecurity. Organizational support positively moderated the impact of perceived benefits and negatively moderated perceived risks on job insecurity and turnover intentions, thus helping employees cope with challenges and reduce uncertainty. The findings underscore the need for organizations to foster supportive environments to manage the impact of ChatGPT on OTA employee retention. The theoretical and practical implications were discussed.}
}
@article{GAO2022111118,
title = {Sharing runtime permission issues for developers based on similar-app review mining},
journal = {Journal of Systems and Software},
volume = {184},
pages = {111118},
year = {2022},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2021.111118},
url = {https://www.sciencedirect.com/science/article/pii/S0164121221002156},
author = {Hongcan Gao and Chenkai Guo and Guangdong Bai and Dengrong Huang and Zhen He and Yanfeng Wu and Jing Xu},
keywords = {Android, Runtime permission, Permission-Related Issues (PRIS), User reviews, Machine learning},
abstract = {The Android operating system introduces an ask-on-first-use permission policy after 6.0 version to regulate access to user data, which raises Permission-Related Issues (PRIS for short). Relevant research has been conducted to identify the PRIS through investigating users’ opinions towards runtime permissions. These efforts mainly focus on helping users understand and be aware of permissions, but neglect to assist developers in discovering permission requirements. In this paper, we propose a novel framework named PRISharer, which mines potential permission issues from the reviews of similar apps to assist developers in discovering possible permission requirements at runtime. PRISharer first builds a deep fine-grained classifier to identify similar apps, and then employs sentiment analysis based keywords extraction to mine permission-related reviews from similar apps’ reviews. Finally, the <category, permission, issues> mappings based on a multi-label learning method are generated to provide a PRIS profile for developers. The results of comparative experiments on more than 12 million reviews of 17,741 Android apps demonstrate that PRISharer achieves (i) superior performance in terms of F1-score for PRIS analysis, with an average improvement of 24.4%, (ii) the best recall (89.3%) in extracting permission-related reviews and (iii) 82.4% positive responses by expert developers, through which the effectiveness of PRISharer is well verified.}
}
@article{KARAYIGIT2021114802,
title = {Detecting abusive Instagram comments in Turkish using convolutional Neural network and machine learning methods},
journal = {Expert Systems with Applications},
volume = {174},
pages = {114802},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.114802},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421002438},
author = {Habibe Karayiğit and Çiğdem {İnan Acı} and Ali Akdağlı},
keywords = {Abusive comment, Hate speech, Classification, Social media, Instagram, Dataset},
abstract = {Instagram is a free photo-sharing platform where each user has a profile and can upload photos for followers to view, like, and comment. Abusive comments on images can be humiliating and harmful to those who share photos. Developing a comment filter in languages other than English is difficult and time-consuming. This paper proposes a dataset called Abusive Turkish Comments (ATC) to detect abusive Instagram comments in Turkish. It is composed of a large number of Instagram comments posted to tabloid and sports accounts (i.e., 10,528 abusive and 19,826 not-abusive). It is the first public dataset dedicated to detecting abusive Turkish messages, as far as we know. The sentiment annotation has been done in sentence-level by assigning polarity to each comment. The performance of the abusive message detection models was evaluated using several performance metrics: Convolutional Neural Network (CNN), five well-known classifiers (i.e., Naive Bayes, Support Vector Machine, Decision Tree, Random Forest, and Logistic Regression), and two reweighted classifiers (i.e., Adaptive Boosting (AdaBoost), eXtreme Gradient Boosting (XGBoost)) were compared in terms of F1-score, precision, and recall. The results showed that the best performance (i.e., Micro-averaged F1-score: 0.974, Macro-averaged F1-score: 0.973, Kappa-value: 0.946) was yielded by the CNN model on the oversampled ATC dataset. The abusive message detection model proposed in this study can contribute to the development of Turkish comment filters on Instagram. Different model combinations are considered to select the best model that gives better recognition accuracy.}
}
@article{PATHAK2024100094,
title = {AI as decision aid or delegated agent: The effects of trust dimensions on the adoption of AI digital agents},
journal = {Computers in Human Behavior: Artificial Humans},
volume = {2},
number = {2},
pages = {100094},
year = {2024},
issn = {2949-8821},
doi = {https://doi.org/10.1016/j.chbah.2024.100094},
url = {https://www.sciencedirect.com/science/article/pii/S2949882124000549},
author = {Aman Pathak and Veena Bansal},
keywords = {Cognitive trust, Affective trust, Social trust, FsQCA, Consumer service},
abstract = {AI digital agents may act as decision-aid or as delegated agents. A decision-aid agent helps a user make decisions, whereas a delegated agent makes decisions on behalf of the consumer. The study determines the factors affecting the adoption intention of AI digital agents as decision aids and delegated agents. The domain of study is banking, financial services, and Insurance sector (BFSI). Due to the unique characteristics of AI digital agents, trust has been identified as an important construct in the extant literature. The study decomposed trust into social, cognitive, and affective trust. We incorporated PLS-SEM and fsQCA to examine the factors drawn from the literature. The findings from PLS-SEM suggest that perceived AI quality affects cognitive trust, perceived usefulness affects affective trust, and social trust affects cognitive and affective trust. The intention to adopt AI as a decision-aid is influenced by affective and cognitive trust. The intention to adopt AI as delegated agents is influenced by social, cognitive, and affective trust. FsQCA findings indicate that combining AI quality, perceived usefulness, and trust (social, cognitive, and affective) best explains the intention to adopt AI as a decision aid and delegated agents.}
}
@article{AMIRI2024132827,
title = {Comprehensive survey of artificial intelligence techniques and strategies for climate change mitigation},
journal = {Energy},
volume = {308},
pages = {132827},
year = {2024},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2024.132827},
url = {https://www.sciencedirect.com/science/article/pii/S036054422402601X},
author = {Zahra Amiri and Arash Heidari and Nima Jafari Navimipour},
keywords = {Climate change, Artificial intelligence, Systematic literature review, Machine learning, Deep learning},
abstract = {With the galloping progress of the changing climates all around the world, Machine Learning (ML) approaches have been prevalently studied in many types of research in this area. ML is a robust tool for acquiring perspectives from data. In this paper, we elaborate on climate change mitigation issues and ML approaches leveraged to solve these issues and aid in the improvement and function of sustainable energy systems. ML has been employed in multiple applications and many scopes of climate subjects such as ecosystems, agriculture, buildings and cities, industry, and transportation. So, a Systematic Literature Review (SLR) is applied to explore and evaluate findings from related research. In this paper, we propose a novel taxonomy of Deep Learning (DL) method applications for climate change mitigation, a comprehensive analysis that has not been conducted before. We evaluated these methods based on critical parameters such as accuracy, scalability, and interpretability and quantitatively compared their results. This analysis provides new insights into the effectiveness and reliability of DL methods in addressing climate change challenges. We classified climate change ML methods into six key customizable groups: ecosystems, industry, buildings and cities, transportation, agriculture, and hybrid applications. Afterward, state-of-the-art research on ML mechanisms and applications for climate change mitigation issues has been highlighted. In addition, many problems and issues related to ML implementation for climate change have been mapped, which are predicted to stimulate more researchers to manage the future disastrous effects of climate change. Based on the findings, most of the papers utilized Python as the most common simulation environment 38.5 % of the time. In addition, most of the methods were analyzed and evaluated in terms of some parameters, namely accuracy, latency, adaptability, and scalability, respectively. Lastly, classification is the most frequent ML task within climate change mitigation, accounting for 40 % of the total. Furthermore, Convolutional Neural Networks (CNNs) are the most widely utilized approach for a variety of applications.}
}
@article{YOUSAFZAI2024737,
title = {X-News dataset for online news categorization},
journal = {International Journal of Intelligent Computing and Cybernetics},
volume = {17},
number = {4},
pages = {737-758},
year = {2024},
issn = {1756-378X},
doi = {https://doi.org/10.1108/IJICC-04-2024-0184},
url = {https://www.sciencedirect.com/science/article/pii/S1756378X24000037},
author = {Samia Nawaz Yousafzai and Hooria Shahbaz and Armughan Ali and Amreen Qamar and Inzamam Mashood Nasir and Sara Tehsin and Robertas Damaševičius},
keywords = {News categorization, BERT classifier, POS tagging, Social media analytics, Deep learning for text analysis, Sentiment analysis},
abstract = {Purpose
The objective is to develop a more effective model that simplifies and accelerates the news classification process using advanced text mining and deep learning (DL) techniques. A distributed framework utilizing Bidirectional Encoder Representations from Transformers (BERT) was developed to classify news headlines. This approach leverages various text mining and DL techniques on a distributed infrastructure, aiming to offer an alternative to traditional news classification methods.
Design/methodology/approach
This study focuses on the classification of distinct types of news by analyzing tweets from various news channels. It addresses the limitations of using benchmark datasets for news classification, which often result in models that are impractical for real-world applications.
Findings
The framework’s effectiveness was evaluated on a newly proposed dataset and two additional benchmark datasets from the Kaggle repository, assessing the performance of each text mining and classification method across these datasets. The results of this study demonstrate that the proposed strategy significantly outperforms other approaches in terms of accuracy and execution time. This indicates that the distributed framework, coupled with the use of BERT for text analysis, provides a robust solution for analyzing large volumes of data efficiently. The findings also highlight the value of the newly released corpus for further research in news classification and emotion classification, suggesting its potential to facilitate advancements in these areas.
Originality/value
This research introduces an innovative distributed framework for news classification that addresses the shortcomings of models trained on benchmark datasets. By utilizing cutting-edge techniques and a novel dataset, the study offers significant improvements in accuracy and processing speed. The release of the corpus represents a valuable contribution to the field, enabling further exploration into news and emotion classification. This work sets a new standard for the analysis of news data, offering practical implications for the development of more effective and efficient news classification systems.}
}
@article{LI2025103277,
title = {HSE: A plug-and-play module for unified fault diagnosis foundation models},
journal = {Information Fusion},
volume = {123},
pages = {103277},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2025.103277},
url = {https://www.sciencedirect.com/science/article/pii/S1566253525003501},
author = {Qi Li and Bojian Chen and Qitong Chen and Xuan Li and Zhaoye Qin and Fulei Chu},
keywords = {Intelligent fault diagnosis, Heterogeneous signal embedding, Signal fusion, Signal foundation model, Plug-and-play module},
abstract = {Intelligent Fault Diagnosis (IFD) plays a crucial role in industrial applications, where developing foundation models analogous to ChatGPT for comprehensive fault diagnosis remains a significant challenge. Current IFD methodologies are constrained by their inability to construct unified models capable of processing heterogeneous signal types, varying sampling rates, and diverse signal lengths across different equipment. To address these limitations, we propose a novel Heterogeneous Signal Embedding (HSE) module that projects heterogeneous signals into a unified signal space, offering seamless integration with existing IFD architectures as a plug-and-play solution. The HSE framework comprises two primary components: the Temporal-Aware Patching (TAP) module for embedding heterogeneous signals into a unified space, and the Cross-Dimensional Patch Fusion (CDPF) module for fusing embedded signals with temporal information into unified representations. We validate the efficacy of HSE through two comprehensive case studies: a simulation signal dataset and three distinct bearing datasets with heterogeneous features. Our experimental results demonstrate that HSE significantly enhances traditional fault diagnosis models, improving both diagnostic accuracy and generalization capability. While conventional approaches necessitate separate models for specific signal types, sampling frequencies, and signal lengths, HSE-enabled architectures successfully learn unified representations across diverse signal. The results from bearing fault diagnosis applications confirm substantial improvements in both diagnostic precision and cross-dataset generalization. As a pioneering contribution toward IFD foundation models, the proposed HSE framework establishes a fundamental architecture for advancing unified fault diagnosis systems.}
}
@article{YANG2023111577,
title = {ExploitGen: Template-augmented exploit code generation based on CodeBERT},
journal = {Journal of Systems and Software},
volume = {197},
pages = {111577},
year = {2023},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2022.111577},
url = {https://www.sciencedirect.com/science/article/pii/S0164121222002539},
author = {Guang Yang and Yu Zhou and Xiang Chen and Xiangyu Zhang and Tingting Han and Taolue Chen},
keywords = {Exploit code, Code generation, Template parser, CodeBERT, Neural network},
abstract = {Exploit code is widely used for detecting vulnerabilities and implementing defensive measures. However, automatic generation of exploit code for security assessment is a challenging task. In this paper, we propose a novel template-augmented exploit code generation approach ExploitGen based on CodeBERT. Specifically, we first propose a rule-based Template Parser to generate template-augmented natural language descriptions (NL). Both the raw and template-augmented NL sequences are encoded to context vectors by the respective encoders. For better learning semantic information, ExploitGen incorporates a semantic attention layer, which uses the attention mechanism to extract and calculate each layer’s representational information. In addition, ExploitGen computes the interaction information between the template information and the semantics of the raw NL and designs a residual connection to append the template information into the semantics of the raw NL. Comprehensive experiments on two datasets show the effectiveness of ExploitGen after comparison with six state-of-the-art baselines. Apart from the automatic evaluation, we conduct a human study to evaluate the quality of generated code in terms of syntactic and semantic correctness. The results also confirm the effectiveness of ExploitGen.}
}
@article{LEE2024143049,
title = {Universal artificial intelligence workflow for factory energy saving: Ten case studies},
journal = {Journal of Cleaner Production},
volume = {468},
pages = {143049},
year = {2024},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2024.143049},
url = {https://www.sciencedirect.com/science/article/pii/S0959652624024983},
author = {Dasheng Lee and Chienchieh Lin},
keywords = {Artificial intelligence, AI, Non-AI method, Factory energy saving, Universal workflow, Return on investment, ROI},
abstract = {Numerous studies have affirmed that artificial intelligence (AI) can effectively enable energy savings in factories. However, there is currently a lack of explicit research that identifies the energy-saving effects of AI methods as compared to the conventional practices employed in factories, which involve the replacement of equipment with high energy efficiency (non-AI method), with respect to variance in the effectiveness. Thus, this ignited the motivation for this study, wherein the research team developed a “Universal AI workflow for energy saving” which adopted a standard workflow of “learning, Modeling, and Prediction.” Additionally, a three-year research project was launched to conduct empirical tests in ten factories, comparing the efficacy differences between AI and Non-AI methods. In 2021, research team established the energy consumption baselines for each factory through data collection; the entirety of 2022 was dedicated to AI integrations; and energy-saving benefits were evaluated in 2023. During the research period, each factory also implemented other non-AI energy-saving methods which were juxtaposed with AI approaches for comparison. Our study results showed that average AI-enabled energy savings across the ten factories amounted to 106,124 kWh/yr, while average energy savings from non-AI methods amounted to 1,231,625 kWh/yr. Despite the inferior energy-saving results, the AI methods had an average return on investment (ROI) of 0.46 years while non-AI methods had an average ROI of 6.22 years, indicating that AI methods could recover the investment costs through energy savings in less than one year. It is also worth noting that the ten factories produced a diverse range of products encompassing wafers, steel, and foods, but the AI methods used for energy saving employed the same workflow. By contrast, the non-AI methods necessitated distinct workflows and detailed engineering adjustments. It can be inferred from these empirical findings that AI techniques for achieving energy-saving control in factories can be integrated using standardized processes. In summary, AI methods can facilitate standardized workflows in factories and offer substantial energy-saving benefits that can typically cover investments within a year. AI represents a highly viable energy conservation option for factories seeking to maintain uniformity in product processes and realize efficient ROIs.}
}