TY  - JOUR
T1  - Transforming towards inclusion-by-design: Information system design principles shaping data-driven financial inclusiveness
AU  - Sulastri, Reni
AU  - Janssen, Marijn
AU  - van de Poel, Ibo
AU  - Ding, Aaron
JO  - Government Information Quarterly
VL  - 41
IS  - 4
SP  - 101979
PY  - 2024
DA  - 2024/12/01/
SN  - 0740-624X
DO  - https://doi.org/10.1016/j.giq.2024.101979
UR  - https://www.sciencedirect.com/science/article/pii/S0740624X24000716
KW  - Digital governance
KW  - Inclusion
KW  - Lending systems
KW  - Value-based requirements
KW  - Inclusion by design
KW  - System-level transformation
AB  - Digitalization and datafication of financial systems result in more efficiency, but might also result in the exclusions of certain groups. Governments are looking for ways to increase inclusions and leave no one behind. For this, they must govern an organizational ecosystem of public and private parties. We derive value-based requirements through a systematic research methodology and iteratively refine design principles for achieving inclusivity goals. This refinement process is enriched by interviews with field experts, leading to the formulation of key Design principles: the essential role of inclusive metrics, leveraging alternative data sources, ensuring transparency in loan processes and the ability for decision contestation, providing tailored credit solutions, and maintaining long-term system sustainability. The government's role is to ensure a level playing field where all parties have equal access to the data. Following the principles ensures that exclusion and discrimination become visible and can be avoided. This study underscores the necessity for system-level transformations, inclusion-by-design, and advocacy for a new system design complemented by regulatory updates, new data integration, inclusive AI, and organizational collaborative shifts. These principles can also be used in different data-driven governance situations.
ER  - 

TY  - JOUR
T1  - Transferable Features from 1D-Convolutional Network for Industrial Malware Classification
AU  - Wang, Liwei
AU  - Sun, Jiankun
AU  - Luo, Xiong
AU  - Yang, Xi
JO  - CMES - Computer Modeling in Engineering and Sciences
VL  - 130
IS  - 2
SP  - 1003
EP  - 1016
PY  - 2021
DA  - 2021/12/10/
SN  - 1526-1492
DO  - https://doi.org/10.32604/cmes.2022.018492
UR  - https://www.sciencedirect.com/science/article/pii/S152614922100059X
KW  - Transfer learning
KW  - malware classification
KW  - sequence data modeling
KW  - convolutional network
AB  - With the development of information technology, malware threats to the industrial system have become an emergent issue, since various industrial infrastructures have been deeply integrated into our modern works and lives. To identify and classify new malware variants, different types of deep learning models have been widely explored recently. Generally, sufficient data is usually required to achieve a well-trained deep learning classifier with satisfactory generalization ability. However, in current practical applications, an ample supply of data is absent in most specific industrial malware detection scenarios. Transfer learning as an effective approach can be used to alleviate the influence of the small sample size problem. In addition, it can also reuse the knowledge from pre-trained models, which is beneficial to the real-time requirement in industrial malware detection. In this paper, we investigate the transferable features learned by a 1D-convolutional network and evaluate our proposed methods on 6 transfer learning tasks. The experiment results show that 1D-convolutional architecture is effective to learn transferable features for malware classification, and indicate that transferring the first 2 layers of our proposed 1D-convolutional network is the most efficient way to reuse the learned features.
ER  - 

TY  - JOUR
T1  - Green artificial intelligence initiatives: Potentials and challenges
AU  - Alzoubi, Yehia Ibrahim
AU  - Mishra, Alok
JO  - Journal of Cleaner Production
VL  - 468
SP  - 143090
PY  - 2024
DA  - 2024/08/25/
SN  - 0959-6526
DO  - https://doi.org/10.1016/j.jclepro.2024.143090
UR  - https://www.sciencedirect.com/science/article/pii/S0959652624025393
KW  - Artificial intelligence
KW  - Carbon footprint
KW  - Cloud
KW  - Green AI
KW  - Green AI tools
KW  - Sustainability
AB  - Recently, the widespread adoption of artificial intelligence, particularly generative AI technology, has surged across various industries. However, a notable drawback of this technology is its significant energy consumption during model training and operation, which poses challenges to sustainability goals and the environment. Consequently, various initiatives have emerged to promote what is termed "green artificial intelligence," aiming to mitigate these environmental impacts. Nevertheless, research discussing these initiatives remains scarce. Hence, this study aims to identify green artificial intelligence initiatives that contribute to environmental friendliness. This paper has comprehensively reviewed the existing literature, professional websites, and expert blogs to identify and analyze available green AI initiatives. This paper has identified 55 such initiatives, broadly categorized into six themes: cloud optimization, model efficiency, carbon footprinting, sustainability-focused AI development, open-source initiatives, and green AI research and community. This study discusses the strengths and limitations of each initiative to offer a comprehensive overview. The findings provide valuable insights, particularly for industries interested in green artificial intelligence and green technology in general. While some tools have been recognized and studied, comprehensive research and analysis are still required to empirically evaluate the majority of other tools due to their early stages of development in this field.
ER  - 

TY  - JOUR
T1  - Who gets the money? A qualitative analysis of fintech lending and credit scoring through the adoption of AI and alternative data
AU  - Tigges, Maximilian
AU  - Mestwerdt, Sönke
AU  - Tschirner, Sebastian
AU  - Mauer, René
JO  - Technological Forecasting and Social Change
VL  - 205
SP  - 123491
PY  - 2024
DA  - 2024/08/01/
SN  - 0040-1625
DO  - https://doi.org/10.1016/j.techfore.2024.123491
UR  - https://www.sciencedirect.com/science/article/pii/S0040162524002877
KW  - Alternative data
KW  - Credit scoring
KW  - Fintech
KW  - Artificial intelligence
KW  - Ethics
AB  - Credit scoring plays an important role in determining the accessibility of credit in the financial sector. This in turn has a significant impact on how economic opportunities are distributed. Our study examines the use of AI and alternative data in fintech lending through the lens of Information Asymmetry Theory. By employing a qualitative research design using the Gioia method, we extract, analyze, and synthesize insights from a diverse group of 26 experts in fintech lending, artificial intelligence, machine learning, data science, and academia. Our results reveal several important findings: the enhancement of predictive proficiency and risk management, the decrease in default rates, the extension of credit access by including previously ‘unbanked populations’, the introduction of real-time creditworthiness assessment and new business models for entrepreneurs, the enhancement of credit market efficiencies and positive effects on the stability of financial markets. In addition, our study highlights the necessity for rigorous and critical ethical considerations of important challenges such as the question of consent, algorithmic transparency, data quality, data misuse, representativeness, traceability, responsibility, bias and discrimination. The reasonable goal of a more fair, resilient, sustainable and accessible credit system will require a joint effort to balance leveraging technological innovations with respecting peoples' right to privacy.
ER  - 

TY  - JOUR
T1  - Navigating the muddy waters of bias in artificial intelligence research: Understanding divergent meanings and conceptions
AU  - Jarrahi, Mohammad Hossein
AU  - Karami, Amir
AU  - Conway, Patrick
AU  - Memariani, Ali
AU  - Lutz, Christoph
JO  - Technology in Society
VL  - 84
SP  - 103127
PY  - 2026
DA  - 2026/03/01/
SN  - 0160-791X
DO  - https://doi.org/10.1016/j.techsoc.2025.103127
UR  - https://www.sciencedirect.com/science/article/pii/S0160791X25003173
KW  - Artificial intelligence
KW  - Bias
KW  - Sociotechnical
KW  - Machine learning
KW  - Topic modeling
AB  - As artificial intelligence (AI) pervades many decision-making domains, AI bias grows in importance. Although there is increasing awareness of the social and ethical consequences of biased AI, understanding bias from the perspective of those who develop these systems, such as the AI research community, is less clear. In this study, we employ topic modeling on 6520 articles to explore how the AI research community interprets the concept of bias. Our results show that the definition of bias is dispersed and complex within the community, often exhibiting even divergent conceptions (some even view and introduce bias as a tunable statistical parameter rather than an undesirable issue). The research community as a whole needs to engage more effectively with the concept of bias and establish a more cohesive understanding of it. We specifically argue that, although some sub-communities view bias as an issue that can be captured and mitigated through technical, computational, or statistical methods, it is not solely a technical problem. It instead involves contextual, social, and ethical factors that require broader sociotechnical perspectives and solutions.
ER  - 

TY  - JOUR
T1  - Automated event extraction of CVE descriptions
AU  - Wei, Ying
AU  - Bo, Lili
AU  - Sun, Xiaobing
AU  - Li, Bin
AU  - Zhang, Tao
AU  - Tao, Chuanqi
JO  - Information and Software Technology
VL  - 158
SP  - 107178
PY  - 2023
DA  - 2023/06/01/
SN  - 0950-5849
DO  - https://doi.org/10.1016/j.infsof.2023.107178
UR  - https://www.sciencedirect.com/science/article/pii/S0950584923000320
KW  - Vulnerability events
KW  - Event extraction
KW  - Vulnerability analysis
AB  - Context:
The dramatically increasing number of vulnerabilities makes manual vulnerability analysis increasingly more difficult. Automatic extraction of vulnerability information can help improve vulnerability analysis. However, the existing vulnerability information extraction methods do not extract from the perspective of events, and the existing event extraction methods do not consider the unique sentence structure characteristics of vulnerability descriptions, which makes it difficult to extract vulnerability information effectively.
Objective:
To extract vulnerability information, we treat each vulnerability as an event, and propose an approach, VE-Extractor, to automatically perform vulnerability event extraction from textual descriptions in vulnerability reports for vulnerability analysis, including extraction of vulnerability event trigger (cause) and event arguments (e.g., consequence, operation).
Method:
First, we propose a new labeling method BIOFR (Begin, Inside, Outside, Front, Rear) to construct an event-perspective vulnerability data benchmark. Then, we design a question template based on event trigger, to automatically extract vulnerability event arguments through the BERT Q&A model.
Results:
Experiments show the effectiveness of VE-Extractor for automatically extracting events from vulnerability description, with significant performance improvement over state-of-the-art techniques, e.g., F1-score is increased by 45.12% and 21.02% in vulnerability consequence and operation extraction, respectively.
Conclusion:
The proposed VE-Extractor achieves a higher precision and accuracy than the state-of-the-art methods. Experiments results show that our approach is effective in extracting vulnerability event information and can be used to assist vulnerability analysis, such as vulnerability classification.
ER  - 

TY  - JOUR
T1  - Blockchain For Logistics 4.0: A Systematic Review and Prospects
AU  - Rachana Harish, Arjun
AU  - Liu, Xinlai
AU  - Wang, Xin
AU  - Pan, Shenle
AU  - Dai, Hong-Ning
AU  - Li, Ming
AU  - Huang, George Q.
JO  - Transportation Research Part E: Logistics and Transportation Review
VL  - 201
SP  - 104269
PY  - 2025
DA  - 2025/09/01/
SN  - 1366-5545
DO  - https://doi.org/10.1016/j.tre.2025.104269
UR  - https://www.sciencedirect.com/science/article/pii/S1366554525003102
KW  - Logistics 4.0
KW  - Blockchain
KW  - Decentralized ledger
KW  - Smart contract
KW  - Systematic Literature Review (SLR)
AB  - Logistics 4.0, with its supporting technologies, is gaining popularity with researchers and practitioners as a way to navigate the dynamic and demanding logistics environment. Blockchain technology is such a topical technology in logistics. The logistics industry benefits from blockchain as it enhances communication and collaboration among stakeholders through applications such as information sharing, goods monitoring and tracing, and executing financing and payments. However, the perceived lack of clarity on the adoption benefits has increased decision-maker’s hesitation to adopt and utilize blockchain technology in the logistics industry. To fully understand blockchain applications in the context of logistics, this paper conducts a systematic literature review (SLR) on blockchain research in logistics. A selection of 113 articles undergoes review based on a two-axis framework. The first axis lists the core blockchain technologies, such as decentralized ledger, consensus mechanism, and smart contract. It comprehensively reviews their interplay with the foundational logistics requirements, such as agility, collaboration, and resilience. The second axis reviews domains such as e-commerce logistics, business logistics, green logistics, and logistics financing, which are key avenues for blockchain applications. The key themes arising from the review are a precursor to deriving avenues for future research. Finally, we present Web 4.0 in logistics, the new wave of generative artificial intelligence (AI)-empowered blockchain innovation that can shape future logistics. More specifically, we explore the role of modern AI agents in driving future research for seamless blockchain integration to logistics, enabling logistics ESG (Environmental, Social, and Governance) and organizational adoption. Besides addressing a gap in existing literature, the concepts discussed in this study allow for a comprehensive understanding of blockchain adoption in logistics, thus encouraging wider practical application of novel technology. From a theoretical lens, this study helps comprehend blockchain technology and its significance to logistics research.
ER  - 

TY  - JOUR
T1  - Combining AI and AM – Improving approximate matching through transformer networks
AU  - Uhlig, Frieder
AU  - Struppek, Lukas
AU  - Hintersdorf, Dominik
AU  - Göbel, Thomas
AU  - Baier, Harald
AU  - Kersting, Kristian
JO  - Forensic Science International: Digital Investigation
VL  - 45
SP  - 301570
PY  - 2023
DA  - 2023/07/01/
SN  - 2666-2817
DO  - https://doi.org/10.1016/j.fsidi.2023.301570
UR  - https://www.sciencedirect.com/science/article/pii/S2666281723000793
KW  - Deep learning approximate matching
KW  - DLAM
KW  - Fuzzy hashes
KW  - Approximate matching
KW  - Transformer
KW  - Deep learning
KW  - Artificial intelligence
AB  - Approximate matching is a well-known concept in digital forensics to determine the similarity between digital artifacts. An important use case of approximate matching is the reliable and efficient detection of case-relevant data structures on a blacklist (e.g., malware or corporate secrets), if only fragments of the original are available. For instance, if only a cluster of indexed malware is still present during the digital forensic investigation, the approximate matching algorithm shall be able to assign the fragment to the blacklisted malware. However, traditional approximate matching functions like TLSH and ssdeep fail to detect files based on their fragments if the presented piece is relatively small compared to the overall file size (e.g., like one-third of the total file). A second well-known issue with traditional approximate matching algorithms is the lack of scaling due to the ever-increasing lookup databases. In this paper, we propose an improved matching algorithm based on transformer-based models from the field of natural language processing. We call our approach Deep Learning Approximate Matching (DLAM). As a concept from artificial intelligence, DLAM gets knowledge of characteristic blacklisted patterns during its training phase. Then DLAM is able to detect the patterns in a typically much larger file, that is DLAM focuses on the use case of fragment detection. Our evaluation is inspired by two widespread blacklist use cases: the detection of malware (e.g., in JavaScript) and corporate secrets (e.g., pdf or office documents). We reveal that DLAM has three key advantages compared to the prominent conventional approaches TLSH and ssdeep. First, it makes the tedious extraction of known to be bad parts obsolete, which is necessary until now before any search for them with approximate matching algorithms. This allows efficient classification of files on a much larger scale, which is important due to exponentially increasing data to be investigated. Second, depending on the use case, DLAM achieves a similar (in case of mrsh-cf and mrsh-v2) or even significantly higher accuracy (in case of ssdeep and TLSH) in recovering fragments of blacklisted files. For instance, in the case of JavaScript files, our assessment shows that DLAM provides an accuracy of 93% on our test corpus, while TLSH and ssdeep show a classification accuracy of only 50%. Third, we show that DLAM enables the detection of file correlations in the output of TLSH and ssdeep even for fragment sizes, where the respective matching function of TLSH and ssdeep fails.
ER  - 

TY  - JOUR
T1  - Developing and leveraging digital twins in engineering design
AU  - Anwer, N.
AU  - Stark, R.
AU  - Tao, F.
AU  - Erkoyuncu, J.A.
JO  - CIRP Annals
VL  - 74
IS  - 2
SP  - 843
EP  - 868
PY  - 2025
DA  - 2025/01/01/
SN  - 0007-8506
DO  - https://doi.org/10.1016/j.cirp.2025.05.002
UR  - https://www.sciencedirect.com/science/article/pii/S0007850625001477
KW  - Design
KW  - Digital twin
KW  - Digital twin engineering
AB  - Digital twins are digital representations of real-world entities constantly fed by dynamic, bidirectional communication and updates throughout the lifecycle of these sophisticated paired systems. Developing digital twins in production engineering creates new avenues for engineering design in the context of digital transformation in manufacturing and sociotechnical systems. This paper reviews the foundational concepts, methodologies, and applications of digital twins in engineering design, covering both their architecture and development (engineering of digital twins), and their utilisation to enhance design activities (engineering with digital twins). An overview of the current state-of-the-art is presented, challenges are highlighted, and future research directions are addressed.
ER  - 

TY  - JOUR
T1  - Agile and iterative governance: China’s regulatory response to AI
AU  - Xiao, Baiyang
JO  - Computer Law & Security Review
VL  - 58
SP  - 106183
PY  - 2025
DA  - 2025/09/01/
SN  - 2212-473X
DO  - https://doi.org/10.1016/j.clsr.2025.106183
UR  - https://www.sciencedirect.com/science/article/pii/S2212473X25000562
KW  - AI governance
KW  - China
KW  - EU AI act
KW  - Responsive regulation
KW  - Comparative legal study
AB  - Generative AI has been a buzzword in various sectors, advancing at an exponential rate and profoundly transforming the way we communicate and innovate. However, its potential benefits come with compelling ethical and legal risks, necessitating proper guardrails steering AI in beneficial directions. Amid the global race to AI regulation, China has exhibited strong and open ambition in shaping the emerging global AI order. In response to challenges posed by AI, China has not only implemented agile administrative intervention in policy support, central coordination, and investment, but also adopted an AI governance framework that characterized by ‘complexity,’ ‘agility,’ ‘stability,’ and ‘flexibility.’ Nevertheless, while an agile, sector-specific approach to AI governance may yield short-term benefits, it raises long-term concerns, including opaque decision-making, weak enforcement, fragmented oversight, and inadequate protection of fundamental rights. In particular, governance fragmentation marked by overlapping regulatory bodies and layered rulemaking risks producing piecemeal and outdated regulations that struggle to keep pace with rapid technological change. Current interventions often prioritize systemic stability over ethical clarity and robust supervision, while strategic ambiguity further complicates the implementation of AI ethics and hinders effective oversight, whether internal or administrative. Instead of calling for an omnibus AI law that applies a uniform package of rules, Chinese regulators chose to adapt horizontal elements into vertical regulations through a set of bureaucratic know-how and iterative regulatory tools. Through a comparative legal analysis, this paper finds that comprehending the intricacies of China’s AI regulatory approach is vital not only for projecting its future technological progression but also for understanding its impact on international tech competition. Differences in diverse AI governance may offer valuable insights while commonalities in AI governance values and principles hold promises for global cooperation in responsible AI governance. Moreover, it is plausible to expect that China will reconcile existing horizontal regulatory tools in a horizontal legislative package, while the EU AI Act provides valuable practical implications for the transition from vertical AI-related regulations to a horizontal Chinese AI Law, and the decentralized regulatory approach adopted by the U.S. serves as a useful reference for multi-stakeholder and multi-level cooperation. In addition, EU’s rights-driven framework and US’s market-driven model may serve as critical benchmarks, influencing China’s state-driven approach to harmonizing its legislative strategies for a responsive AI regulation.
ER  - 

TY  - JOUR
T1  - ODL-BCI: Optimal deep learning model for brain-computer interface to classify students confusion via hyperparameter tuning
AU  - Miah, Md Ochiuddin
AU  - Habiba, Umme
AU  - Kabir, Md Faisal
JO  - Brain Disorders
VL  - 13
SP  - 100121
PY  - 2024
DA  - 2024/03/01/
SN  - 2666-4593
DO  - https://doi.org/10.1016/j.dscb.2024.100121
UR  - https://www.sciencedirect.com/science/article/pii/S2666459324000064
KW  - Brain-computer interface
KW  - Electroencephalogram
KW  - Hyperparameter tuning
KW  - Machine learning
KW  - Deep learning
KW  - Neural network
KW  - Bayesian optimization
AB  - Brain-computer interface (BCI) research has gained increasing attention in educational contexts, offering the potential to monitor and enhance students’ cognitive states. Real-time classification of students’ confusion levels using electroencephalogram (EEG) data presents a significant challenge in this domain. Since real-time EEG data is dynamic and highly dimensional, current approaches have some limitations for predicting mental states based on this data. This paper introduces an optimal deep learning (DL) model for the BCI, ODL-BCI, optimized through hyperparameter tuning techniques to address the limitations of classifying students’ confusion in real time. Leveraging the “confused student EEG brainwave” dataset, we employ Bayesian optimization to fine-tune hyperparameters of the proposed DL model. The model architecture comprises input and output layers, with several hidden layers whose nodes, activation functions, and learning rates are determined utilizing selected hyperparameters. We evaluate and compare the proposed model with some state-of-the-art methods and standard machine learning (ML) classifiers, including Decision Tree, AdaBoost, Bagging, MLP, Naïve Bayes, Random Forest, SVM, and XG Boost, on the EEG confusion dataset. Our experimental results demonstrate the superiority of the optimized DL model, ODL-BCI. It boosts the accuracy between 4% and 9% over the current approaches, outperforming all other classifiers in the process. The ODL-BCI implementation source codes can be accessed by anyone at https://github.com/MdOchiuddinMiah/ODL-BCI.
ER  - 

TY  - JOUR
T1  - A systematic literature review on security testing of Ethereum smart contracts
AU  - Mnasri, Marwa
AU  - Maâlej, Afef Jmal
AU  - Jmaiel, Mohamed
JO  - Blockchain: Research and Applications
SP  - 100314
PY  - 2025
DA  - 2025/06/10/
SN  - 2096-7209
DO  - https://doi.org/10.1016/j.bcra.2025.100314
UR  - https://www.sciencedirect.com/science/article/pii/S2096720925000417
KW  - Security testing
KW  - Smart contract vulnerabilities
KW  - Blockchain security
KW  - Ethereum smart contracts
AB  - The security testing of Ethereum smart contracts has become increasingly important with the rise of decentralized applications (DApps) and blockchain technology. This systematic literature review (SLR) aims to provide a comprehensive overview of the state-of-the-art techniques, methodologies, tools, and challenges in the security testing of Ethereum smart contracts. By synthesizing and analyzing existing research articles, conference papers, and other relevant sources, this SLR identifies common trends, gaps, and areas for future research in this domain. The review covers various aspects of security testing, including vulnerability detection, testing frameworks, automated analysis tools, and best practices. In addition, it explores the impact of security vulnerabilities on smart contract ecosystems and proposes recommendations to improve the effectiveness and efficiency of security testing processes. This SLR serves as a valuable resource for researchers, practitioners, and developers interested in improving the security and reliability of Ethereum smart contracts.
ER  - 

TY  - JOUR
T1  - Artificial intelligence, knowledge and human resource management: A systematic literature review of theoretical tensions and strategic implications
AU  - Úbeda-García, Mercedes
AU  - Marco-Lajara, Bartolomé
AU  - Zaragoza-Sáez, Patrocinio C.
AU  - Poveda-Pareja, Esther
JO  - Journal of Innovation & Knowledge
VL  - 10
IS  - 6
SP  - 100809
PY  - 2025
DA  - 2025/11/01/
SN  - 2444-569X
DO  - https://doi.org/10.1016/j.jik.2025.100809
UR  - https://www.sciencedirect.com/science/article/pii/S2444569X25001544
KW  - Artificial Intelligence
KW  - Human resource management
KW  - Bibliometric analysis
KW  - SciMAT
KW  - Science mapping
AB  - This article studies the interaction between artificial intelligence (AI) and human resource management (HRM) through a bibliometric analysis of 203 articles published between 2002 and 2024 in the Web of Science database. The analysis identifies six fundamental strategic research themes, ranging from automation and predictive analysis to the personalisation of the employee experience. The paper illustrates the growing importance of AI and HRM, evidenced by an exponential increase in publications since 2016, with a significant peak after the COVID-19 pandemic. The conclusions highlight the need for a balanced approach that integrates technological innovation with rigorous ethical principles, particularly in critical areas such as algorithmic transparency, fairness in decision-making and personal data management. The research also provides a valuable roadmap for future research and sustainable organisational practices in the digital age, highlighting the importance of addressing emerging challenges such as technostress and the implications of personalisation in hybrid work environments. In addition to capturing the ways in which AI is transforming traditional HRM processes and the aspects covered by the literature so far, this paper also establishes a frame of reference for understanding and addressing the evolution of talent management in a rapidly changing technological context.
ER  - 

TY  - JOUR
T1  - Hello me, meet the real me: Voice synthesis attacks on voice assistants
AU  - Bilika, Domna
AU  - Michopoulou, Nikoletta
AU  - Alepis, Efthimios
AU  - Patsakis, Constantinos
JO  - Computers & Security
VL  - 137
SP  - 103617
PY  - 2024
DA  - 2024/02/01/
SN  - 0167-4048
DO  - https://doi.org/10.1016/j.cose.2023.103617
UR  - https://www.sciencedirect.com/science/article/pii/S0167404823005278
KW  - Voice assistants
KW  - Voice synthesis
KW  - Android
KW  - IOS
KW  - Security
KW  - Synthesised voice
AB  - The radical advances in telecommunications and computer science have enabled a myriad of applications and novel seamless interactions with computing interfaces. Voice Assistants (VAs) have become the norm for smartphones, and millions of VAs incorporated in smart devices are used to control these devices in the smart home context. Previous research has shown that they are prone to attacks, leading vendors to implement countermeasures. One of these measures is to allow only a specific individual, the device's owner, to perform potentially dangerous tasks that may disclose personal information, involve monetary transactions, etc. To understand the extent to which VAs provide the necessary protection to their users, we experimented with two of the most widely used VAs, which the participants trained. We then utilised voice synthesis, using samples provided by participants, to synthesise commands that were used to trigger the corresponding VA and perform a dangerous task. Our extensive results showed that more than 30% of our audio synthesis attacks were successful and at least one successful attack for more than half of the participants. Moreover, they illustrate statistically significant variation among vendors and, in one case, even gender bias. The outcomes are rather alarming and require the deployment of further countermeasures to prevent exploitation, as the number of VAs in use is currently comparable to the world population.
ER  - 

TY  - JOUR
T1  - DeepVATS: Deep Visual Analytics for Time Series
AU  - Rodriguez-Fernandez, Victor
AU  - Montalvo-Garcia, David
AU  - Piccialli, Francesco
AU  - Nalepa, Grzegorz J.
AU  - Camacho, David
JO  - Knowledge-Based Systems
VL  - 277
SP  - 110793
PY  - 2023
DA  - 2023/10/09/
SN  - 0950-7051
DO  - https://doi.org/10.1016/j.knosys.2023.110793
UR  - https://www.sciencedirect.com/science/article/pii/S0950705123005439
KW  - Deep learning
KW  - Visual analytics
KW  - Time series
KW  - Masked AutoEncoder
AB  - The field of Deep Visual Analytics (DVA) has recently arisen from the idea of developing Visual Interactive Systems supported by deep learning, in order to provide them with large-scale data processing capabilities and to unify their implementation across different data and domains. In this paper we present DeepVATS, an open-source tool that brings the field of DVA into time series data. DeepVATS trains, in a self-supervised way, a masked time series autoencoder that reconstructs patches of a time series, and projects the knowledge contained in the embeddings of that model in an interactive plot, from which time series patterns and anomalies emerge and can be easily spotted. The tool includes a back-end for data processing pipeline and model training, as well as a front-end with an interactive user interface. We report on results that validate the utility of DeepVATS, running experiments on both synthetic and real datasets. The code is publicly available on https://github.com/vrodriguezf/deepvats.
ER  - 

TY  - JOUR
T1  - Run time dynamic digital twins and dynamic digital twins networks
AU  - Vodyaho, Alexander
AU  - Delhibabu, Radhakrishnan
AU  - Ignatov, Dmitry I.
AU  - Zhukova, Nataly
JO  - Future Generation Computer Systems
VL  - 172
SP  - 107823
PY  - 2025
DA  - 2025/11/01/
SN  - 0167-739X
DO  - https://doi.org/10.1016/j.future.2025.107823
UR  - https://www.sciencedirect.com/science/article/pii/S0167739X25001189
KW  - Digital twin
KW  - Digital twin networks
KW  - Dynamic digital threads
KW  - Model synthesis
AB  - Digital twins are widely used for building various types of cyber–physical systems. There are a huge number of publications devoted to the use of digital twins in production systems. Much less attention is paid to the issues of building runtime digital twins. The article describes an approach to building complex distributed cyber–physical systems with a high level of architectural dynamics built on fog and edge computing platforms based on the use of digital twins. The issues of implementing runtime digital twins and distributed systems of runtime digital twins are considered. The requirements to runtime digital twins are defined. Typical problem statements for constructing and maintaining a runtime digital twin system are formulated. A reference architecture of a dynamic runtime digital twin is proposed, which includes a model of the observed system (or the object) and a model processor. The dynamic model of the observed and managed system is considered as a key element of the digital twin. Possible approaches to the synthesis of built-in models of runtime digital twins are discussed. Examples of using the proposed approach to solve practical problems are given. The described approach may be of interest to specialists involved in research and development of various types of information systems implemented on Internet of Things platforms, such as smart cities, smart transport, medical information systems, etc. It is proposed to conduct further research and development in the areas of creating human digital twins.
ER  - 

TY  - JOUR
T1  - FuzzyDoo: A framework for finding flaws in the 5G landscape
AU  - Garroppo, Rosario G.
AU  - Pagano, Michele
AU  - Pongelli, Gabriele
JO  - Computer Networks
VL  - 272
SP  - 111734
PY  - 2025
DA  - 2025/11/01/
SN  - 1389-1286
DO  - https://doi.org/10.1016/j.comnet.2025.111734
UR  - https://www.sciencedirect.com/science/article/pii/S1389128625007005
KW  - 5G
KW  - Fuzzing
KW  - Black-box
KW  - Testing
AB  - The increasing complexity and criticality of 5G networks demand rigorous security testing methodologies, particularly in black-box environments where source code access is restricted. This paper introduces FuzzyDoo, an open-source, mutation-based structure-aware fuzzing framework designed to assess the robustness and security of 5G Core (5GC) network functions under black-box conditions. FuzzyDoo advances the state of the art by enabling dynamic test message generation for encrypted communications, supporting extensible protocol integration, and facilitating flexible deployment of monitoring components in multi-system environments. The paper details the framework modular architecture – to the best of our knowledge, the first of its kind in the open-source domain – and demonstrates its efficacy through experimental evaluations on three open-source 5GC frameworks. These experiments reveal implementation-specific vulnerabilities and underscore FuzzyDoo diagnostic capabilities for root cause analysis.
ER  - 

TY  - JOUR
T1  - CLIP-MEI: Exploit more effective information for few-shot action recognition
AU  - Deng, XuanHan
AU  - Yang, WenZhu
AU  - Zhao, XinBo
AU  - Zhou, Tong
AU  - Deng, Xin
JO  - Knowledge-Based Systems
VL  - 326
SP  - 113965
PY  - 2025
DA  - 2025/09/27/
SN  - 0950-7051
DO  - https://doi.org/10.1016/j.knosys.2025.113965
UR  - https://www.sciencedirect.com/science/article/pii/S095070512501010X
KW  - Few-shot action recognition
KW  - Multi-modal
KW  - Semantic information
KW  - Task-based feature enhancement
KW  - Latent information mining
AB  - Few-shot action recognition (FSAR) aims to address the challenge of limited labeled data, yet most existing methods struggle to effectively tackle the insufficient visual information arising from scarce labeled samples. So, this paper proposes the CLIP-MEI (Contrastive Language-Image Pre-training, More Effective Information) framework, which enhances model representation through multi-modal feature fusion and latent information mining. Specifically, we build a CLIP-based prototype matching framework and design three core modules: (1) Query-specific Semantic information Augmentation (QSA), which generates adaptive semantic embeddings by integrating support set label semantics with query visual features to mitigate semantic disparities between support and query sets; (2) Task-based Feature Enhancement (TFE), which optimizes feature representations by exploiting latent relationships between support and query sets within the same task; and (3) Motion Information Compensation (MIC), which extracts highly invariant motion features by aligning shallow and deep motion representations. Extensive experimental results demonstrate that CLIP-MEI establishes new performance records across diverse benchmark datasets, notably achieving leading results on HMDB51. For example, it attains a 1-shot accuracy of 76.4% on HMDB51, outperforming baseline by 10.1%. The implementation can be accessed via GitHub.
ER  - 

TY  - JOUR
T1  - Beyond the headlines: analyzing news topics related to port productivity across different phases of the pandemic
AU  - Mahdikhani, Maryam
AU  - Karbasi, Ata
AU  - Mahdikhani, Mehri
JO  - International Journal of Productivity and Performance Management
VL  - 74
IS  - 9
SP  - 2961
EP  - 2987
PY  - 2025
DA  - 2025/10/28/
SN  - 1741-0401
DO  - https://doi.org/10.1108/IJPPM-04-2024-0224
UR  - https://www.sciencedirect.com/science/article/pii/S1741040125000366
KW  - Port productivity
KW  - Performance attributes
KW  - Textual analysis
KW  - Topic modeling
KW  - News media
AB  - Purpose
This study aims to analyze the key topics and discussion patterns related to port productivity in news articles, both before and after the COVID-19 pandemic, while tracking their evolution over time. Additionally, it seeks to examine the prominence and development of port policies across the selected time periods.
Design/methodology/approach
BERTopic topic modeling, a deep learning-based model, non-negative matrix factorization and latent semantic analysis topic modeling algorithms were employed to analyze the content of 652 news articles spanning January 1st, 2018, to September 9th, 2023.
Findings
The study revealed that, in the pre-pandemic era, news articles primarily focused on port efficiency and the integration of technological innovations, with an emphasis on transportation policy. However, in the post-pandemic period, the dominant themes shifted towards smart ports, sustainability, energy efficiency and the global market, with a particular emphasis on sustainability policies. These findings provide valuable insights into key terms associated with port productivity, offering a strategic roadmap for stakeholders to navigate the evolving dynamics of the global port industry across different phases of the pandemic.
Originality/value
The study provides practical and theoretical implications for policymakers, industry practitioners and research scholars to learn the knowledge structure of the research field and to develop research studies on the potential gaps in the fields for future studies.
ER  - 

TY  - JOUR
T1  - Illusions of truth—Experimental insights into human and algorithmic detections of fake online reviews
AU  - Plotkina, Daria
AU  - Munzel, Andreas
AU  - Pallud, Jessie
JO  - Journal of Business Research
VL  - 109
SP  - 511
EP  - 523
PY  - 2020
DA  - 2020/03/01/
SN  - 0148-2963
DO  - https://doi.org/10.1016/j.jbusres.2018.12.009
UR  - https://www.sciencedirect.com/science/article/pii/S0148296318306192
KW  - Deceptive communication
KW  - Fake online reviews
KW  - Human deceit detection
KW  - Truth bias
KW  - Online deception detection
KW  - Opinion spam
AB  - The issue of fake online reviews is increasingly relevant due to the growing importance of online reviews to consumers and the growing frequency of deceptive corporate practices. It is, therefore, necessary to be able to detect fake online reviews. An experiment with 1041 respondents allowed us to create two pools of reviews (fake and truthful) and compare them for psycholinguistic deception cues. The resulting automated tool accounted for review valence and incentive and detected deceptive reviews with 81% accuracy. A follow-up experiment with 407 consumers showed that humans have only a 57% accuracy of detection, even when a deception mindset is activated with information on cues of fake online reviews. Therefore, micro-linguistic automated detection can be used to filter the content of reviewing websites to protect online users. Our independent analysis of reviewing websites confirms the presence of dubious content and, therefore, the need to introduce more sophisticated filtering approaches.
ER  - 

TY  - JOUR
T1  - A graph proximity feature augmentation approach for identifying accounts of terrorists on twitter
AU  - Aleroud, Ahmed
AU  - Abu-Alsheeh, Nisreen
AU  - Al-Shawakfa, Emad
JO  - Computers & Security
VL  - 99
SP  - 102056
PY  - 2020
DA  - 2020/12/01/
SN  - 0167-4048
DO  - https://doi.org/10.1016/j.cose.2020.102056
UR  - https://www.sciencedirect.com/science/article/pii/S0167404820303291
KW  - Feature augmentation
KW  - Latent dirichlet allocation (lda)
KW  - Social network analysis
KW  - Temporal analysis
KW  - Terrorism Informatics
KW  - Graph Neighborhood
AB  - With the popularity of social networks, terrorist groups such as ISIS encouraged others to follow their activities, share their ideas, recruit fans, radicalize communities, and raise funds to support future attacks. This has led to the emergence of radicalized online accounts that belong to terrorists or their fans. Existing techniques for counter-terrorism investigations which aim to suspend such accounts are based on reports by users or syntactic-based sentiment analysis techniques, which are not accurate on short texts shared by terrorist such as tweets. This work proposed a feature augmentation approach to enrich the content of tweets before investigating them to discover the radicalized online contents. The augmented tweets are then used to classify accounts into Pro-ISIS or Anti-ISIS categories. We utilized topic modeling as a baseline method for feature augmentation. We studied the effects of utilizing tweets at different time intervals on the quality of the generated models that classify tweets and the corresponding accounts. We then introduced a novel feature augmentation approach that utilizes Neighborhood Overlap, a graph proximity technique that discovers terms having a strong relationship with the Pro-ISIS category. Terms extracted from tweets are represented as nodes in a graph, which is then partitioned into clusters containing different terms. Terms in strongly connected parts of each cluster are augmented to the original term vectors of the tweets based on the similarity between those terms and each tweet. We compared our approach with other baseline augmentation techniques such Term-to-Term correlation, Topic Modeling, and other existing techniques. Experimental results on a dataset containing Pro- and Anti-ISIS tweets show that our approach is quite promising to automate the identification of terrorist contents online. The results have shown that using graph proximity measures such as Neighborhood Overlap for term augmentation gains higher Precision, Recall, and F-measure than the typical approaches. In addition, we found that applying time-based analysis with term augmentation to identify radicalized accounts enhanced the Precision of the investigation process.
ER  - 

TY  - JOUR
T1  - Use of supervised machine learning to detect abuse of COVID-19 related domain names
AU  - Wang, Zheng
JO  - Computers and Electrical Engineering
VL  - 100
SP  - 107864
PY  - 2022
DA  - 2022/05/01/
SN  - 0045-7906
DO  - https://doi.org/10.1016/j.compeleceng.2022.107864
UR  - https://www.sciencedirect.com/science/article/pii/S0045790622001549
KW  - COVID-19
KW  - Malicious domain name
KW  - Classification
KW  - Machine learning
KW  - Deep learning
AB  - A comprehensive evaluation of supervised machine learning models for COVID-19 related domain name detection is presented. One representative conventional machine learning implementation and nineteen state-of-the-art deep learning implementations are evaluated. The deep learning implementation architectures evaluated include the recurrent, convolutional, and hybrid models. The detection rate metrics and the computing time metrics are considered in the evaluation. The result reveals that advanced deep learning models outperform conventional machine learning models in terms of detection rate. The results also show evidence of a tradeoff between detection rate and computing speed for the selection of machine learning models/architectures. High-frequency lexical analysis is provided for a better understanding of the COVID-19 related domain names. The limitations, implications, and considerations of the use of supervised machine learning to detect abuse of COVID-19 related domain names are discussed.
ER  - 

TY  - JOUR
T1  - Recent advances in wind turbine condition monitoring using SCADA data: A state-of-the-art review
AU  - Wang, Shun
AU  - Vidal, Yolanda
AU  - Pozo, Francesc
JO  - Reliability Engineering & System Safety
VL  - 267
SP  - 111838
PY  - 2026
DA  - 2026/03/01/
SN  - 0951-8320
DO  - https://doi.org/10.1016/j.ress.2025.111838
UR  - https://www.sciencedirect.com/science/article/pii/S0951832025010385
KW  - Wind energy
KW  - Wind turbine
KW  - SCADA
KW  - Condition monitoring
KW  - Fault detection
KW  - Fault diagnosis
KW  - Fault prognosis
AB  - With the rapid increase worldwide in the number of installed wind turbines, as an increasingly vital role in the global energy landscape, requirements and expenses of maintenance have also increased significantly. This surge in wind turbine installations worldwide has escalated maintenance demands and costs, making condition monitoring an essential focus for research and operational optimization. Built-in supervisory control and data acquisition (SCADA) systems could offer a cost-effective approach to condition monitoring in wind turbines, using existing infrastructure to reduce the need for additional sensors and simplifying system complexity. This review critically surveys state-of-the-art SCADA-based condition monitoring techniques for wind turbines. The paper first examines essential data preprocessing strategies tailored for SCADA data challenges. Subsequently, key monitoring methodologies are reviewed, including model-based and data-driven paradigms, analyzing their applications, strengths, and limitations. Finally, the challenges and future trends of SCADA-based wind turbine condition monitoring are discussed. This comprehensive overview serves as a valuable reference for researchers and industry practitioners seeking to enhance wind turbine reliability and operational efficiency through advanced monitoring techniques.
ER  - 

TY  - JOUR
T1  - Human-centric assembly in smart factories
AU  - Wang, Lihui
AU  - Gao, Robert X.
AU  - Krüger, Jörg
AU  - Váncza, József
JO  - CIRP Annals
VL  - 74
IS  - 2
SP  - 789
EP  - 815
PY  - 2025
DA  - 2025/01/01/
SN  - 0007-8506
DO  - https://doi.org/10.1016/j.cirp.2025.04.058
UR  - https://www.sciencedirect.com/science/article/pii/S0007850625001064
KW  - Assembly
KW  - Robot
KW  - Human-centricity
AB  - Assembly in future smart factories needs to address three challenges, including human centricity, sustainability, and resilience. Conventional approaches for automation in assembly have reached a bottleneck in terms of operation automomy, leaving various tasks to continued manual labour by human operators. To ease the burden on humans both physically and intellectually, human-centric assembly enhanced by augmented robots, cognitive systems, mixed reality and collaborative intelligence, assisted by thought-driven brain robotic controls, provides a promising solution. Within the context, this keynote provides an in-depth analysis of the state of human-centric assembly and identifies potentially fruitful research directions in future smart factories.
ER  - 

TY  - JOUR
T1  - Interaction dataset of autonomous vehicles with traffic lights and signs
AU  - Li, Zheng
AU  - Bao, Zhipeng
AU  - Meng, Haoming
AU  - Shi, Haotian
AU  - Li, Qianwen
AU  - Yao, Handong
AU  - Li, Xiaopeng
JO  - Communications in Transportation Research
VL  - 5
SP  - 100201
PY  - 2025
DA  - 2025/12/01/
SN  - 2772-4247
DO  - https://doi.org/10.1016/j.commtr.2025.100201
UR  - https://www.sciencedirect.com/science/article/pii/S2772424725000411
KW  - Autonomous vehicles (AVs)
KW  - Traffic lights
KW  - Stop signs
KW  - Interaction data
KW  - Waymo motion dataset
AB  - This study presents the development of a comprehensive dataset capturing interactions between autonomous vehicles (AVs) and traffic control devices, specifically traffic lights and stop signs. Derived from the Waymo Motion dataset, our work addresses a critical gap in the existing literature by providing real-world trajectory data on how AVs navigate these traffic control devices. We propose a methodology for identifying and extracting relevant interaction trajectory data from the Waymo Motion dataset, incorporating over 37,000 instances with traffic lights and 44,000 with stop signs. Our methodology includes defining rules to identify various interaction types, extracting trajectory data, and applying a wavelet-based denoising method to smooth the acceleration and speed profiles and eliminate anomalous values, thereby enhancing the trajectory quality. Quality assessment metrics indicate that trajectories obtained in this study have anomaly proportions in acceleration and jerk profiles reduced to near-zero levels across all interaction categories. By making this dataset publicly available, we aim to address the current gap in datasets containing AV interaction behaviors with traffic lights and signs. Based on the organized and published dataset, we can gain a more in-depth understanding of AVs’ behavior when interacting with traffic lights and signs. This will facilitate research on AV integration into existing transportation infrastructures and networks, supporting the development of more accurate behavioral models and simulation tools.
ER  - 

TY  - JOUR
T1  - Human-centered AI: advancing ethical, transparent, and context-aware systems for sustainable development
AU  - Nizamani, Mir Muhammad
AU  - Zhang, Hai-Li
AU  - Lai, Zhongping
JO  - Technology in Society
VL  - 84
SP  - 103121
PY  - 2026
DA  - 2026/03/01/
SN  - 0160-791X
DO  - https://doi.org/10.1016/j.techsoc.2025.103121
UR  - https://www.sciencedirect.com/science/article/pii/S0160791X25003112
KW  - AI ethics
KW  - Bias mitigation
KW  - Human-AI collaboration
KW  - Interdisciplinary research
KW  - User experience
AB  - Human-Centered AI (HCAI) represents a transformative approach to artificial intelligence development, focusing on aligning AI systems with human values, societal needs, and ethical standards. This paper explores the key frameworks, challenges, and opportunities associated with HCAI, emphasizing its application across diverse domains such as healthcare, urban planning, agriculture, and education. HCAI-driven innovations, including Human-Centered Generative AI, Human-AI Co-Creation, and Explainable AI, are enhancing creativity, trust, and decision-making in both individual and collaborative contexts. The paper identifies critical challenges, including technical limitations, ethical concerns, human-AI interaction issues, and data privacy concerns, which must be addressed to ensure effective and ethical AI deployment. It also highlights the role of interdisciplinary research and cross-sector collaboration in overcoming these challenges and advancing HCAI applications. Looking ahead, the integration of AI with big data, longitudinal studies, and real-time evaluations will play a pivotal role in refining AI systems and ensuring their long-term success. By adopting a socio-technical approach and fostering public participation in AI design, HCAI can promote more sustainable, equitable, and human-centric technological development. Ultimately, the paper argues that HCAI has the potential to drive innovation in ways that enhance human capabilities, build trust, and contribute positively to societal well-being.
ER  - 

TY  - JOUR
T1  - Preparing educational leaders for the age of AI: Lessons from a graduate course combining curriculum innovation and institutional inquiry
AU  - Stewart, Csilla M.
AU  - Nugent, Patricia
AU  - Williams Newman, Rachel
AU  - Burns, MJane
AU  - Jeremiah, Jacob
AU  - Barrio, Gerardo
JO  - The International Journal of Management Education
VL  - 24
IS  - 1
SP  - 101313
PY  - 2026
DA  - 2026/03/01/
SN  - 1472-8117
DO  - https://doi.org/10.1016/j.ijme.2025.101313
UR  - https://www.sciencedirect.com/science/article/pii/S1472811725001831
KW  - Artificial intelligence
KW  - Educational leadership
KW  - Curriculum redesign
KW  - AI literacy
KW  - Management education
KW  - Qualitative research
AB  - This qualitative case study explores the integration of artificial intelligence (AI) into a graduate-level educational leadership course at a Midwestern university. In response to increasing demand for AI literacy in education, the course was redesigned using Fadel et al.‘s (2024) four-dimensional learning framework combining interdisciplinary knowledge, skills, character, and meta-learning with scaffolded experiences involving generative AI tools. Eighteen students conducted interviews and completed institutional analyses of AI adoption across diverse educational settings. Inductive coding of their reflections revealed eleven emergent themes, including curriculum innovation, automation, ethical concerns, and inconsistent policy development. Findings highlight the uneven AI integration in education and highlight the value of experiential, field-based assignments in leadership and management education. This study offers a replicable instructional model for embedding AI into leadership curricula and contributes practice-based insights into the evolving role of AI in educational strategy and organizational change.
ER  - 

TY  - JOUR
T1  - Forecasting the trajectory of personal watercrafts using models based on recurrent neural networks
AU  - Žužić, Lucija
AU  - Hržić, Franko
AU  - Lerga, Jonatan
JO  - Expert Systems with Applications
VL  - 284
SP  - 127964
PY  - 2025
DA  - 2025/07/25/
SN  - 0957-4174
DO  - https://doi.org/10.1016/j.eswa.2025.127964
UR  - https://www.sciencedirect.com/science/article/pii/S0957417425015866
KW  - Personal watercraft
KW  - Trajectory forecasting
KW  - Recurrent neural network
KW  - Attention mechanism
KW  - Foundation model
AB  - Monitoring and predicting personal watercraft trajectories is a novel and largely unexplored research area where any development is valuable for various rental services. Unlike existing work focused on specific maritime routes, this study introduces a location-agnostic deep-learning approach capable of generalizing across diverse environments. This is achieved by using an innovative preprocessing approach including offset and scaling. A novel Long Short-Term Memory (LSTM) bidirectional and convolutional model developed by the authors for binary peptide classification was adapted to accommodate the forecasting of continuous values. By leveraging Recurrent Neural Network (RNN) architectures with LSTM and Gated Recurrent Unit (GRU) layers, and cutting-edge attention-based and foundation models, we benchmark trajectory forecasting performance using real-world data from 1282 rental sites worldwide. Most notably, we extend the applicability of the listed models and the original LSTM bidirectional and convolutional models to maritime trajectory estimation, eliminating the need for training separate models for different locations while achieving superior predictive accuracy. Our results demonstrate that foundation models and attention mechanisms significantly outperform traditional methods, offering a robust and scalable solution for watercraft trajectory forecasting. These findings pave the way for intelligent monitoring systems that enhance maritime safety and operational efficiency.
ER  - 

TY  - JOUR
T1  - Identifying self-disclosed anxiety on Twitter: A natural language processing approach
AU  - Zarate, Daniel
AU  - Ball, Michelle
AU  - Prokofieva, Maria
AU  - Kostakos, Vassilis
AU  - Stavropoulos, Vasileios
JO  - Psychiatry Research
VL  - 330
SP  - 115579
PY  - 2023
DA  - 2023/12/01/
SN  - 0165-1781
DO  - https://doi.org/10.1016/j.psychres.2023.115579
UR  - https://www.sciencedirect.com/science/article/pii/S0165178123005292
KW  - Anxiety
KW  - Cyber-phenotype
KW  - Digital footprint
KW  - Natural language processing
KW  - Sentiment analysis
KW  - Twitter
AB  - Background
Text analyses of social media posts are a promising source of mental health information. This study used natural language processing to explore distinct language patterns on Twitter related to self-reported anxiety diagnosis.
Methods
A total of 233.000 tweets made by 605 users (300 reporting anxiety diagnosis and 305 not) over six months were comparatively analysed, considering user behavior, Linguistic Inquiry Word Count (LIWC), and sentiment analysis. Twitter users with a self-disclosed diagnosis of anxiety were classified as ‘anxious’ to facilitate group comparisons.
Results
Supervised machine learning models showed a high prediction accuracy (Naïve Bayes 81.1 %, Random Forests 79.8 %, and LASSO-regression 79.4 %) in identifying Twitter users’ self-disclosed diagnosis of anxiety. Additionally, a Latent Profile Analysis (LPA) identified four profiles characterized by high sentiment (31 % anxious participants), low sentiment (68 % anxious), self-immersed (80 % anxious), and normative behavior (38 % anxious).
Conclusion
The digital footprint of self-disclosed anxiety on Twitter posts presented a high frequency of words conveying either negative sentiment, a low frequency of positive sentiment, a reduced frequency of posting, and lengthier texts. These distinct patterns enabled highly accurate prediction of anxiety diagnosis. On this basis, appropriately resourced, awareness raising, online mental health campaigns are advocated.
ER  - 

TY  - JOUR
T1  - Towards a methodology for ethical artificial intelligence system development: A necessary trustworthiness taxonomy
AU  - Braga, Carlos Mario
AU  - Serrano, Manuel A.
AU  - Fernández-Medina, Eduardo
JO  - Expert Systems with Applications
VL  - 286
SP  - 128034
PY  - 2025
DA  - 2025/08/15/
SN  - 0957-4174
DO  - https://doi.org/10.1016/j.eswa.2025.128034
UR  - https://www.sciencedirect.com/science/article/pii/S0957417425016550
KW  - Artificial Intelligence
KW  - Methodology
KW  - Trustworthy
KW  - Ethics
KW  - Generative AI
KW  - Taxonomy
KW  - Sociotechnical system
AB  - Recently, generative artificial intelligence (GenAI) has arisen and been rapidly adopted; due to its emergent abilities, there is a significantly increased need for risk management in the implementation of such systems. At the same time, many proposals for translating ethics into AI, as well as the first agreements by regulators governing the use of artificial intelligence (AI), have surfaced. This underscores the need for Trustworthy AI, which implies reliability, compliance, and ethics. However, there is still a lack of unified criteria, and more critically, a lack of systematic methodologies for operationalizing trustworthiness within AI development processes. Trustworthiness is crucial, as it ensures that the system performs consistently under expected conditions while adhering to moral and legal standards. The problem of ensuring trustworthiness must be addressed as a preliminary step in creating a methodology for building AI systems with these desirable features. Based on a systematic literature review (SLR), we analyze the ethical, legal, and technological challenges that AI projects face, identifying key considerations and gaps in current approaches. This article presents a detailed and structured sociotechnical taxonomy related to the concept of Trustworthy AI, grounded in the analysis of all relevant texts on the topic, and designed to enable the systematic integration of ethical, legal, and technological principles into AI development processes. The taxonomy establishes a sociotechnical foundation that reflects the interconnected nature of technological, ethical, and legal considerations, and serves as the conceptual basis for CRISP-TAI, a proposed specialized development lifecycle currently under validation, aimed at systematically operationalizing trustworthiness principles across all phases of AI system engineering.
ER  - 

TY  - JOUR
T1  - Interpretability research of deep learning: A literature survey
AU  - Xu, Biao
AU  - Yang, Guanci
JO  - Information Fusion
VL  - 115
SP  - 102721
PY  - 2025
DA  - 2025/03/01/
SN  - 1566-2535
DO  - https://doi.org/10.1016/j.inffus.2024.102721
UR  - https://www.sciencedirect.com/science/article/pii/S1566253524004998
KW  - Deep learning
KW  - Interpretability
KW  - Active explanations
KW  - Passive explanations
KW  - Explainable artificial intelligence
AB  - Deep learning (DL) has been widely used in various fields. However, its black-box nature limits people's understanding and trust in its decision-making process. Therefore, it becomes crucial to research the DL interpretability, which can elucidate the model's decision-making processes and behaviors. This review provides an overview of the current status of interpretability research. First, the DL's typical models, principles, and applications are introduced. Then, the definition and significance of interpretability are clarified. Subsequently, some typical interpretability algorithms are introduced into four groups: active, passive, supplementary, and integrated explanations. After that, several evaluation indicators for interpretability are briefly described, and the relationship between interpretability and model performance is explored. Next, the specific applications of some interpretability methods/models in actual scenarios are introduced. Finally, the interpretability research challenges and future development directions are discussed.
ER  - 

TY  - JOUR
T1  - Fraud and complexity theory: Moving beyond the fraud triangle towards a new theoretical framework
AU  - Freeman, Christopher
JO  - Journal of Economic Criminology
VL  - 10
SP  - 100198
PY  - 2025
DA  - 2025/12/01/
SN  - 2949-7914
DO  - https://doi.org/10.1016/j.jeconc.2025.100198
UR  - https://www.sciencedirect.com/science/article/pii/S2949791425000740
KW  - Complexity theory
KW  - Fraud
KW  - Economic crime
KW  - Criminology
KW  - Critical realism
AB  - Fraud research has long been dominated by micro-level theories—most notably the fraud triangle—that focus on individual motivation while neglecting the wider systems that shape and sustain fraudulent behaviour. This paper introduces a conceptual framework that bridges that gap by integrating established offender-based theories with a novel macro-level understanding of fraud as an emergent property of a complex adaptive system. It develops and restructures Fraud Field Theory to place opportunities—the points of interaction between threat and safeguard fields—at the centre of analysis. Drawing on complexity theory, systems thinking, and critical realism, the study explores how feedback, adaptation, and layered causal mechanisms connect individual actions with collective outcomes. The argument is conceptual rather than empirical: causal loop diagrams are used as heuristic devices to illustrate how interactions within and across systems may generate emergent patterns of fraud. The revised framework is then located within a critical realist ontology that distinguishes between empirical events, actual but unobserved processes, and the deeper generative mechanisms that condition them. This synthesis demonstrates how micro-level offender logics can be interpreted within broader organisational, technological, and societal contexts. If fraud stems from the adaptive dynamics of these interconnected systems rather than from isolated acts of wrongdoing, the efficacy of deterrence-focused policies will remain limited. A new policy frame is needed, one that treats fraud control as a process of continual system adjustment, strengthening resilience, mapping systems, and adapting interventions as the environment continually evolves.
ER  - 

TY  - JOUR
T1  - A risk assessment framework for online transactions via Graph Neural Networks and efficient probabilistic prediction
AU  - Chang, Jicai
AU  - Fu, Xuejing
AU  - Chen, Zhen
AU  - Pan, Li
AU  - Liu, Shijun
JO  - Engineering Applications of Artificial Intelligence
VL  - 163
SP  - 112766
PY  - 2026
DA  - 2026/01/01/
SN  - 0952-1976
DO  - https://doi.org/10.1016/j.engappai.2025.112766
UR  - https://www.sciencedirect.com/science/article/pii/S0952197625027976
KW  - Online transaction
KW  - Risk assessment
KW  - Probabilistic prediction
KW  - Multivariate time series
KW  - Graph Neural Networks
KW  - Sparse self-attention
AB  - Transactions are integral to daily life, but the occurrence of abnormal behaviors can lead to significant risks. Online transaction risk is characterized by the accumulation of abnormal behaviors, where their frequency surpasses a predefined threshold, resulting in measurable probabilities and consequences. Therefore, the assessment of online transaction risk heavily depends on probabilistic predictions of the accumulated frequency of abnormal behaviors, presenting two major challenges. Firstly, abnormal behaviors across different instances (e.g., behavior types, product categories, regions, and platforms) exhibit temporal correlations, such as co-occurrence and concomitance, which most probabilistic models fail to identify and utilize effectively. Additionally, these models do not fully address the real-time demands. To address these challenges, we propose a novel risk assessment framework based on Graph Neural Network (GNN) and probabilistic prediction, named GNN-Probformer. The framework uses Dynamic Time Warping to capture temporal correlations between abnormal behavior frequency sequences and constructs a graph structure through clustering. It then employs Graph Neural Networks to aggregate features and learn representations through a novel embedding module. A sparse self-attention mechanism and an efficient encoder–decoder architecture are incorporated to further enhance performance, while probabilistic predictions are generated through Monte Carlo sampling and cumulative distribution functions. Experimental results on a real-world dataset demonstrate that GNN-Probformer achieves substantial performance gains, with a 15% reduction in normalized deviation. At the 90th percentile, it further reduces normalized quantile loss by 15% and improves the F1-score by 16%, while also reducing training time and inference time by 47% and 38%, respectively.
ER  - 

TY  - JOUR
T1  - Harnessing customized AI to create voice of customer via GPT3.5
AU  - Shahin, Mohammad
AU  - Chen, F. Frank
AU  - Hosseinzadeh, Ali
JO  - Advanced Engineering Informatics
VL  - 61
SP  - 102462
PY  - 2024
DA  - 2024/08/01/
SN  - 1474-0346
DO  - https://doi.org/10.1016/j.aei.2024.102462
UR  - https://www.sciencedirect.com/science/article/pii/S1474034624001101
KW  - ChatGPT
KW  - VoC
KW  - Lean Six Sigma
KW  - Industry 5.0
KW  - Artificial General Intelligence
AB  - The integration of customer feedback is universally acknowledged as crucial in the product development process. Yet, traditional feedback collection methods employed by companies, such as interviews and surveys, have remained mainly unchanged and come with limitations. Interviews often fail to accurately capture customers' needs due to communication barriers, while surveys prompt only incremental changes instead of inspiring innovation. This challenge is compounded in the service industry, where feedback is intangible and more difficult to quantify. Text analysis presents a promising solution to delve into customer preferences more deeply, providing insights that can guide the development of new products and services. Our research advances the use of generative AI, specifically the GPT engine, beyond its conventional role as a chatbot. We innovate by adapting it to extract actionable insights from customer-service interactions, offering real-time, valuable data for decision-making and representing a significant leap forward in Voice of the Customer (VoC) analysis.
ER  - 

TY  - JOUR
T1  - A verifiable EVM-based cross-language smart contract implementation scheme for matrix calculation
AU  - He, Yunhua
AU  - Yang, Yigang
AU  - Wang, Chao
AU  - Xie, Anke
AU  - Ma, Li
AU  - Wu, Bin
AU  - Wu, Yongdong
JO  - Digital Communications and Networks
VL  - 11
IS  - 2
SP  - 432
EP  - 441
PY  - 2025
DA  - 2025/04/01/
SN  - 2352-8648
DO  - https://doi.org/10.1016/j.dcan.2024.03.003
UR  - https://www.sciencedirect.com/science/article/pii/S2352864824000348
KW  - Smart contract
KW  - Blockchain
KW  - Cross-language programming
KW  - Bilinear pairing
KW  - Publicly verifiable computation
AB  - The wide application of smart contracts allows industry companies to implement some complex distributed collaborative businesses, which involve the calculation of complex functions, such as matrix operations. However, complex functions such as matrix operations are difficult to implement on Ethereum Virtual Machine (EVM)-based smart contract platforms due to their distributed security environment limitations. Existing off-chain methods often result in a significant reduction in contract execution efficiency, thus a platform software development kit interface implementation method has become a feasible way to reduce overheads, but this method cannot verify operation correctness and may leak sensitive user data. To solve the above problems, we propose a verifiable EVM-based smart contract cross-language implementation scheme for complex operations, especially matrix operations, which can guarantee operation correctness and user privacy while ensuring computational efficiency. In this scheme, a verifiable interaction process is designed to verify the computation process and results, and a matrix blinding technology is introduced to protect sensitive user data in the calculation process. The security analysis and performance tests show that the proposed scheme can satisfy the correctness and privacy of the cross-language implementation of smart contracts at a small additional efficiency cost.
ER  - 

TY  - JOUR
T1  - A I and contemporary challenges: The good, bad and the scary
AU  - Krishna, Venni V.
JO  - Journal of Open Innovation: Technology, Market, and Complexity
VL  - 10
IS  - 1
SP  - 100178
PY  - 2024
DA  - 2024/03/01/
SN  - 2199-8531
DO  - https://doi.org/10.1016/j.joitmc.2023.100178
UR  - https://www.sciencedirect.com/science/article/pii/S2199853123002809
KW  - Artificial Intelligence
KW  - AI conundrum
KW  - Human centred AI
KW  - Slaughter bots
KW  - AI and level
KW  - Playing field
AB  - The way in which powerful AI technologies could transform our lives, society, economics, governance and most importantly ethics and morality surrounding it is quite popular in every day news media as well as our drawing room discussions at home. There is considerable confusion among individuals and communities on the impact of AI. Profit seeking global mega corporations have already poured in billions of dollars in AI research and development to maximise their profits. On the other hand, there are scary and dangerous scenarios shaping up on the impact and use of AI tools. Rouge and non-state actors are weaponizing AI technologies as well as giving rise to new fake platforms. We are not far away from killer robots, autonomous weapons and robotic war fare as depicted in the film Slaughterbots. The main purpose and objective of this essay is to understand what is good, bad and the scary of AI developments? What are the positive and negative impacts of AI on our contemporary society? Should we leave AI technology within the perspective of technological determinism or is there a scope to socially shape the new technology for the benefit of our society. These are some of the important issues that will be addressed in this essay.
ER  - 

TY  - JOUR
T1  - Automating modern code review processes with code similarity measurement
AU  - Kartal, Yusuf
AU  - Akdeniz, E. Kaan
AU  - Özkan, Kemal
JO  - Information and Software Technology
VL  - 173
SP  - 107490
PY  - 2024
DA  - 2024/09/01/
SN  - 0950-5849
DO  - https://doi.org/10.1016/j.infsof.2024.107490
UR  - https://www.sciencedirect.com/science/article/pii/S0950584924000958
KW  - Modern code review
KW  - Vectorization
KW  - Code similarity
KW  - Information retrieval
AB  - Context:
Modern code review is a critical component in software development processes, as it ensures security, detects errors early and improves code quality. However, manual reviews can be time-consuming and unreliable. Automated code review can address these issues. Although deep-learning methods have been used to recommend code review comments, they are expensive to train and employ. Instead, information retrieval (IR)-based methods for automatic code review are showing promising results in efficiency, effectiveness, and flexibility.
Objective:
Our main objective is to determine the optimal combination of the vectorization method and similarity to measure what gives the best results in an automatic code review, thereby improving the performance of IR-based methods.
Method:
Specifically, we investigate different vectorization methods (Word2Vec, Doc2Vec, Code2Vec, and Transformer) that differ from previous research (TF-IDF and Bag-of-Words), and similarity measures (Cosine, Euclidean, and Manhattan) to capture the semantic similarities between code texts. We evaluate the performance of these methods using standard metrics, such as Blue, Meteor, and Rouge-L, and include the run-time of the models in our results.
Results:
Our results demonstrate that the Transformer model outperforms the state-of-the-art method in all standard metrics and similarity measurements, achieving a 19.1% improvement in providing exact matches and a 6.2% improvement in recommending reviews closer to human reviews.
Conclusion:
Our findings suggest that the Transformer model is a highly effective and efficient approach for recommending code review comments that closely resemble those written by humans, providing valuable insight for developing more efficient and effective automated code review systems.
ER  - 

TY  - JOUR
T1  - Emotional resources and knowledge management: a gender perspective on human-centric innovation amidst crises
AU  - Audretsch, David Bruce
AU  - Kariv, Dafna
JO  - Journal of Knowledge Management
VL  - 29
IS  - 10
SP  - 3216
EP  - 3246
PY  - 2025
DA  - 2025/08/18/
SN  - 1367-3270
DO  - https://doi.org/10.1108/JKM-02-2024-0235
UR  - https://www.sciencedirect.com/science/article/pii/S1367327025000808
KW  - Innovation development
KW  - Navigating crises
KW  - Gender
KW  - Emotional capabilities
KW  - Society 5.0
KW  - AI-generated in research
AB  - Purpose
This paper aims to advocate for a paradigm shift that prioritizes a human-centered approach in the pursuit of innovation during crises, urging a departure from the prevailing dominance of the technology-centric approach. The incorporation of emotional capabilities as a dynamic capability is posited as a pivotal contribution, in harmony with the tenets of Society 5.0 and imperative for establishing a robust knowledge management foundation. This research underscores the significance of the human-centered approach, portraying women as exemplars in a novel paradigm of innovation development amid crises.
Design/methodology/approach
This research uses the framework of knowledge management for innovation to focus on the challenges presented by complex crises, now considered the new normal. The study employs a distinct, human centric approach to explore the nexus of gender, opportunities and innovation, during crises, with an emphasis on the founders’ emotional capabilities and resources as catalysts for innovation development.
Findings
This research utilizes mixed methods; qualitative findings driven from AI analyses reveal women’s positive approach toward innovation development in adversity, showcasing the influence of their emotional resources in their innovation pursuits. The subsequent quantitative findings, derived from a sample of 464 tech-founders navigating complex crises, emphasize the role of emotional capabilities as activators of opportunity exploitation for enhancing innovation development during crises, particularly among female founders.
Social implications
The potential for future research lies in exploring diverse emotional dimensions, employing various measures and methodologies. Envisioning upcoming studies that extend our findings across institutional, national and crisis contexts, emotional capabilities and skills may emerge as critical assets relevant to all entrepreneurs, transcending gender boundaries. This paper’s framework sets the stage for promising avenues at the nexus of gender and emotional capabilities in the innovation pursuits, shaping entrepreneurial performance in both challenging and stable conditions.
Originality/value
This research contributes significantly in several key areas. Firstly, it explores innovation development and knowledge management within Society 5.0 during a polycrisis, emphasizing the crucial role of emotional capabilities in activating opportunity exploitation. Secondly, it champions a human-centric premise in innovation, highlighting women as role models for innovation during crises and introducing pathways to tap into external resources, ultimately enriching knowledge management. Thirdly, the innovative methodological approach using Artificial Intelligence (AI) and Natural Language Processing (NLP) to construct synthetic personas is groundbreaking. Finally, it advances effectuation, bricolage and dynamic capabilities frameworks, enriching their theoretical foundations and affirming their relevance for innovation development amid instability.
ER  - 

TY  - JOUR
T1  - SADDLE: A runtime feedback control architecture for adaptive distributed deep learning in heterogeneous GPU clusters
AU  - Kim, HyungJun
AU  - Lee, Eunyoung
AU  - Yu, Heonchang
JO  - Journal of Systems Architecture
VL  - 168
SP  - 103573
PY  - 2025
DA  - 2025/11/01/
SN  - 1383-7621
DO  - https://doi.org/10.1016/j.sysarc.2025.103573
UR  - https://www.sciencedirect.com/science/article/pii/S1383762125002450
KW  - Feedback-driven adaptation
KW  - Control-theoretic training
KW  - Global–local batch optimization
KW  - Event-triggered rebalancing
KW  - Online parameter tuning
KW  - Heterogeneous GPU clusters
AB  - Adaptive training in heterogeneous GPU clusters requires more than isolated heuristics—it demands a real-time, feedback-driven control system. SADDLE is a self-adaptive framework that unifies global batch scaling, local throughput balancing, and transient straggler mitigation into a fully coordinated runtime. It combines scaling guided by the Gradient Noise Scale (GNS), z-score detection over Exponentially Weighted Moving Average (EWMA)-smoothed iteration times, and responsiveness tuned via a Proportional–Integral–Derivative (PID) controller into a single, event-driven control loop. Across vision and language tasks, SADDLE improves training time by up to 2.84×and accuracy by up to 5.26% over strong baselines, while maintaining under 6% runtime overhead. This reframing positions adaptive training as dynamic system regulation, enabling deep learning frameworks to self-optimize under real-world heterogeneity.
ER  - 

TY  - JOUR
T1  - Exploring harmony search for power system optimization: applications, formulations, and open problems
AU  - Oh, Eunsung
AU  - Geem, Zong Woo
JO  - Applied Energy
VL  - 398
SP  - 126452
PY  - 2025
DA  - 2025/11/15/
SN  - 0306-2619
DO  - https://doi.org/10.1016/j.apenergy.2025.126452
UR  - https://www.sciencedirect.com/science/article/pii/S0306261925011821
KW  - Harmony search
KW  - Metaheuristics
KW  - Power systems
KW  - Optimization
KW  - System-level optimization
KW  - Device-level control
KW  - Controller tuning
AB  - Modern power systems must optimize large-scale, nonlinear, multi-objective problems created by renewable integration, rapidly growing distributed resources, and strict reliability and efficiency targets. Conventional techniques often falter under these conditions, whereas Harmony Search (HS) has shown strong potential. Unlike earlier HS surveys, this review provides a structured synthesis of power system applications of HS, with attention to how objective functions and constraints are formulated in HS models. It covers both system-level operations (e.g., economic dispatch, optimal power flow, unit commitment, renewable planning) and device-level control (e.g., load frequency regulation and power system stabilization). Comparative results demonstrate HS’s versatility in meeting cost, emission, and reliability goals, and reveal scenarios where careful formulation improves convergence and solution quality relative to other metaheuristics. Practical guidance is distilled on parameter self-adaptation, hybridization with artificial intelligence models, and constraint-handling schemes that mitigate sensitivity and premature convergence. Remaining challenges include inconsistent modeling practices and limited scalability for real-time or very large networks. Recommended remedies encompass standardized HS formulations and automated parameter tuning to improve reproducibility and performance. The review concludes with a future vision of uncertainty-aware, explainable HS frameworks integrated with digital-twin environments, charting a clear agenda for next-generation power-system optimization.
ER  - 

TY  - JOUR
T1  - Exposing image splicing traces in scientific publications via uncertainty-guided refinement
AU  - Lin, Xun
AU  - Tang, Wenzhong
AU  - Wang, Haoran
AU  - Liu, Yizhong
AU  - Ju, Yakun
AU  - Wang, Shuai
AU  - Yu, Zitong
JO  - Patterns
VL  - 5
IS  - 9
SP  - 101038
PY  - 2024
DA  - 2024/09/13/
SN  - 2666-3899
DO  - https://doi.org/10.1016/j.patter.2024.101038
UR  - https://www.sciencedirect.com/science/article/pii/S2666389924001806
KW  - scientific integrity
KW  - image splicing detection
KW  - convolution neural network
KW  - and uncertainty
AB  - Summary
Recently, a surge in image manipulations in scientific publications has led to numerous retractions, highlighting the importance of image integrity. Although forensic detectors for image duplication and synthesis have been researched, the detection of image splicing in scientific publications remains largely unexplored. Splicing detection is more challenging than duplication detection due to the lack of reference images and more difficult than synthesis detection because of the presence of smaller tampered-with areas. Moreover, disruptive factors in scientific images, such as artifacts, abnormal patterns, and noise, present misleading features like splicing traces, rendering this task difficult. In addition, the scarcity of high-quality datasets of spliced scientific images has limited advancements. Therefore, we propose the uncertainty-guided refinement network (URN) to mitigate these disruptive factors. We also construct a dataset for image splicing detection (SciSp) with 1,290 spliced images by collecting and manually splicing. Comprehensive experiments demonstrate the URN’s superior splicing detection performance.
ER  - 

TY  - JOUR
T1  - Advancements in natural language processing: Implications, challenges, and future directions
AU  - Supriyono, 
AU  - Wibawa, Aji Prasetya
AU  - Suyono, 
AU  - Kurniawan, Fachrul
JO  - Telematics and Informatics Reports
VL  - 16
SP  - 100173
PY  - 2024
DA  - 2024/12/01/
SN  - 2772-5030
DO  - https://doi.org/10.1016/j.teler.2024.100173
UR  - https://www.sciencedirect.com/science/article/pii/S2772503024000598
KW  - Deep learning techniques
KW  - Natural language processing
KW  - Systematic review methodologies
KW  - Text data analysis
KW  - Transformer models
AB  - This research delves into the latest advancements in Natural Language Processing (NLP) and their broader implications, challenges, and future directions. With the ever-increasing volume of text data generated daily from diverse sources, extracting relevant and valuable information is becoming more complex. Conventional manual techniques for handling and examining written information are laborious and susceptible to mistakes, underscoring the necessity for effective automated alternatives. The advancements in Natural Language Processing (NLP), namely in transformer-based models and deep learning techniques, have demonstrated considerable potential in improving the precision and consistency of various NLP applications. This work presents a novel approach that combines systematic review methods with sophisticated NLP approaches to enhance the overall efficiency of NLP systems. The proposed strategy guarantees an organized and clear literature review process, resulting in more informative and contextually relevant results. The report examines NLP's implications, problems, and opportunities, providing significant insights that are anticipated to propel improvements in NLP technology and its application in many industries.
ER  - 

TY  - JOUR
T1  - Blockchain-assisted explainable decision traces (BAXDT): An approach for transparency and accountability in artificial intelligence systems
AU  - Parlak, İsmail Enes
JO  - Knowledge-Based Systems
VL  - 329
SP  - 114402
PY  - 2025
DA  - 2025/11/04/
SN  - 0950-7051
DO  - https://doi.org/10.1016/j.knosys.2025.114402
UR  - https://www.sciencedirect.com/science/article/pii/S0950705125014418
KW  - Explainable artificial intelligence (XAI)
KW  - Blockchain
KW  - Decision traceability
KW  - Artificial intelligence accountability
KW  - Auditability
AB  - The increasing opacity and lack of verifiable audit trails in AI decision-making systems pose significant challenges to establishing trust and accountability, particularly in high-impact domains. This paper introduces Blockchain-Assisted Explainable Decision Traces (BAXDT), a novel architecture designed to enhance the transparency and auditability of AI systems. BAXDT creates comprehensive, immutable records for each AI decision by integrating model outputs, SHAP-based XAI summaries, a novel Explanation Density Metric, and detailed model/data context into a unified JSON trace. The 0.80 threshold for the Explanation Density Metric was empirically supported by Kneedle-based automatic threshold detection. The BAXDT architecture leverages blockchain by recording a cryptographic hash of each decision trace on-chain, while the full trace is stored off-chain. The system's effectiveness was demonstrated through a multi-faceted evaluation: simulations across three diverse public datasets (medical, financial, educational) confirmed its domain-agnostic applicability; a scalability analysis of up to 20,000 traces demonstrated its efficient and linear performance; and a successful deployment on the Ethereum Sepolia public testnet verified its real-world viability. A case study on text data further underscored the framework's flexibility. BAXDT provides a robust framework for documenting AI decisions - what, why, based on what, and when - thereby fostering trustworthy AI and supporting regulatory compliance.
ER  - 

TY  - JOUR
T1  - AI as an emerging trend for managing employee efficiency in the retail and services industries
AU  - Shaunn Mattingly, E.
AU  - Kroes, James R.
AU  - Manikas, Andrew S.
JO  - Business Horizons
PY  - 2025
DA  - 2025/06/28/
SN  - 0007-6813
DO  - https://doi.org/10.1016/j.bushor.2025.06.005
UR  - https://www.sciencedirect.com/science/article/pii/S0007681325001089
KW  - Generative artificial intelligence
KW  - Employee efficiency
KW  - Research and development intensity
KW  - AI benefits by industry
KW  - AI in retail and services
AB  - Generative artificial intelligence (AI) is transforming the nature of work, offering clear benefits in efficiency and innovation. Yet, for practitioners, adoption remains complex because of limited resources and the demands of existing operations. This study explores how firms manage these challenges by examining AI adoption through the lens of the exploration–exploitation trade-off. Using data from public company filings, we analyze adoption rates, drivers and barriers, links to R&D investment, and industry-specific patterns. We find that firms with lower employee efficiency and greater size adopt AI more readily. AI adoption also correlates with changes in the scale and focus of R&D spending. Notably, retail and services firms—among the least efficient in terms of labor productivity—are leading in AI uptake. However, their focus remains on automating routine tasks and expanding product offerings, rather than investing in workforce development. These results provide useful guidance for leaders who want to use AI effectively, particularly in industries needing to improve employee productivity.
ER  - 

TY  - JOUR
T1  - Development and validation of Generative AI Competence Scale (GenAIComp) among university students
AU  - Lee, Seul Chan
AU  - Baby, Tiju
AU  - Vongvit, Rattawut
AU  - Lee, Jieun
AU  - Kim, Young Woo
AU  - Cha, Min Chul
AU  - Yoon, Sol Hee
JO  - Technology in Society
VL  - 84
SP  - 103059
PY  - 2026
DA  - 2026/03/01/
SN  - 0160-791X
DO  - https://doi.org/10.1016/j.techsoc.2025.103059
UR  - https://www.sciencedirect.com/science/article/pii/S0160791X25002490
KW  - Generative AI
KW  - AI competence
KW  - Digital literacy
KW  - GenAIComp
AB  - The rapid development of Generative Artificial Intelligence (Generative AI) across several sectors underscores the need for a systematic tool to evaluate AI competence. Current digital literacy frameworks lack AI-specific competencies, resulting in inconsistencies in the assessment of AI competence. This study aims to establish a standardized assessment framework for Generative AI competence by identifying key skill factors and empirically validating a structured evaluation tool called the Generative AI Competence Scale (GenAIComp). The proposed GenAIComp has five essential factors: Information and Data Literacy, Communication and Collaboration, Digital Content Creation, Safety and Ethics, and Problem-Solving. A quantitative approach was employed, incorporating expert validation, pilot testing, and extensive empirical evaluation involving 1000 participants, principally university students. The factor analysis confirmed a robust 5-factor structure with strong psychometric properties. The final model demonstrated excellent fit indices, confirming its reliability and validity in assessing Generative AI competence across the five key factors. Research demonstrates that educational background considerably impacts AI competence, with individuals from technical disciplines showing a greater aptitude for problem-solving and content generation. Gender-based disparities were noted, with males achieving marginally higher scores in several factors, but with minimal effect sizes. Correlation analysis indicated that perceived AI expertise and frequency of AI utilization significantly influenced competence, especially in data literacy and problem-solving, and exhibited less correlation with ethical awareness. GenAIComp provides a reliable tool for assessing AI competence, helping educators, industry experts, and policymakers to design AI training programs and integrate AI literacy into curricula and thereby AI technology advancement in society. Future research should explore its applicability across cultures and include performance-based assessments to enhance AI competence.
ER  - 

TY  - JOUR
T1  - Stop bugging me! Evading modern-day wiretapping using adversarial perturbations
AU  - Mathov, Yael
AU  - Ben Senior, Tal
AU  - Shabtai, Asaf
AU  - Elovici, Yuval
JO  - Computers & Security
VL  - 121
SP  - 102841
PY  - 2022
DA  - 2022/10/01/
SN  - 0167-4048
DO  - https://doi.org/10.1016/j.cose.2022.102841
UR  - https://www.sciencedirect.com/science/article/pii/S0167404822002358
KW  - Adversarial examples
KW  - Privacy protection
AB  - Mass surveillance systems for voice over IP (VoIP) conversations pose a great risk to privacy. These automated systems use learning models to analyze conversations, and calls that involve specific topics are routed to a human agent for further examination. In this study, we present an adversarial-learning-based framework for privacy protection for VoIP conversations. We present a novel method that finds a universal adversarial perturbation (UAP), which, when added to the audio stream, prevents an eavesdropper from automatically detecting the conversation’s topic. As shown in our experiments, the UAP is agnostic to the speaker or audio length, and its volume can be changed in real time, as needed. Our real-world solution uses a Teensy microcontroller that acts as an external microphone and adds the UAP to the audio in real time. We examine different speakers, VoIP applications (Skype, Zoom, Slack, Google Meet, and Microsoft Teams), and audio lengths. Our results in the real world suggest that our approach is a feasible solution for privacy protection.
ER  - 

TY  - JOUR
T1  - Half a Century of Information Processing & Management: A bibliometric retrospective
AU  - Khorshidi, Mohammad Sadegh
AU  - Merigó, José M.
AU  - Beydoun, Ghassan
JO  - Information Processing & Management
VL  - 62
IS  - 6
SP  - 104238
PY  - 2025
DA  - 2025/11/01/
SN  - 0306-4573
DO  - https://doi.org/10.1016/j.ipm.2025.104238
UR  - https://www.sciencedirect.com/science/article/pii/S0306457325001797
KW  - Information Processing
KW  - Management
KW  - Bibliometrics
KW  - Web of Science
KW  - Scopus
KW  - VOS viewer
AB  - Established in 1963 under the title Information Storage and Retrieval, the journal adopted its current name, Information Processing & Management (IPM), in 1975, reflecting a broadening scope aligned with computational and cognitive developments in information science. This study uses data from Web of Science and Scopus databases to deliver a longitudinal, multi-perspective bibliometric and science mapping analysis of IPM’s evolution from 1963 to 2023. Employing co-citation analysis, bibliographic coupling, keyword co-occurrence, and thematic mapping via VOSviewer and Bibliometrix, the analysis delineates the structural, conceptual, and topical transformation of the journal content. Co-citation networks uncover foundational cores in information retrieval, relevance theory, and evaluation methodologies, while also revealing temporal shifts toward natural language processing, deep learning, and social media analytics. Bibliographic coupling identifies coherent intellectual clusters centered on GNN-based recommendation systems, blockchain-secured infrastructures, and sentiment-aware retrieval frameworks. Keyword co-occurrence and topic evolution trajectories illustrate the journal’s recent pivot toward transformer models, misinformation detection, ethical AI, and interdisciplinary convergence across cognitive science, machine learning, and computational linguistics. Regional co-word analysis underscores epistemological diversity and geographic differentiation across North America, Europe, and East Asia. Productivity and influence metrics highlight the ascent of East Asian institutions and the emergence of globally distributed citation impact. Finally, SciVal-based topic and topic cluster analyses reveal the journal’s role in advancing highly cited research (as measured by FWCI) in areas such as ABSA, multi-view clustering, and health informatics. This work not only charts IPM’s conceptual landscape and disciplinary diffusion but also provides actionable intelligence on the journal’s strategic positioning within the broader information and computational sciences.
ER  - 

TY  - JOUR
T1  - A new perspective on precursors and rare events from a systematic review
AU  - Wen, He
JO  - Journal of Loss Prevention in the Process Industries
VL  - 99
SP  - 105785
PY  - 2026
DA  - 2026/01/01/
SN  - 0950-4230
DO  - https://doi.org/10.1016/j.jlp.2025.105785
UR  - https://www.sciencedirect.com/science/article/pii/S0950423025002438
KW  - Rare events
KW  - Accident precursor
KW  - Bayesian inference
KW  - Logistic scaling
KW  - Safety analytics
AB  - Estimating the probability of ultra-rare but catastrophic events, such as core melts, offshore blowouts, or chemical explosions, poses a fundamental challenge due to the lack of observed failures. To overcome this “zero-failure dilemma,” safety researchers have turned to accident precursors: abnormal conditions that share causal pathways with potential disasters. This paper presents the first cross-sector systematic review of 72 peer-reviewed studies on precursor-based probability estimation, spanning nuclear, chemical, offshore, rail, and autonomous systems. This study classifies existing methods into four methodological families, including frequency-based rules, Bayesian estimators, Accident Sequence Precursor (ASP) logic trees, and Bayesian networks (BNs). Moreover, to address key gaps such as multi-precursor interaction and recurrence modeling, this study proposes a novel Logistic–Exponential (LE) framework. This model captures how accident risk grows with increasing types of precursors and how repeated signals contribute less over time, using a compact and analytically manageable form. This study concludes by identifying future research directions to improve the model's interpretability, cross-domain applicability, and potential for integration into real-time safety monitoring systems.
ER  - 

TY  - JOUR
T1  - The role of artificial intelligence for early warning systems: Status, applicability, guardrails, and ways forward
AU  - Tiggeloven, Timothy
AU  - Pfeiffer, Samira
AU  - Matanó, Alessia
AU  - van den Homberg, Marc
AU  - Thalheimer, Lisa
AU  - Reichstein, Markus
AU  - Torresan, Silvia
JO  - iScience
VL  - 28
IS  - 11
SP  - 113689
PY  - 2025
DA  - 2025/11/21/
SN  - 2589-0042
DO  - https://doi.org/10.1016/j.isci.2025.113689
UR  - https://www.sciencedirect.com/science/article/pii/S2589004225019509
KW  - Earth sciences
KW  - Environmental science
KW  - Remote sensing
KW  - Artificial intelligence
KW  - Social sciences
KW  - Research methodology social sciences
AB  - Summary
Artificial intelligence (AI) is gaining momentum in earth sciences as a tool to analyze complex natural hazards and their impacts. Such analyses are critical for effective Early Warning Systems (EWSs), which is aiming to generate timely and actionable risk information to protect sectors, systems, and people. Despite advancements in AI, its role in EWS remains underexplored across the four pillars of the Early Warning for All (EW4All) framework; risk knowledge, forecasting, warning dissemination and communication and response preparedness. This study draws on a systematic literature review to assess AI methods utilized in the context of EWS, examines their challenges and opportunities and discusses guiding questions for responsible use. Our study highlights key gaps across knowledge, application and policy. Moreover, we call for coordinated efforts to develop responsible AI frameworks that enhance EWS while ensuring they remain inclusive, accessible, and people-centred that ultimately supports the goal of EW4All by 2027.
ER  - 

TY  - JOUR
T1  - “Eh? Aye!”: Categorisation bias for natural human vs AI-augmented voices is influenced by dialect
AU  - Kirk, Neil W.
JO  - Computers in Human Behavior: Artificial Humans
VL  - 4
SP  - 100153
PY  - 2025
DA  - 2025/05/01/
SN  - 2949-8821
DO  - https://doi.org/10.1016/j.chbah.2025.100153
UR  - https://www.sciencedirect.com/science/article/pii/S2949882125000374
KW  - AI
KW  - Voice perception
KW  - Dialect
KW  - Speech
KW  - Voice technology
KW  - Signal detection theory
AB  - Advances in AI-assisted voice technology have made it easier to clone or disguise voices, creating a wide range of synthetic voices using different accents, dialects, and languages. While these developments offer positive applications, they also pose risks for misuse. This raises the question as to whether listeners can reliably distinguish between human and AI-enhanced speech and whether prior experiences and expectations about language varieties that are traditionally less-represented by technology affect this ability. Two experiments were conducted to investigate listeners’ ability to categorise voices as human or AI-enhanced in both a standard and a regional Scottish dialect. Using a Signal Detection Theory framework, both experiments explored participants' sensitivity and categorisation biases. In Experiment 1 (N = 100), a predominantly Scottish sample showed above-chance performance in distinguishing between human and AI-enhanced voices, but there was no significant effect of dialect on sensitivity. However, listeners exhibited a bias toward categorising voices as “human”, which was concentrated within the regional Dundonian Scots dialect. In Experiment 2 (N = 100) participants from southern and eastern England, demonstrated reduced overall sensitivity and a Human Categorisation Bias that was more evenly spread across the two dialects. These findings have implications for the growing use of AI-assisted voice technology in linguistically diverse contexts, highlighting both the potential for enhanced representation of Minority, Indigenous, Non-standard and Dialect (MIND) varieties, and the risks of AI misuse.
ER  - 

TY  - JOUR
T1  - Navigating parasitic strategy
AU  - Ma, Hao
AU  - Su, Mengyue
JO  - Organizational Dynamics
VL  - 54
IS  - 4, Part 2
SP  - 101178
PY  - 2025
DA  - 2025/12/01/
SN  - 0090-2616
DO  - https://doi.org/10.1016/j.orgdyn.2025.101178
UR  - https://www.sciencedirect.com/science/article/pii/S0090261625000555
KW  - Parasites
KW  - Hosts
KW  - Parasitic strategy
KW  - Symbiosis
KW  - Digital economy
AB  - Parasitism is a unique modus operandi of venture creation and development, particularly within the realm of the digital economy, where parasite firms make their living within or onto other firms or ecosystems and access their hosts’ resources uninvited. Based on the potential harm and benefits offered by the parasites, four types of parasites could be outlined that include commensal, reciprocal, siphoned, and abducted. Parasite firms need to attend to the three-stage implementation process of the parasitic strategy in terms of selection and engagement of a proper host, adaptation and penetration within the host, and renewal and transformation for ultimate self-fulfillment. This article offers a myriad of practical guidance for executives pursuing such a parasitic strategy as well as advices for executives in the corresponding hosts.
ER  - 

TY  - JOUR
T1  - Interpretable research of fuzzy methods: A literature survey
AU  - Zhang, Ke
AU  - Shao, Tianhao
AU  - Sun, Yi
AU  - Xu, Xinyao
AU  - Zhang, Xiaoxiong
AU  - Zhou, Xiaolei
AU  - Ding, Kun
AU  - Huang, Shan
JO  - Information Fusion
VL  - 126
SP  - 103524
PY  - 2026
DA  - 2026/02/01/
SN  - 1566-2535
DO  - https://doi.org/10.1016/j.inffus.2025.103524
UR  - https://www.sciencedirect.com/science/article/pii/S1566253525005962
KW  - Explainable artificial intelligence
KW  - Fuzzy system
KW  - Deep fuzzy models
KW  - Ensemble fuzzy model
KW  - Interpretability
KW  - Explainability
AB  - Recently, fuzzy logic and fuzzy systems have become a key focus in artificial intelligence (AI) research due to their superior interpretability. By leveraging human-understandable semantic rules and intuitive membership functions, they effectively enhance model transparency. However, as fusion between fuzzy methods and complex models like deep learning increases, the inherent interpretability of traditional fuzzy systems faces challenges—not all fuzzy models inherently maintain interpretability, especially when rule bases expand or couple with black-box architectures. Therefore, in the context of explainable artificial intelligence (XAI), in-depth research on the principles, evaluation criteria, and optimization strategies of interpretable fuzzy methods is essential for enhancing the transparency, trustworthiness, and user acceptance of AI systems. This paper provides a comprehensive review and classification of relevant developments, explores accuracy-interpretability trade-off in fuzzy models, and outlines future directions. First, we introduce foundational definitions of interpretability and typical XAI methodologies. Subsequently, we analyze the unique interpretability advantages of fuzzy models through the lenses of fuzzy sets and fuzzy rules. A novel five-layer tree-like taxonomy is proposed, categorizing interpretable fuzzy methods into fourteen distinct classes based on interpretation types and model structures. Further, we present interpretability evaluation metrics for fuzzy sets, rules, and rule bases from semantic and complexity perspectives, alongside discussions on the trade-off between interpretability and accuracy. Fuzzy methods under trustworthy AI and practical applications across diverse scenarios are also illustrated. Finally, we identify key challenges and future trends in interpretable fuzzy research. The main aim is to deepen scholarly understanding of the field and establish a robust foundation for subsequent studies.
ER  - 

TY  - JOUR
T1  - A Comprehensive review on technological breakthroughs in precision agriculture: IoT and emerging data analytics
AU  - Saini, Anil Kumar
AU  - Yadav, Anshul Kumar
AU  - Dhiraj, 
JO  - European Journal of Agronomy
VL  - 163
SP  - 127440
PY  - 2025
DA  - 2025/02/01/
SN  - 1161-0301
DO  - https://doi.org/10.1016/j.eja.2024.127440
UR  - https://www.sciencedirect.com/science/article/pii/S1161030124003617
KW  - Precision Agriculture
KW  - Internet of Things
KW  - Artificial Intelligence
KW  - Communication technology
AB  - Rapid population expansion has led to a corresponding rise in the demand for sustenance. Researchers have found that traditional agricultural practices are insufficient to meet the demands of commodities, and their inefficiency poses the most pressing obstacle to addressing the growing global food demand. Precision agriculture (PA) is an advanced hierarchy farming system supported by multidisciplinary technologies such as specialized sensors, communication protocols, algorithms, and management tools, helping mitigate the problems of conventional farming by ensuring maximum production and minimum wastage. Given the rapid evolution of the aforementioned multidisciplinary technologies, this review paper analyzed 24337 research documents from 1938 to April 2024 using bibliographical software from the Scopus dataset. Internet of Things (IoT), Agriculture Robots (AR), and Artificial Intelligence (AI) are currently driving ongoing research, with frequency occurrences of 12.245, 8.259, and 7.791, highlighting the trend towards interconnected farming systems and data-driven automated systems. Bibliographical evidence indicates the current utilization of AI, AR, and IoT for accurate assessments like crop yield prediction, disease and weed detection, and soil analysis. Additionally, China is the most productive country in terms of publication, while the United States leads in terms of patents. This review paper also explores emerging trends that could guide future research, including blockchain technology, big data analysis, computing paradigms, and drone technology. Subsequently, a PA framework has been suggested to facilitate innovation in this field, followed by the open issues, highlighting the ongoing concerns related to insufficient infrastructure, integration, cost, and security measures, with the aim to engage all stakeholders.
ER  - 

TY  - JOUR
T1  - RecMaL: Rectify the malware family label via hybrid analysis
AU  - Yang, Wang
AU  - Gao, Mingzhe
AU  - Chen, Ligeng
AU  - Liu, Zhengxuan
AU  - Ying, Lingyun
JO  - Computers & Security
VL  - 128
SP  - 103177
PY  - 2023
DA  - 2023/05/01/
SN  - 0167-4048
DO  - https://doi.org/10.1016/j.cose.2023.103177
UR  - https://www.sciencedirect.com/science/article/pii/S0167404823000871
KW  - Malware family
KW  - Hybrid analysis
KW  - AV labels
KW  - Behavioral semantics
KW  - Label rectification
AB  - Intelligent applications can be significantly impacted by incorrectly categorized data. Recently, artificial intelligence technology has been deployed in an increasing number of security-related scenarios, but the issue of data mislabeling has received little attention. We concentrate on the problem of malware mislabeling in this paper. Unfortunately, in the security field, the mislabeling issue of malware is not taken seriously. Existing work attempts to aggregate the AV labels to alleviate malware mislabeling. This will mislead the security analyst and pass the error to subsequent data-driven applications. Therefore, we conduct an in-depth analysis to explore the severity of the malware mislabel issue, and try to rectify the description of malware generated from anti-virus engines. We first propose a malware label correction tool called RecMaL. It employs hybrid analyses for malware label rectifying. According to the thorough exploratory analysis, we figure out the core reasons for mislabeling issues and summarize them into 3 types. To verify the effectiveness and how RecMaL benefits the downstream applications (e.g., malware classification), we evaluate RecMaL through a series of experiments and show that the main components of RecMaL improve the performance, which proves our method effectively alleviates the mislabeling issue.
ER  - 

TY  - JOUR
T1  - Challenges of implementing ChatGPT on education: Systematic literature review
AU  - García-López, Iván Miguel
AU  - González González, Carina Soledad
AU  - Ramírez-Montoya, María-Soledad
AU  - Molina-Espinosa, José-Martín
JO  - International Journal of Educational Research Open
VL  - 8
SP  - 100401
PY  - 2025
DA  - 2025/06/01/
SN  - 2666-3740
DO  - https://doi.org/10.1016/j.ijedro.2024.100401
UR  - https://www.sciencedirect.com/science/article/pii/S2666374024000839
KW  - Higher education
KW  - Educational innovation
KW  - Chatgpt
KW  - Generative artificial intelligence
KW  - Education 4.0
AB  - Since its launch in 2022, ChatGPT has sparked considerable interest in higher education, raising debates about its benefits, challenges, and ethical implications. This systematic literature review, spanning January 2019 to January 2024, analyzes 42 articles from Web of Science and Scopus to identify key opportunities and challenges in its academic integration. Four core issues emerge: (a) technological integration and obsolescence, emphasizing the need for scalable, modular infrastructures; (b) personalization and equity, focusing on the balance between individualized learning and avoiding algorithmic bias; (c) data quality and security, highlighting the importance of transparent data management and robust encryption to protect sensitive information; and (d) ethics and human-AI collaboration, stressing the importance of institutional policies and continuous teacher intervention to ensure responsible and effective use. This study advances the discourse by recommending sustainable strategies for AI adoption, including professional development and fairness audits, while underscoring the critical role of human oversight in maximizing ChatGPT's educational impact. Ultimately, it offers actionable insights for institutions to align AI use with ethical principles and long-term educational goals.
ER  - 

TY  - JOUR
T1  - Societal impacts of chatbot and mitigation strategies for negative impacts: A large-scale qualitative survey of ChatGPT users
AU  - Wei, Xinyi
AU  - Chu, Xiaoyuan
AU  - Geng, Jingyu
AU  - Wang, Yuhui
AU  - Wang, Pengcheng
AU  - Wang, HongXia
AU  - Wang, Caiyu
AU  - Lei, Li
JO  - Technology in Society
VL  - 77
SP  - 102566
PY  - 2024
DA  - 2024/06/01/
SN  - 0160-791X
DO  - https://doi.org/10.1016/j.techsoc.2024.102566
UR  - https://www.sciencedirect.com/science/article/pii/S0160791X24001143
KW  - Chatbot
KW  - Social impact
KW  - Economy
KW  - Policy
KW  - Management
KW  - Technological optimization
AB  - Discussing ChatGPT's societal impact often focuses on specific domains or groups, typically involving a limited number of participants. In contrast, this study investigates the impact of this artificial intelligence (AI) on society as a whole by conducting a qualitative online survey with 1298 daily users from Hinterland of China across various industries. Identified key benefits include enhanced efficiency and production, improved communication, understand, and cooperation, increased social equalization and harmony, and bolstered public mental health. On the downside, concerns arose about (un)employment issues, biased or false outputs, potential for unethical applications, and user dependency and its negative consequences. To address these concerns, participants suggested use restrictions and management, promoting independent and critical thinking/practice, enhancing user literacy, increasing individual irreplaceability, technological improvement and optimization, and safeguarding against AI threats to human society. The above results elucidate the impact of ChatGPT on various social domains and potential solutions for its negative effects. These findings augment existing research on AI user models from a societal impact perspective, providing a foundation for proposing AI user models that consider the social implications of AI. These outcomes also aim to assist policymakers, researchers, and designers in better understanding the societal impact and user concerns regarding AI, thereby informing the design and enhancement of products, services, or policies.
ER  - 

TY  - JOUR
T1  - A Dual-Layer Attention Based CAPTCHA Recognition Approach with Guided Visual Attention
AU  - Derea, Zaid
AU  - Zou, Beiji
AU  - Kui, Xiaoyan
AU  - Thobhani, Alaa
AU  - Abdussalam, Amr
JO  - CMES - Computer Modeling in Engineering and Sciences
VL  - 142
IS  - 3
SP  - 2841
EP  - 2867
PY  - 2025
DA  - 2025/03/03/
SN  - 1526-1492
DO  - https://doi.org/10.32604/cmes.2025.059586
UR  - https://www.sciencedirect.com/science/article/pii/S1526149225000499
KW  - Text-based CAPTCHA image recognition
KW  - guided visual attention
KW  - web security
KW  - computer vision
AB  - Enhancing website security is crucial to combat malicious activities, and CAPTCHA (Completely Automated Public Turing tests to tell Computers and Humans Apart) has become a key method to distinguish humans from bots. While text-based CAPTCHAs are designed to challenge machines while remaining human-readable, recent advances in deep learning have enabled models to recognize them with remarkable efficiency. In this regard, we propose a novel two-layer visual attention framework for CAPTCHA recognition that builds on traditional attention mechanisms by incorporating Guided Visual Attention (GVA), which sharpens focus on relevant visual features. We have specifically adapted the well-established image captioning task to address this need. Our approach utilizes the first-level attention module as guidance to the second-level attention component, incorporating two LSTM (Long Short-Term Memory) layers to enhance CAPTCHA recognition. Our extensive evaluation across four diverse datasets—Weibo, BoC (Bank of China), Gregwar, and Captcha 0.3—shows the adaptability and efficacy of our method. Our approach demonstrated impressive performance, achieving an accuracy of 96.70% for BoC and 95.92% for Webo. These results underscore the effectiveness of our method in accurately recognizing and processing CAPTCHA datasets, showcasing its robustness, reliability, and ability to handle varied challenges in CAPTCHA recognition.
ER  - 

TY  - JOUR
T1  - Intercepting Hail Hydra: Real-time detection of Algorithmically Generated Domains
AU  - Casino, Fran
AU  - Lykousas, Nikolaos
AU  - Homoliak, Ivan
AU  - Patsakis, Constantinos
AU  - Hernandez-Castro, Julio
JO  - Journal of Network and Computer Applications
VL  - 190
SP  - 103135
PY  - 2021
DA  - 2021/09/15/
SN  - 1084-8045
DO  - https://doi.org/10.1016/j.jnca.2021.103135
UR  - https://www.sciencedirect.com/science/article/pii/S1084804521001545
KW  - Malware
KW  - Domain Generation Algorithms
KW  - Botnets
KW  - DNS
KW  - Algorithmically Generated Domain
AB  - A crucial technical challenge for cybercriminals is to keep control over the potentially millions of infected devices that build up their botnets, without compromising the robustness of their attacks. A single, fixed C&C server, for example, can be trivially detected either by binary or traffic analysis and immediately sink-holed or taken-down by security researchers or law enforcement. Botnets often use Domain Generation Algorithms (DGAs), primarily to evade take-down attempts. DGAs can enlarge the lifespan of a malware campaign, thus potentially enhancing its profitability. They can also contribute to hindering attack accountability. In this work, we introduce HYDRAS, the most comprehensive and representative dataset of Algorithmically-Generated Domains (AGD) available to date. The dataset contains more than 100 DGA families, including both real-world and adversarially designed ones. We analyse the dataset and discuss the possibility of differentiating between benign requests (to real domains) and malicious ones (to AGDs) in real-time. The simultaneous study of so many families and variants introduces several challenges; nonetheless, it alleviates biases found in previous literature employing small datasets which are frequently overfitted, exploiting characteristic features of particular families that do not generalise well. We thoroughly compare our approach with the current state-of-the-art and highlight some methodological shortcomings in the actual state of practice. The outcomes obtained show that our proposed approach significantly outperforms the current state-of-the-art in terms of both classification performance and efficiency.
ER  - 

TY  - JOUR
T1  - Research on cooperative control model of power grid equipment manufacturing quality risk under blockchain
AU  - Chen, Yunfeng
AU  - Liu, Jicheng
AU  - Song, Yanan
AU  - Duan, Bingfan
AU  - Meng, Xian
JO  - Computers & Industrial Engineering
VL  - 208
SP  - 111400
PY  - 2025
DA  - 2025/10/01/
SN  - 0360-8352
DO  - https://doi.org/10.1016/j.cie.2025.111400
UR  - https://www.sciencedirect.com/science/article/pii/S0360835225005467
KW  - Blockchain
KW  - Grid equipment
KW  - Manufacturing quality
KW  - Risk assessment
KW  - Collaborative management
AB  - Power grid equipment is a major infrastructure for constructing a new power system and promoting the rapid development of new quality productivity. The reliability of power grid equipment manufacturing quality is particularly important. In order to improve the quality risk management capability of power grid equipment, this study firstly identifies and evaluates the quality risk factors based on text mining technology, complex network theory and machine learning algorithm. Secondly, it constructs a blockchain-based cloud supervision manufacturing model for real-time quality risk control. It analyses the gaming behaviours and strategy choices of grid company, manufacturer and quality inspector in the blockchain environment for realizing the collaborative control. Thirdly, with the above three-party game strategy, a multi-objective quality risk control model is further presented under the blockchain. It considers the quality traceability, quality consistency, and customer satisfaction, for controlling the grid equipment manufacturing quality risk. Finally, the improved particle swarm algorithm (IPSO) is used to solve the case study. The results show that the blockchain technology can help to reduce the equipment manufacturing quality risk, and improve the equipment manufacturing quality performance. This study provides a theoretical basis and practical guidance for the management and control of quality risk in the grid equipment manufacturing.
ER  - 

TY  - JOUR
T1  - Con-Detect: Detecting adversarially perturbed natural language inputs to deep classifiers through holistic analysis
AU  - Ali, Hassan
AU  - Khan, Muhammad Suleman
AU  - AlGhadhban, Amer
AU  - Alazmi, Meshari
AU  - Alzamil, Ahmed
AU  - Al-utaibi, Khaled
AU  - Qadir, Junaid
JO  - Computers & Security
VL  - 132
SP  - 103367
PY  - 2023
DA  - 2023/09/01/
SN  - 0167-4048
DO  - https://doi.org/10.1016/j.cose.2023.103367
UR  - https://www.sciencedirect.com/science/article/pii/S0167404823002778
KW  - Machine learning security
KW  - Adversarial detection
KW  - Adversarial machine learning
KW  - Secure natural language processing
KW  - Adversarial signatures
AB  - Deep Learning (DL) algorithms have shown wonders in many Natural Language Processing (NLP) tasks such as language-to-language translation, spam filtering, fake-news detection, and comprehension understanding. However, research has shown that the adversarial vulnerabilities of deep learning networks manifest themselves when DL is used for NLP tasks. Most mitigation techniques proposed to date are supervised—relying on adversarial retraining to improve the robustness—which is impractical. This work introduces a novel, unsupervised detection methodology for detecting adversarial inputs to NLP classifiers. In summary, we note that minimally perturbing an input to change a model’s output—a major strength of adversarial attacks—is a weakness that leaves unique statistical marks reflected in the cumulative contribution scores of the input. Particularly, we show that the cumulative contribution score, called CF-score, of adversarial inputs is generally greater than that of the clean inputs. We thus propose Con-Detect—a Contribution based Detection method—for detecting adversarial attacks against NLP classifiers. Con-Detect can be deployed with any classifier without having to retrain it. We experiment with multiple attackers—Text-bugger, Text-fooler, PWWS—on several architectures—MLP, CNN, LSTM, Hybrid CNN-RNN, BERT—trained for different classification tasks—IMDB sentiment classification, fake-news classification, AG news topic classification—under different threat models—Con-Detect-blind attacks, Con-Detect-aware attacks, and Con-Detect-adaptive attacks—and show that Con-Detect can reduce the attack success rate (ASR) of different attacks from 100% to as low as 0% for the best cases and ≈70% for the worst case. Even in the worst case, we note a 100% increase in the required number of queries and a 50% increase in the number of words perturbed, suggesting that Con-Detect is hard to evade.
ER  - 

TY  - JOUR
T1  - Common vulnerability scoring system prediction based on open source intelligence information sources
AU  - Kühn, Philipp
AU  - Relke, David N.
AU  - Reuter, Christian
JO  - Computers & Security
VL  - 131
SP  - 103286
PY  - 2023
DA  - 2023/08/01/
SN  - 0167-4048
DO  - https://doi.org/10.1016/j.cose.2023.103286
UR  - https://www.sciencedirect.com/science/article/pii/S0167404823001967
KW  - IT Security
KW  - Common vulnerability scoring system
KW  - Classification
KW  - National vulnerability database
KW  - Security management
KW  - Deep learning
AB  - The number of newly published vulnerabilities is constantly increasing. Until now, the information available when a new vulnerability is published is manually assessed by experts using a Common Vulnerability Scoring System (CVSS) vector and score. This assessment is time consuming and requires expertise. Various works already try to predict CVSS vectors or scores using machine learning based on the textual descriptions of the vulnerability to enable faster assessment. However, for this purpose, previous works only use the texts available in databases such as National Vulnerability Database. With this work, the publicly available web pages referenced in the National Vulnerability Database are analyzed and made available as sources of texts through web scraping. A Deep Learning based method for predicting the CVSS vector is implemented and evaluated. The present work provides a classification of the National Vulnerability Database’s reference texts based on the suitability and crawlability of their texts. While we identified the overall influence of the additional texts is negligible, we outperformed the state-of-the-art with our Deep Learning prediction models.
ER  - 

TY  - JOUR
T1  - The enabling technologies for digitalization in the chemical process industry
AU  - Pietrasik, Marcin
AU  - Wilbik, Anna
AU  - Grefen, Paul
JO  - Digital Chemical Engineering
VL  - 12
SP  - 100161
PY  - 2024
DA  - 2024/09/01/
SN  - 2772-5081
DO  - https://doi.org/10.1016/j.dche.2024.100161
UR  - https://www.sciencedirect.com/science/article/pii/S2772508124000231
KW  - Digitalization
KW  - Chemical process industry
KW  - Enabling technology
AB  - In this paper, we provide an overview of the technologies that enable digitalization in the chemical process industry and describe their applications to solve problems in industrial settings. This is done through the identification and categorization of these technologies, thereby providing structure to an otherwise loosely connected basket of technologies and casting a spotlight on state-of-the-art technologies that offer great potential but are still underutilized in industrial applications. Furthermore, we identify the problem domains that characterize the chemical process industry and connect them to development aspects in the industry that lend themselves to digital solutions. For each of these connections, we select the technologies most essential to bridging the gap between problem and solution. This allows practitioners to better understand the relevancy of digitalization to their problems and provides a starting point for further investigation of potential solutions. The connections are substantiated by reference to successful industrial applications, highlighting previous works that have been published in the field. They are further verified by industry experts through brainstorm sessions, interviews, and a workshop.
ER  - 

TY  - JOUR
T1  - Privacy and personal data risk governance for generative artificial intelligence: A Chinese perspective
AU  - Ye, Xiongbiao
AU  - Yan, Yuhong
AU  - Li, Jia
AU  - Jiang, Bo
JO  - Telecommunications Policy
VL  - 48
IS  - 10
SP  - 102851
PY  - 2024
DA  - 2024/11/01/
SN  - 0308-5961
DO  - https://doi.org/10.1016/j.telpol.2024.102851
UR  - https://www.sciencedirect.com/science/article/pii/S0308596124001484
KW  - Generative AI
KW  - Privacy and personal data protection
KW  - Risk governance
KW  - Chinese law
AB  - The rapid development of generative artificial intelligence (AI) has attracted global attention and posed challenges to existing data governance frameworks. The increased technical complexity and expanded scale of data usage not only make it more difficult to regulate AI but also present challenges for the current legal system. This article, which takes ChatGPT's training data and working principles as a starting point, examines specific privacy risks, data leakage risks, and personal data risks posed by generative AI. It also analyzes the latest practices in privacy and personal data protection in China. This article finds that while China's governance on privacy and personal data protection takes a macro-micro integration approach and a private-and-public law integration approach, there are shortcomings in the legal system. Given that the current personal data protection system centered on individual control is unsuitable for the modes of data processing by generative AI, and that private law is insufficient in safeguarding data privacy, urgent institutional innovation is needed to achieve the objective of “trustworthy AI.”
ER  - 

TY  - JOUR
T1  - “ChatGPT seems too good to be true”: College students’ use and perceptions of generative AI
AU  - Baek, Clare
AU  - Tate, Tamara
AU  - Warschauer, Mark
JO  - Computers and Education: Artificial Intelligence
VL  - 7
SP  - 100294
PY  - 2024
DA  - 2024/12/01/
SN  - 2666-920X
DO  - https://doi.org/10.1016/j.caeai.2024.100294
UR  - https://www.sciencedirect.com/science/article/pii/S2666920X24000973
KW  - Generative AI
KW  - Artificial intelligence
KW  - Equity
KW  - Higher education
KW  - Educational technology
AB  - This study investigates how U.S. college students (N = 1001) perceive and use ChatGPT, exploring its relationship with societal structures and student characteristics. Regression results show that gender, age, major, institution type, and institutional policy significantly influenced ChatGPT use for general, writing, and programming tasks. Students in their 30s–40s were more likely to use ChatGPT frequently than younger students. Non-native English speakers were more likely than native speakers to use ChatGPT frequently for writing, suggesting its potential as a support tool for language learners. Institutional policies allowing ChatGPT use predicted higher use of ChatGPT. Thematic analysis and natural language processing of open-ended responses revealed varied attitudes towards ChatGPT, with some fearing institutional punishment for using ChatGPT and others confident in their appropriate use of ChatGPT. Computer science majors expressed concerns about job displacement due to the advent of generative AI. Higher-income students generally viewed ChatGPT more positively than their lower-income counterparts. Our research underscores how technology can both empower and marginalize within educational settings; we advocate for equitable integration of AI in academic environments for diverse students.
ER  - 

TY  - JOUR
T1  - Exploring the Impact of Data Poisoning Attacks on Machine Learning Model Reliability
AU  - Verde, Laura
AU  - Marulli, Fiammetta
AU  - Marrone, Stefano
JO  - Procedia Computer Science
VL  - 192
SP  - 2624
EP  - 2632
PY  - 2021
DA  - 2021/01/01/
T2  - Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021
SN  - 1877-0509
DO  - https://doi.org/10.1016/j.procs.2021.09.032
UR  - https://www.sciencedirect.com/science/article/pii/S1877050921017695
KW  - Poisoned Big Data
KW  - Data Poisoning Attacks
KW  - Security
KW  - Reliability
KW  - Resilient Machine Learning
KW  - Disorders detection
KW  - Voice quality assessment.
AB  - Recent years have seen the widespread adoption of Artificial Intelligence techniques in several domains, including healthcare, justice, assisted driving and Natural Language Processing (NLP) based applications (e.g., the Fake News detection). Those mentioned are just a few examples of some domains that are particularly critical and sensitive to the reliability of the adopted machine learning systems. Therefore, several Artificial Intelligence approaches were adopted as support to realize easy and reliable solutions aimed at improving the early diagnosis, personalized treatment, remote patient monitoring and better decision-making with a consequent reduction of healthcare costs. Recent studies have shown that these techniques are venerable to attacks by adversaries at phases of artificial intelligence. Poisoned data set are the most common attack to the reliability of Artificial Intelligence approaches. Noise, for example, can have a significant impact on the overall performance of a machine learning model. This study discusses the strength of impact of noise on classification algorithms. In detail, the reliability of several machine learning techniques to distinguish correctly pathological and healthy voices by analysing poisoning data was evaluated. Voice samples selected by available database, widely used in research sector, the Saarbruecken Voice Database, were processed and analysed to evaluate the resilience and classification accuracy of these techniques. All analyses are evaluated in terms of accuracy, specificity, sensitivity, F1-score and ROC area.
ER  - 

TY  - JOUR
T1  - Architectural tactics in software architecture: A systematic mapping study
AU  - Márquez, Gastón
AU  - Astudillo, Hernán
AU  - Kazman, Rick
JO  - Journal of Systems and Software
VL  - 197
SP  - 111558
PY  - 2023
DA  - 2023/03/01/
SN  - 0164-1212
DO  - https://doi.org/10.1016/j.jss.2022.111558
UR  - https://www.sciencedirect.com/science/article/pii/S0164121222002345
KW  - Architectural tactics
KW  - Systematic mapping study
KW  - Software architecture
KW  - Quality attributes
AB  - Architectural tactics are a key abstraction of software architecture, and support the systematic design and analysis of software architectures to satisfy quality attributes. Since originally proposed in 2003, architectural tactics have been extended and adapted to address additional quality attributes and newer kinds of systems, making quite hard for researchers and practitioners to master this growing body of specialized knowledge. This paper presents the design, execution and results of a systematic mapping study of architectural tactics in software architecture literature. The study found 552 studies in well-known digital libraries, of which 79 were selected and 12 more were added with snowballing, giving a total of 91 primary studies. Key findings are: (i) little rigor has been used to characterize and define architectural tactics; (ii) most architectural tactics proposed in the literature do not conform to the original definition; and (iii) there is little industrial evidence about the use of architectural tactics. This study organizes and summarizes the scientific literature to date about architectural tactics, identifies research opportunities, and argues for the need of more systematic definition and description of tactics. Editor’s note: Open Science material was validated by the Journal of Systems and Software Open Science Board.
ER  - 

TY  - JOUR
T1  - CSFuzzer: A grey-box fuzzer for network protocol using context-aware state feedback
AU  - Song, Xiangpu
AU  - Zeng, Yingpei
AU  - Wu, Jianliang
AU  - Li, Hao
AU  - Zuo, Chaoshun
AU  - Zhao, Qingchuan
AU  - Guo, Shanqing
JO  - Computers & Security
VL  - 157
SP  - 104581
PY  - 2025
DA  - 2025/10/01/
SN  - 0167-4048
DO  - https://doi.org/10.1016/j.cose.2025.104581
UR  - https://www.sciencedirect.com/science/article/pii/S0167404825002706
KW  - Protocol fuzzing
KW  - State feedback
KW  - Software testing
AB  - Code coverage-guided fuzzers have achieved great success in discovering vulnerabilities, but since code coverage does not adequately describe protocol states, they are not effective enough for protocol fuzzing. Although there has been some work introducing state feedback to guide state exploration in protocol fuzzing, they ignore the complexity of protocol state space, e.g., state variables have different categories and are diverse in data type and number, facing the challenges of inaccurate state variable identification and low fuzzing efficiency. In this paper, we propose a novel context-aware state-guided fuzzing approach, CSFuzzer, to address the above challenges. CSFuzzer first divides the state variables into two categories, i.e., protocol-state variables and sub-state variables based on the context of the states, and automatically identifies and distinguishes these two categories of state variables from code. Then, CSFuzzer uses a new state coverage metric named context-aware state transition coverage to more efficiently guide fuzzing. We have implemented a prototype of CSFuzzer and evaluated it on 12 open-source protocol programs. Our experiments show that CSFuzzer outperforms the existing state-of-the-art fuzzers in terms of code and state coverage as well as fuzzing efficiency. CSFuzzer successfully discovered 10 zero-day vulnerabilities, which have been confirmed by the stakeholders and assigned 9 CVEs/CNVDs.
ER  - 

TY  - JOUR
T1  - Uncovering the impact of IP location display on user behavior in China's social platforms: A policy-driven analysis
AU  - Li, Jiaxuan
AU  - Luo, Yifan
AU  - Yuan, Qinjian
JO  - Telecommunications Policy
VL  - 49
IS  - 6
SP  - 102978
PY  - 2025
DA  - 2025/07/01/
SN  - 0308-5961
DO  - https://doi.org/10.1016/j.telpol.2025.102978
UR  - https://www.sciencedirect.com/science/article/pii/S0308596125000758
KW  - IP location display
KW  - Social media platforms
KW  - Privacy disclosure
KW  - Behavior characterization
KW  - Geographical discrimination
KW  - Difference-in-differences
AB  - To explore the impact of IP location display policies on user behavior on social media platforms, this study conducted a main experiment and three extended experiments. Based on Panopticon theory and Privacy calculus theory, this study used a Difference-in-Differences approach in the main experiment to analyze the impact of IP location display policies on the user comments volume, emotion expression and privacy disclosure. Additionally, this study employed dynamic topic modeling, text mining techniques and Regression Discontinuity Design in the extended study to explore the geographical heterogeneity of IP location display policies, the impact on user comment dynamics, and location-based incivility. The findings not only deepen the understanding of social media privacy and IP location policy research, but also provide valuable insights for online policymakers, social media platform managers and Internet users about the impact of IP location display.
ER  - 

TY  - JOUR
T1  - Generative spatial artificial intelligence for sustainable smart cities: A pioneering large flow model for urban digital twin
AU  - Huang, Jeffrey
AU  - Bibri, Simon Elias
AU  - Keel, Paul
JO  - Environmental Science and Ecotechnology
VL  - 24
SP  - 100526
PY  - 2025
DA  - 2025/03/01/
SN  - 2666-4984
DO  - https://doi.org/10.1016/j.ese.2025.100526
UR  - https://www.sciencedirect.com/science/article/pii/S2666498425000043
KW  - Sustainable smart cities
KW  - Generative artificial intelligence
KW  - Generative spatial artificial intelligence
KW  - Foundation models
KW  - Large flow model
KW  - Urban digital twin
KW  - Urban planning and design
AB  - Rapid urbanization, alongside escalating resource depletion and ecological degradation, underscores the critical need for innovative urban development solutions. In response, sustainable smart cities are increasingly turning to cutting-edge technologies—such as Generative Artificial Intelligence (GenAI), Foundation Models (FMs), and Urban Digital Twin (UDT) frameworks—to transform urban planning and design practices. These transformative tools provide advanced capabilities to analyze complex urban systems, optimize resource management, and enable evidence-based decision-making. Despite recent progress, research on integrating GenAI and FMs into UDT frameworks remains scant, leaving gaps in our ability to capture complex urban flows and multimodal dynamics essential to achieving environmental sustainability goals. Moreover, the lack of a robust theoretical foundation and real-world operationalization of these tools hampers comprehensive modeling and practical adoption. This study introduces a pioneering Large Flow Model (LFM), grounded in a robust foundational framework and designed with GenAI capabilities. It is specifically tailored for integration into UDT systems to enhance predictive analytics, adaptive learning, and complex data management functionalities. To validate its applicability and relevance, the Blue City Project in Lausanne City is examined as a case study, showcasing the ability of the LFM to effectively model and analyze urban flows—namely mobility, goods, energy, waste, materials, and biodiversity—critical to advancing environmental sustainability. This study highlights how the LFM addresses the spatial challenges inherent in current UDT frameworks. The LFM demonstrates its novelty in comprehensive urban modeling and analysis by completing impartial city data, estimating flow data in new locations, predicting the evolution of flow data, and offering a holistic understanding of urban dynamics and their interconnections. The model enhances decision-making processes, supports evidence-based planning and design, fosters integrated development strategies, and enables the development of more efficient, resilient, and sustainable urban environments. This research advances both the theoretical and practical dimensions of AI-driven, environmentally sustainable urban development by operationalizing GenAI and FMs within UDT frameworks. It provides sophisticated tools and valuable insights for urban planners, designers, policymakers, and researchers to address the complexities of modern cities and accelerate the transition towards sustainable urban futures.
ER  - 

TY  - JOUR
T1  - Distributed machine learning for next-generation communication networks: A survey on privacy, fairness, efficiency, and trade-offs
AU  - Zhang, Wenqi
AU  - Zhan, Dongyang
AU  - Yu, Haining
AU  - Zhang, Langtong
AU  - Zhao, Bei
AU  - Du, Xuetao
AU  - Tian, Zhihong
JO  - Information Fusion
VL  - 126
SP  - 103657
PY  - 2026
DA  - 2026/02/01/
SN  - 1566-2535
DO  - https://doi.org/10.1016/j.inffus.2025.103657
UR  - https://www.sciencedirect.com/science/article/pii/S1566253525007298
KW  - Distributed machine learning
KW  - Federated learning
KW  - Split learning
KW  - Privacy preservation
KW  - Fairness
KW  - Efficiency
AB  - With the rise of next-generation communication networks such as 6G, IoT, and edge computing, distributed machine learning (DML) has emerged as a key enabling technology. Among DML paradigms, federated learning (FL) and split learning (SL) are particularly promising for decentralized model training with privacy preservation. However, their integration into communication systems introduces new challenges involving communication efficiency, resource constraints, and system adaptability. This survey provides a comprehensive analysis of DML architectural designs, deployment paradigms, and optimization strategies. We systematically compare FL and SL, explore optimization dimensions – privacy, fairness, and efficiency – and propose a novel three-dimensional octant classification framework to characterize trade-offs. Practical deployment scenarios, technical challenges, and emerging trends are also discussed, with emphasis on 6G, IoT, and multi-agent intelligent systems. This work offers actionable insights and a structured foundation for future DML research in communication-centric environments. By emphasizing the fusion of privacy, fairness, and efficiency objectives, this work contributes to the development of trustworthy and optimized intelligent systems for next-generation communication networks.
ER  - 

TY  - JOUR
T1  - BIM ontology for information management (BIM-OIM)
AU  - F.H, Abanda
AU  - A, Akintola
AU  - V.V, Tuhaise
AU  - J.H.M, Tah
JO  - Journal of Building Engineering
VL  - 107
SP  - 112762
PY  - 2025
DA  - 2025/08/01/
SN  - 2352-7102
DO  - https://doi.org/10.1016/j.jobe.2025.112762
UR  - https://www.sciencedirect.com/science/article/pii/S2352710225009994
KW  - BIM
KW  - BIM execution plan
KW  - Information management
KW  - ISO 19650
KW  - Ontology
AB  - The adoption of Building Information Modelling (BIM) in the construction industry has been hindered by numerous barriers, notably the limited understanding of its concepts, protocols, and the intricate interplay between processes, people, and technologies. To address these challenges, a range of standards and guidelines have been developed, most notably the ISO 19650 series, which offer a comprehensive framework for implementing various aspects of BIM in construction projects. However, despite the BIM's collaborative philosophy, the standards and specifications that guide its adoption and implementation seldom reveal and explain the relationships between their key elements and concepts. This lack of clarity limits understanding and undermines the very essence of collaboration that BIM seeks to promote in construction projects. The text-based nature of the standards and specifications makes it difficult to identify common concepts that cut across the different project phases, their relationships, and interdependencies. This study proposes a BIM ontology for information management (BIM-OIM) that makes BIM process data more available and easily useable, allowing other researchers and practitioners to implement, and extend its use within their domains of practice. To achieve the practice-driven goal of BIM-OIM, Yet Another Methodology for Ontology (YAMO), one of the leading ontology engineering methodologies, was used to develop BIM-OIM. BIM-OIM is a formal and structured representation of ISO 19650 knowledge that is machine-processable. This representation enhances understanding, promotes reusability, and supports practical applications throughout the information management lifecycle. Key applications include the development of BIM Execution Plans, compliance checking for information containers, and identifying the roles of various stakeholders within a project.
ER  - 

TY  - JOUR
T1  - Balancing privacy and usability: A design science research approach for cookie consent mechanisms
AU  - Abdallah, Ammar
AU  - Ahmad, Ala'eddin
AU  - Said, Belal
JO  - Journal of Open Innovation: Technology, Market, and Complexity
VL  - 11
IS  - 2
SP  - 100520
PY  - 2025
DA  - 2025/06/01/
SN  - 2199-8531
DO  - https://doi.org/10.1016/j.joitmc.2025.100520
UR  - https://www.sciencedirect.com/science/article/pii/S2199853125000551
KW  - Website cookie consent
KW  - General Data Protection Regulation (GDPR)
KW  - Protection Motivation Theory (PMT)
KW  - Technology Acceptance Model (TAM)
KW  - Design Science Research (DSR)
AB  - Digital analytics literature has received increasing research attention regarding website cookies owing to the implementation of the General Data Protection Regulation (GDPR) and its role in online privacy decisions and data collection. However, there is still a gap in the literature about users’ perspectives and beliefs about the intention to accept (opt-in) to the website cookie consent. Therefore, this study focuses on measuring user behavior toward website cookie consent mechanisms by integrating the Technology Acceptance Model (TAM), Protection Motivation Theory (PMT), and Design Science Research (DSR). Through a survey of 384 participants aware of website cookie consent in Jordan, this study examined the influence of factors such as perceived usefulness, ease of use, and threat and coping appraisals on accepting data collection through website cookies while browsing the Internet. The findings highlight the higher influence of perceived ease of use on driving consent, highlighting the need for consent that is user-friendly while protecting data privacy. By integrating DSR, this study incorporates open innovation principles by proposing innovative design requirements for website cookie consents. In addition, it offers valuable insights for both academics and practitioners by proposing a starting point for further investigation of technology acceptance, online privacy decisions, and data collection privacy regulations.
ER  - 

TY  - JOUR
T1  - A knowledge graph-driven framework of multi-stakeholder synergistic operation and maintenance for complex products: design, implementation and industrial validation
AU  - Zhao, Xin
AU  - Wang, Ruixuan
AU  - Ren, Shan
AU  - Zhang, Geng
AU  - Zhang, Yingfeng
JO  - Advanced Engineering Informatics
VL  - 68
SP  - 103746
PY  - 2025
DA  - 2025/11/01/
SN  - 1474-0346
DO  - https://doi.org/10.1016/j.aei.2025.103746
UR  - https://www.sciencedirect.com/science/article/pii/S1474034625006391
KW  - Complex products
KW  - Knowledge graph
KW  - Knowledge navigation
KW  - Synergistic operation and maintenance
AB  - To ensure the optimal-state operation, low maintenance costs and optimized resource allocation, synergistic operation and maintenance (OM) mode of complex products (CPs) is becoming a current research hotspot. However, multiple stakeholders (e.g. user, manufacturer and service provider) involved in the synergistic OM of CPs are facing challenges in integrating heterogeneous data, resolving ambiguities in service requirements and generating explainable maintenance strategies due to inconsistent data semantics and fragmented knowledge divide. Moreover, although the promising application of knowledge graph (KG) to handle large-scale data semantically and resulting in better knowledge sharing, the research simultaneously combing multiple stakeholder synergistic OM and KG is in its infancy. To address these challenges and problems, a KG-driven framework of multi-stakeholder synergistic OM for CPs is developed is this paper. Then, a formal ontology modelling method for consistent representation of multi-stakeholder heterogeneous data and a Naive Bayes-enabled knowledge navigation model for reducing the ambiguities of multi-stakeholder OM information are proposed to provide technical support for effective implementation of the framework. Finally, an industrial application scenario of an electric multiple units (EMU) trailer bogie OM is presented to validate the feasibility and effectiveness of the framework. The result shows that the developed framework achieves 87.8% accuracy in OM knowledge classification and retrieval (e.g., fault cause identification and maintenance strategy recommendation). Therefore, it can be used to effectively facilitate the heterogeneous data using and implement the multi-source knowledge sharing among different stakeholders for generating explainable and actionable synergistic OM strategies for unfamiliar maintenance tasks.
ER  - 

TY  - JOUR
T1  - So much more than test cases – An industrial study on testing of software units and components
AU  - Mårtensson, Torvald
JO  - Journal of Systems and Software
VL  - 228
SP  - 112479
PY  - 2025
DA  - 2025/10/01/
SN  - 0164-1212
DO  - https://doi.org/10.1016/j.jss.2025.112479
UR  - https://www.sciencedirect.com/science/article/pii/S0164121225001475
KW  - Industrial study
KW  - In practice
KW  - Software testing
KW  - Software component
KW  - Software unit
KW  - Test tools
KW  - Test methods
AB  - While software-developing organizations increasingly write test cases and continuously run them to find defects and ensure quality, the associated maintenance and execution costs increase over time. According to practitioners in the software industry, more test cases is not the solution, which calls for other methods and tools that can complement traditional test cases. Based on a cross-company workshop with 30 participants, three focus groups with 19 participants, and a validation workshop with 30 participants, this paper presents insights from practitioners at six large-scale companies, describing experiences from real industrial settings with regards to testing of software units and components. As every tool is also a cost for the organization, this paper presents a structured and holistic approach based on grounded theory, where methods and tools are evaluated and selected from four categories, representing test activities with different purposes: Optimize feedback loops, Improve test efficiency, Improve test suite effectiveness, and Technical debt management. The participants in the study also described how methods and tools used within their companies were often not well described in the company’s ways-of-working, which caused frustration and confusion. Feedback from the continuous integration and delivery pipeline must be understandable (describing what the problem is), actionable (describing what the developer needs to do), and explained (describing and motivating why this is important). The findings from this study provide actionable strategies for how testing of units and components can be conducted in a more structured way, which could help companies cut costs, save time and identify problems earlier.
ER  - 

TY  - JOUR
T1  - Expert system for extracting keywords in educational texts and textbooks based on transformers models
AU  - Rico, Irene Cid
AU  - Espada, Jordán Pascual
JO  - Expert Systems with Applications
VL  - 282
SP  - 127735
PY  - 2025
DA  - 2025/07/05/
SN  - 0957-4174
DO  - https://doi.org/10.1016/j.eswa.2025.127735
UR  - https://www.sciencedirect.com/science/article/pii/S0957417425013570
KW  - BERT
KW  - Keywords
KW  - Key terms
KW  - Keyphrase
KW  - Textbook dataset
KW  - Educational texts
KW  - Pretrained models
KW  - Transformers
KW  - Token classification task
AB  - Automated keyword extraction is widely used for tasks like classification and summarization, but generic methods often fail to address domain-specific requirements. In education, texts are designed to help students grasp and retain key concepts needed for exercises and resolve questions. Despite the variety of existing keyword extraction algorithms, none are specifically adapted to the unique structure and purpose of educational materials like textbooks or lecture notes.Supervised methods have demonstrated their effectiveness in various domains through advanced techniques like contextual embeddings and domain-specific fine-tuning, Our study proposes a novel solution leveraging pretrained transformer models, specifically BERT, to adapt to the structure of educational materials for effective keyword extraction. Our research demonstrates that by fine-tuning BERT models to the specific characteristics of educational texts, we can achieve more accurate and relevant keyword extraction. YodkW, our adapted model, outperforms traditional algorithms in identifying the key concepts that are essential for educational purposes. Performance is quantified using the F1 score relative to text books key terms list, Preliminary results demonstrate that our approach can improve the identification of key concepts pertinent to student understanding and facilitate the automatic generation of test questions.
ER  - 

TY  - JOUR
T1  - Fortifying NLP models against poisoning attacks: The power of personalized prediction architectures
AU  - Ferdinan, Teddy
AU  - Kocoń, Jan
JO  - Information Fusion
VL  - 114
SP  - 102692
PY  - 2025
DA  - 2025/02/01/
SN  - 1566-2535
DO  - https://doi.org/10.1016/j.inffus.2024.102692
UR  - https://www.sciencedirect.com/science/article/pii/S1566253524004706
KW  - Natural language processing
KW  - Poisoning attack
KW  - Adversarial machine learning
KW  - Personalization
KW  - Text classification
AB  - In Natural Language Processing (NLP), state-of-the-art machine learning models heavily depend on vast amounts of training data. Often, this data is sourced from third parties, such as crowdsourcing platforms, to enable swift and efficient annotation collection for supervised learning. Yet, such an approach is susceptible to poisoning attacks where malicious agents deliberately insert harmful data to skew the resulting model behavior. Current countermeasures to these attacks either come at a significant cost, lack full efficacy, or are simply non-applicable. This study introduces and evaluates the potential of personalized model architectures as a defense against these threats. By comparing two top-performing personalized model architectures, User-ID and HuBi-Medium, against a standard non-personalized baseline across two NLP tasks and various simulated attack scenarios, we found that the personalized model architectures significantly outperformed the baseline. The robustness advantage increased with the rise in malicious annotations. Notably, the User-ID model excelled in safeguarding predictions for legitimate users from the influence of malicious annotations. Our findings emphasize the benefit of adopting personalized model architectures to bolster NLP system defenses against poisoning attacks.
ER  - 

TY  - JOUR
T1  - Generative AI: The transformative impact of ChatGPT on systemic financial risk in Chinese banks
AU  - Zhao, Yikai
AU  - Dai, Runyu
AU  - Nagayasu, Jun
JO  - Pacific-Basin Finance Journal
VL  - 93
SP  - 102829
PY  - 2025
DA  - 2025/10/01/
SN  - 0927-538X
DO  - https://doi.org/10.1016/j.pacfin.2025.102829
UR  - https://www.sciencedirect.com/science/article/pii/S0927538X25001660
KW  - AI
KW  - ChatGPT
KW  - Systemic financial risk
KW  - Chinese banks
AB  - We investigate the impact of ChatGPT, a generative artificial intelligence (GenAI) application, on the systemic financial risk of Chinese banks. Using a sample of 42 publicly traded banks and employing regression discontinuity (RD) and regression discontinuity difference-in-differences (RD-DID) methodologies, we assess the immediate effects following the launch of ChatGPT on November 30, 2022. Our findings reveal an immediate and significant increase in systemic financial risk, measured by ΔCoVaR. Robustness checks, including placebo tests, alternative risk measures, and varying sample windows, confirm the reliability of these results. Mechanism analysis highlights that transitional challenges during GenAI adoption exacerbate systemic vulnerabilities. Smaller banks, rural commercial banks, and banks with higher nonperforming loan ratios (NPL) face heightened risks, while large state-owned banks remain relatively insulated. These findings underscore the double-edged nature of disruptive innovations such that GenAI integration poses short-term risks to financial stability even if GenAI has transformative potential.
ER  - 

TY  - JOUR
T1  - An architecture for model-based and intelligent automation in DevOps
AU  - Eramo, Romina
AU  - Said, Bilal
AU  - Oriol, Marc
AU  - Bruneliere, Hugo
AU  - Morales, Sergio
JO  - Journal of Systems and Software
VL  - 217
SP  - 112180
PY  - 2024
DA  - 2024/11/01/
SN  - 0164-1212
DO  - https://doi.org/10.1016/j.jss.2024.112180
UR  - https://www.sciencedirect.com/science/article/pii/S0164121224002255
KW  - Software architecture
KW  - DevOps
KW  - Continuous software engineering
KW  - Artificial Intelligence
KW  - Mode-driven engineering
AB  - The increasing complexity of modern systems poses numerous challenges at all stages of system development and operation. Continuous software and system engineering processes, e.g., DevOps, are increasingly adopted and spread across organizations. In parallel, many leading companies have begun to apply artificial intelligence (AI) principles and techniques, including Machine Learning (ML), to improve their products. However, there is no holistic approach that can support and enhance the growing challenges of DevOps. In this paper, we propose a software architecture that provides the foundations of a model-based framework for the development of AI-augmented solutions incorporating methods and tools for continuous software and system engineering and validation. The key characteristic of the proposed architecture is that it allows leveraging the advantages of both AI/ML and Model Driven Engineering (MDE) approaches and techniques in a DevOps context. This architecture has been designed, developed and applied in the context of the European large collaborative project named AIDOaRt. In this paper, we also report on the practical evaluation of this architecture. This evaluation is based on a significant set of technical solutions implemented and applied in the context of different real industrial case studies coming from the AIDOaRt project. Moreover, we analyze the collected results and discuss them according to both architectural and technical challenges we intend to tackle with the proposed architecture.
ER  - 

TY  - JOUR
T1  - Detecting Hate Speech for Hindi-English Code-Mix Text Data Using Dual Contrastive Learning
AU  - Sharma, Amit
AU  - Bhalla, Rajni
JO  - Procedia Computer Science
VL  - 259
SP  - 35
EP  - 43
PY  - 2025
DA  - 2025/01/01/
T2  - Sixth International Conference on Futuristic Trends in Networks and Computing Technologies (FTNCT06), held in Uttarakhand, India
SN  - 1877-0509
DO  - https://doi.org/10.1016/j.procs.2025.03.304
UR  - https://www.sciencedirect.com/science/article/pii/S1877050925010488
KW  - Hate speech Detection
KW  - NLP
KW  - Machine Learning
KW  - Sentiment Analysis
KW  - code-mixed data
KW  - Dual Contrastive Learning
AB  - A bigger problem in today’s world is hate speech on internet, especially as social media sites like Facebook and Twitter rapidly grow. It includes any speech or writing that makes fun of or calls for violence against people or groups because of things that make them unique, like race, religion, gender, or sexual orientation. This paper looks at how to find hate speech in Hindi-English Code-mixed text data using both Traditional machine learning (ML) models and Dual Contrastive Learning (DCL) method. Word embeddings, TF-IDF, and pre-trained SentBERT techniques were used along with other methods to prepare and examine the data. Traditional ML models like KNN, DT, and LR are tested in the study. These models get accuracy rates of 73.63%, 79.78%, and 85.50%, respectively. The proposed DCL model does better than these, as its accuracy rate is 86% and its F1-Score is 91%. The results show that pre-trained SentBERT could help better as extracting the feature in code-mixed data. The accuracy, on the other hand, shows that there is room for improvement. Adding more advanced deep learning and natural language processing methods could make the model even better.
ER  - 

TY  - JOUR
T1  - Product verification using OCR classification and Mondrian conformal prediction
AU  - Oucheikh, Rachid
AU  - Pettersson, Tobias
AU  - Löfström, Tuwe
JO  - Expert Systems with Applications
VL  - 188
SP  - 115942
PY  - 2022
DA  - 2022/02/01/
SN  - 0957-4174
DO  - https://doi.org/10.1016/j.eswa.2021.115942
UR  - https://www.sciencedirect.com/science/article/pii/S0957417421012963
KW  - OCR classification
KW  - Retail product verification
KW  - Mondrian conformal prediction
KW  - Smart self-checkout system
AB  - The retail sector is undergoing an apparent digital transformation that completely revolutionises shopping operations. To stay competitive, retailer stakeholders are forced to rethink and improve their business models to provide an attractive personalised experience to consumers. The self-service checkout process is at the heart of this transformation and should be designed to identify the products accurately and detect any possible anomalous behaviour. In this paper, we introduce a product verification system based on OCR classification and Mondrian conformal prediction. The proposed system includes three components: OCR reading, text classification and product verification. By using image data from existing grocery stores, the system can detect anomalies with high performance, even when there is partial text information on the products. This makes the system applicable for reducing shrinkage loss (caused, for example, by employee theft or shoplifting) in grocery stores by identifying fraudulent behaviours such as barcode switching and miss-scan. Additionally, OCR reading with NLP classification shows that it is in itself a powerful classifier of products.
ER  - 

TY  - JOUR
T1  - Can you fool AI by doing a 180? — A case study on authorship analysis of texts by Arata Osada
AU  - Nieuwazny, Jagna
AU  - Nowakowski, Karol
AU  - Ptaszynski, Michal
AU  - Masui, Fumito
JO  - Information Processing & Management
VL  - 58
IS  - 5
SP  - 102644
PY  - 2021
DA  - 2021/09/01/
SN  - 0306-4573
DO  - https://doi.org/10.1016/j.ipm.2021.102644
UR  - https://www.sciencedirect.com/science/article/pii/S0306457321001345
KW  - Authorship analysis
KW  - Single authorship identification
KW  - Authorship verification
KW  - Similarity detection
KW  - Binary text classification
KW  - Transformers
KW  - Personal ethics
AB  - This paper is our attempt at answering a twofold question covering the areas of ethics and authorship analysis solutions. Firstly, since the methods used for performing authorship analysis imply that an author can be recognized by the content he or she creates, we were interested in finding out whether it would be possible for an author identification system to correctly attribute works to authors if in the course of years they have undergone a major psychological transition. Secondly – and from the point of view of the evolution of an author’s ethical values – we checked what it would mean if the authorship attribution system encounters difficulties in detecting single authorship. We set out to answer those questions through performing a binary authorship analysis task using a text classifier based on a pre-trained transformer model and a baseline method relying on conventional similarity metrics. For the test set, we chose several works of Arata Osada, a Japanese educator and specialist in the history of education, with half of them being books written before the Second World War and another half in the 1950s, in between which the author underwent a transformation in terms of political opinions. As a result, we were able to confirm that in the case of texts authored by Arata Osada in a time span of more than 10 years, while the classification accuracy drops by a large margin and is substantially lower than for texts by other non-fiction writers, confidence scores of the predictions remain at a similar level as in the case of a shorter time span, indicating that the classifier was in many instances tricked into deciding that texts written by Arata Osada over a time span of multiple years were actually written by two different people, which in turn leads us to believe that such a change can affect authorship analysis, and that historical events have great impact on a person’s ethical outlook as expressed in their writings.
ER  - 

TY  - JOUR
T1  - Integration of data science with product design towards data-driven design
AU  - Liu, Ang
AU  - Lu, Stephen
AU  - Tao, Fei
AU  - Anwer, Nabil
JO  - CIRP Annals
VL  - 73
IS  - 2
SP  - 509
EP  - 532
PY  - 2024
DA  - 2024/01/01/
SN  - 0007-8506
DO  - https://doi.org/10.1016/j.cirp.2024.06.003
UR  - https://www.sciencedirect.com/science/article/pii/S0007850624001252
KW  - Product design
KW  - Data science
KW  - Data-driven design
AB  - This paper aims to investigate the scientific integration of data science with product design towards data-driven design (D3). Data science has potential to facilitate design decision-making through insight extraction, predictive analytics, and automatic decisions. A systematic scoping review is conduced to converge various D3 applications in four dimensions: the design dimension about design operations, the data dimension about popular data sources and common data-related challenges, the method dimension about the methodological foundations, and the social/ethical dimension about social/ethical considerations and implications. Based on the state-of-the-art, this paper also highlights potential future research avenues in this dynamic field.
ER  - 

TY  - JOUR
T1  - Better alone than in bad company: Addressing the risks of companion chatbots through data protection by design
AU  - Dewitte, Pierre
JO  - Computer Law & Security Review
VL  - 54
SP  - 106019
PY  - 2024
DA  - 2024/09/01/
SN  - 2212-473X
DO  - https://doi.org/10.1016/j.clsr.2024.106019
UR  - https://www.sciencedirect.com/science/article/pii/S0267364924000852
KW  - Privacy
KW  - Data protection
KW  - GDPR
KW  - AI Act
KW  - Data protection by design
KW  - Data protection impact assessments
KW  - Companion chatbots
KW  - Enforcement
AB  - Recent years have seen a surge in the development and use of companion chatbots, conversational agents specifically designed to act as virtual friends, romantic partners, life coaches or even therapists. Yet, these tools raise many concerns, especially when their target audience is comprised of vulnerable individuals. While the recently adopted AI Act is expected to address some of these concerns, both compliance and enforcement are bound to take time. Since the development of companion chatbots involves the processing of personal data at nearly every step of the process, from training to fine-tuning to deployment, this paper argues that the General Data Protection Regulation (“GDPR”), and data protection by design more specifically, already provides a solid ground for regulators and courts to force controllers to mitigate these risks. In doing so, it sheds light on the broad material scope of Articles 24(1) and 25(1) GDPR, highlights the role of these provisions as proxies to Fundamental Rights Impact Assessments (“FRIAs”), and peels off the many layers of personal data processing involved in the companion chatbots supply chain. That reasoning served as the basis for a complaint lodged with the Belgian data protection authority, the full text and supporting evidence of which are provided as supplementary materials.
ER  - 

TY  - JOUR
T1  - User-friendly and industry-integrated AI for medicinal chemists and pharmaceuticals
AU  - Kapustina, Olga
AU  - Burmakina, Polina
AU  - Gubina, Nina
AU  - Serov, Nikita
AU  - Vinogradov, Vladimir
JO  - Artificial Intelligence Chemistry
VL  - 2
IS  - 2
SP  - 100072
PY  - 2024
DA  - 2024/12/01/
SN  - 2949-7477
DO  - https://doi.org/10.1016/j.aichem.2024.100072
UR  - https://www.sciencedirect.com/science/article/pii/S2949747724000307
KW  - Machine Learning
KW  - Medicinal Chemistry
KW  - Pharmaceutics
KW  - Data-Driven Drug Discovery
AB  - Artificial intelligence has brought crucial changes to the whole field of natural sciences. Myriads of machine learning algorithms have been developed to facilitate the work of experimental scientists. Molecular property prediction and drug synthesis planning become routine tasks. Moreover, inverse design of compounds with tunable properties as well as on-the-fly autonomous process optimization and chemical space exploration became possible in silico. Affordable robotic platforms exist able to perform thousands of experiments every day, analyzing the results and tuning the protocols. Despite this, most of these developments get trapped at the stage of code or overlooked, limiting their use by experimental scientists. Meanwhile, visibility and the number of user-friendly tools and technologies available to date is too low to compensate for this fact, rendering the development of novel therapeutic compounds inefficient. In this Review, we set the goal to bridge the gap between modern technologies and experimental scientists to improve drug development efficacy. Here we survey advanced and easy-to-use technologies able to help medical chemists at every stage of their research, including those integrated in technological processes during COVID-19 pandemic motivated by the need for fast yet precise solutions. Moreover, we review how these technologies are integrated by industry and clinics to streamline drug development and production. These technologies already transform the current paradigm of scientific thinking and revolutionize not only medicinal chemistry, but the whole field of natural sciences.
ER  - 

TY  - JOUR
T1  - TitNet: A time-series model based on multi-period nesting for encrypted traffic classification
AU  - Wang, Congcong
AU  - Li, Xin
AU  - Cui, Zhaoqiang
AU  - Xu, Lina
AU  - Hou, Jiangang
AU  - Sun, Jie
AU  - Xu, Hongji
AU  - Liu, Zhi
JO  - Computer Networks
VL  - 272
SP  - 111702
PY  - 2025
DA  - 2025/11/01/
SN  - 1389-1286
DO  - https://doi.org/10.1016/j.comnet.2025.111702
UR  - https://www.sciencedirect.com/science/article/pii/S1389128625006681
KW  - Encrypted traffic classification
KW  - Multivariate time series
KW  - Multi-period nesting
KW  - TitNet model
KW  - Dynamic frequency selection strategy
AB  - Encrypted traffic classification is essential for network management tasks such as quality-of-service controls, identifying malicious traffic, and enhancing cybersecurity. However, the scarcity of plaintext information and the significant reduction of payload characteristics in encrypted traffic present challenges to effective classification. To tackle these issues, we propose a novel time series model called TitNet, which models network traffic at the session level as a multivariate time series and effectively integrates periodic and spatial features inherent in time series data. Our TitNet contains a dynamic frequency selection strategy(DFSS) that facilitates the conversion of time series data into two-dimensional tensor representations, which is pivotal for accurately discerning the intricate patterns embedded in encrypted traffic. This approach enables TitNet to iteratively transform time series into 2D tensors, effectively exploiting the multi-period nesting characteristics of the data to improve classification performance. Experimental results on the ISCXTor2016 dataset (43 Tor/NonTor categories) robustly indicate that our TitNet excels in the detection, classification, and identification of applications within encrypted traffic, achieving 96.21 % accuracy while handling extreme class imbalance. Nonetheless, TitNet introduces additional computational overhead and relies on fixed session truncation, which may limit scalability and long-range modeling. Future work will explore lightweight variants and improved sequence aggregation strategies to address these challenges.
ER  - 

TY  - JOUR
T1  - GreenBERT: A lightweight green transformer for automated prediction of software vulnerability scores
AU  - Mirtaheri, Seyedeh Leili
AU  - Tafti, Ali Kafi
AU  - Soureshjani, Hamid Heidari
AU  - Pugliese, Andrea
JO  - Array
VL  - 28
SP  - 100536
PY  - 2025
DA  - 2025/12/01/
SN  - 2590-0056
DO  - https://doi.org/10.1016/j.array.2025.100536
UR  - https://www.sciencedirect.com/science/article/pii/S2590005625001638
KW  - Green computing
KW  - Software vulnerability assessment
KW  - Deep learning
KW  - Knowledge distillation
KW  - Transformers
AB  - Timely assessment of software vulnerabilities is critical for effective patch prioritization, yet manual Common Vulnerability Scoring System (CVSS) scoring remains slow and resource-intensive. While transformer-based models such as BERT have advanced automated scoring, their substantial computational demands conflict with sustainable, green computing objectives. This paper introduces GreenBERT, a tailored ensemble of lightweight student Transformers, each specialized on individual CVSS metrics through a targeted multi-head knowledge distillation framework. By jointly optimizing alignment with ground-truth labels and softened outputs from a fine-tuned BERT teacher, GreenBERT efficiently captures complex vulnerability patterns while significantly reducing computational overhead. Extensive experiments on the National Vulnerability Database (NVD) and a more challenging COMBINED dataset demonstrate that GreenBERT achieves an average F1-score improvement exceeding 6% over the BERT baseline, while simultaneously reducing inference time by approximately 80% and cutting energy usage and CO2 emissions by about 70%. These results position GreenBERT as a robust, scalable, and environmentally conscious solution for high-performance vulnerability scoring, effectively reconciling the traditionally conflicting goals of predictive accuracy and sustainable AI.
ER  - 

TY  - JOUR
T1  - Towards human-AI collaboration in the competency-based curriculum development process: The case of industrial engineering and management education
AU  - Padovano, Antonio
AU  - Cardamone, Martina
JO  - Computers and Education: Artificial Intelligence
VL  - 7
SP  - 100256
PY  - 2024
DA  - 2024/12/01/
SN  - 2666-920X
DO  - https://doi.org/10.1016/j.caeai.2024.100256
UR  - https://www.sciencedirect.com/science/article/pii/S2666920X24000596
KW  - Engineering education
KW  - Industry 5.0
KW  - Industrial engineering and management
KW  - Curriculum development
KW  - Artificial intelligence
KW  - Competency-based education
AB  - In the endeavor to advance industrial engineering and management (IEM) education, this research underscores the imperative of supporting a dynamic and responsive adaptation of a competency-based curriculum (CBC) to meet the demands of an ever-evolving industrial landscape and job market. Our study contributes to competency-based education (CBE) by demonstrating how Artificial Intelligence (AI) can inform the definition of a CBC in the IEM field, thus initiating the pioneering steps towards a collaborative human-AI approach in CBC design. Through a stepwise methodology based on semantic analysis, text mining, natural language processing (NLP) models, informetrics approaches, and clustering algorithms, we provide data-driven insights to inform the curriculum development process. This approach enabled us to identify educational gap, particularly in domains such as digital twin engineering and human-centric IEM. Moreover, this study advocates for higher education institutions (HEIs) to embrace a more structured and collaborative approach to continuously developing competency-based curricula. In this perspective, AI (including generative AI) emerges as a valuable ally in curriculum design. This approach proves instrumental in crafting competitive and appealing curricula, especially at peripheral universities. This study culminates in an updated WING model showing how to build Industry 5.0 related curricula and a series of recommendations for engineering educators.
ER  - 

TY  - JOUR
T1  - Development model based on visual image big data applied to art management
AU  - Ju, Jiehui
AU  - Ma, Yanghui
AU  - Gong, Ting
AU  - Zhuang, Er
JO  - Heliyon
VL  - 10
IS  - 17
SP  - e37478
PY  - 2024
DA  - 2024/09/15/
SN  - 2405-8440
DO  - https://doi.org/10.1016/j.heliyon.2024.e37478
UR  - https://www.sciencedirect.com/science/article/pii/S2405844024135098
KW  - Art management
KW  - Visual image
KW  - Big data application
KW  - Management model development
AB  - This paper aims to explore the application of visual image big data (BD) in art management, and proposes and develops a new art management model. First of all, this study conducted extensive research on the overview and application of big data, focusing on analyzing the characteristics of big data and its characteristics and application methods in art management. By introducing image processing (IP) technology, this paper expounds on the application of visual image technology in art management in detail and discusses the classification of computer vision images to determine its application direction. On this basis, this paper proposes the application of visual images and big data in art management from three aspects: the accurate acquisition of visual images, the development model of art management, and the development of visual image technology in art resource management and teaching, and strengthens the development model of art management based on IP algorithm. Experiments and surveys show that the art management model development system built by the newly introduced visual image technology, big data technology, and IP algorithm can increase user satisfaction by 24 %. This result shows that the new model has a significant effect in improving the efficiency and quality of art management, providing strong technical support for the field of art management, while also providing designers with a more accurate tool for assessing market trends, helping to adhere to and promote good design concepts.
ER  - 

TY  - JOUR
T1  - SoK: Timeline based event reconstruction for digital forensics: Terminology, methodology, and current challenges
AU  - Breitinger, Frank
AU  - Studiawan, Hudan
AU  - Hargreaves, Chris
JO  - Forensic Science International: Digital Investigation
VL  - 53
SP  - 301932
PY  - 2025
DA  - 2025/07/01/
T2  - DFRWS USA 2025 - Selected Papers from the 25th Annual Digital Forensics Research Conference USA
SN  - 2666-2817
DO  - https://doi.org/10.1016/j.fsidi.2025.301932
UR  - https://www.sciencedirect.com/science/article/pii/S266628172500071X
KW  - Event reconstruction
KW  - Timeline
KW  - Digital investigation
KW  - Methodology
KW  - Artifacts
KW  - Terminology
KW  - Framework
KW  - Challenges
AB  - Event reconstruction is a technique that examiners can use to attempt to infer past activities by analyzing digital artifacts. Despite its significance, the field suffers from fragmented research, with studies often focusing narrowly on aspects like timeline creation or tampering detection. This paper addresses the lack of a unified perspective by proposing a comprehensive framework for timeline-based event reconstruction, adapted from traditional forensic science models. We begin by harmonizing existing terminology and presenting a cohesive diagram that clarifies the relationships between key elements of the reconstruction process. Through a comprehensive literature survey, we classify and organize the main challenges, extending the discussion beyond common issues like data volume. Lastly, we highlight recent advancements and propose directions for future research, including specific research gaps. By providing a structured approach, key findings, and a clearer understanding of the underlying challenges, this work aims to strengthen the foundation of digital forensics.
ER  - 

TY  - JOUR
T1  - ESFLM: Efficient and Secure Federated Learning Model with Homomorphic Encryption
AU  - Li, Yang
AU  - Xia, Chunhe
AU  - Li, Chang
AU  - Li, Xiaojian
AU  - Wang, Tianbo
JO  - Advanced Engineering Informatics
VL  - 69
SP  - 104118
PY  - 2026
DA  - 2026/01/01/
SN  - 1474-0346
DO  - https://doi.org/10.1016/j.aei.2025.104118
UR  - https://www.sciencedirect.com/science/article/pii/S1474034625010110
KW  - Federated learning
KW  - Homomorphic encryption
KW  - Secret sharing
AB  - Federated learning based on homomorphic encryption has attracted widespread attention for its strong security and enhanced protection of user data privacy. However, the nature of encrypted computation introduces three major challenges: computation efficiency, attack defense, and contribution assessment. The first concerns the efficiency of encrypted computation during model aggregation, the second involves defense malicious attacks under encryption, and the third addresses the fairness of contribution assessment for encrypted local models. This paper presents an Efficient and Secure Federated Learning Model with Homomorphic Encryption (ESFLM) to protect model privacy and tackle the aforementioned challenges. First, we leverage multiple nodes to perform parallel aggregation of local models, thereby improving the efficiency of encrypted model aggregation. Second, we introduce trusted supervise nodes to inspect local models when the global model is under attack, enabling effective defense of malicious behavior under homomorphic encryption. Finally, we fairly reward local training nodes based on their verified training time, even when local models remain encrypted. Experiments on three real-world datasets demonstrate that our model significantly outperforms baseline approaches in terms of both efficiency and security.
ER  - 

TY  - JOUR
T1  - Trust, transparency, and adoption in generative AI for software engineering: Insights from Twitter discourse
AU  - Basha, Manaal
AU  - Rodríguez-Pérez, Gema
JO  - Information and Software Technology
VL  - 186
SP  - 107804
PY  - 2025
DA  - 2025/10/01/
SN  - 0950-5849
DO  - https://doi.org/10.1016/j.infsof.2025.107804
UR  - https://www.sciencedirect.com/science/article/pii/S0950584925001430
KW  - Generative AI
KW  - Code generation tools
KW  - AI adoption
KW  - Human-AI interaction
AB  - Context:
The rise of AI-driven coding assistants, such as GitHub Copilot and ChatGPT, are transforming software development practices. Despite their growing impact, informal user feedback on these tools is often neglected.
Objective:
This study aims to analyze Twitter/X conversations to understand user opinions on the benefits, challenges, and barriers associated with Code Generation Tools (CGTs) in software engineering. By incorporating diverse perspectives from developers, hobbyists, students, and critics, this research provides a comprehensive view of public sentiment.
Methods:
We employed a hybrid approach using BERTopic and open coding to collect and analyze data from approximately 90,000 tweets. The focus was on identifying themes and sentiments related to various CGTs. The study sought to determine the most frequently discussed topics and their related sentiment, followed by highlighting the reoccurring feedback or criticisms that could influence generative AI (GenAI) adoption in software engineering.
Results:
Our analysis identified several significant themes, including productivity enhancements, shifts in developer practices, regulatory uncertainty, and a demand for neutral GenAI content. While some users praised the efficiency benefits of CGTs, others raised concerns regarding intellectual property, transparency, and potential biases.
Conclusion:
The findings highlight that addressing issues of trust, accountability, and legal clarity is essential for the successful integration of CGTs in software development. These insights underscore the need for ongoing dialogue and refinement of CGTs to better align with user expectations and mitigate concerns.
ER  - 

TY  - JOUR
T1  - A Novel CAPTCHA Recognition System Based on Refined Visual Attention
AU  - Derea, Zaid
AU  - Zou, Beiji
AU  - Kui, Xiaoyan
AU  - Abdullah, Monir
AU  - Thobhani, Alaa
AU  - Abdussalam, Amr
JO  - Computers, Materials and Continua
VL  - 83
IS  - 1
SP  - 115
EP  - 136
PY  - 2025
DA  - 2025/03/26/
SN  - 1546-2218
DO  - https://doi.org/10.32604/cmc.2025.062729
UR  - https://www.sciencedirect.com/science/article/pii/S1546221825002802
KW  - Text-based CAPTCHA recognition
KW  - refined visual attention
KW  - web security
KW  - computer vision
AB  - Improving website security to prevent malicious online activities is crucial, and CAPTCHA (Completely Automated Public Turing test to tell Computers and Humans Apart) has emerged as a key strategy for distinguishing human users from automated bots. Text-based CAPTCHAs, designed to be easily decipherable by humans yet challenging for machines, are a common form of this verification. However, advancements in deep learning have facilitated the creation of models adept at recognizing these text-based CAPTCHAs with surprising efficiency. In our comprehensive investigation into CAPTCHA recognition, we have tailored the renowned UpDown image captioning model specifically for this purpose. Our approach innovatively combines an encoder to extract both global and local features, significantly boosting the model’s capability to identify complex details within CAPTCHA images. For the decoding phase, we have adopted a refined attention mechanism, integrating enhanced visual attention with dual layers of Long Short-Term Memory (LSTM) networks to elevate CAPTCHA recognition accuracy. Our rigorous testing across four varied datasets, including those from Weibo, BoC, Gregwar, and Captcha 0.3, demonstrates the versatility and effectiveness of our method. The results not only highlight the efficiency of our approach but also offer profound insights into its applicability across different CAPTCHA types, contributing to a deeper understanding of CAPTCHA recognition technology.
ER  - 

TY  - JOUR
T1  - Reinforcement learning based deep fuzzy hierarchical clustering to generate personalized non-fungible token artwork
AU  - Daliri, Arman
AU  - Mahdavi, Nora
AU  - Zabihimayvan, Mahdieh
AU  - Norouzi Baranghar, Aynaz
AU  - Zaeimzadeh, Nima
AU  - mohammadzadeh, Javad
JO  - Neurocomputing
VL  - 659
SP  - 131821
PY  - 2026
DA  - 2026/01/01/
SN  - 0925-2312
DO  - https://doi.org/10.1016/j.neucom.2025.131821
UR  - https://www.sciencedirect.com/science/article/pii/S0925231225024932
KW  - Digital Arts
KW  - Non-Fungible Token Artwork
KW  - Fuzzy
KW  - Hierarchical
KW  - Deep learning
KW  - Clustering
AB  - In the realm of information technology and software development, digital assets increasingly manifest as Non-Fungible Tokens (NFTs) on blockchain platforms, embodying intrinsic material value that enhances user satisfaction. Despite their potential, the creation of NFTs remains costly, time-consuming, and labor-intensive. To address these challenges, we introduce a digital asset generation engine specifically designed for producing NFT artwork. Utilizing real-world datasets curated by digital art experts, our engine synthesizes individual image layers into cohesive, high-quality artistic outputs. Leveraging artificial intelligence, we employ a novel Deep Fuzzy Hierarchical Clustering approach, which integrates autoencoder neural networks, fuzzy clustering, and hierarchical clustering methods. This integrated approach enables precise classification of image layers, achieving an impressive accuracy rate of 95 %. Here, we demonstrate the potential of AI-enhanced solutions in the digital art and NFT space. Our engine not only reduces costs and labor intensity in digital art production but also allows users to personalize their NFT collections by selecting desired layers and specifying rarity, arrangement order, and metadata details. This study underscores the significance of intersectional research between artificial intelligence and fine arts, opening avenues for future advancements in computational art analysis and creative AI applications.
ER  - 

TY  - JOUR
T1  - Conceptual modeling: Foundations, a historical perspective, and a vision for the future
AU  - Mylopoulos, John
AU  - Guizzardi, Giancarlo
AU  - Guarino, Nicola
JO  - Data & Knowledge Engineering
VL  - 160
SP  - 102483
PY  - 2025
DA  - 2025/11/01/
SN  - 0169-023X
DO  - https://doi.org/10.1016/j.datak.2025.102483
UR  - https://www.sciencedirect.com/science/article/pii/S0169023X25000783
KW  - Conceptual models
KW  - Conceptual modeling
KW  - Knowledge representation
KW  - Ontologies
KW  - Philosophical foundations
AB  - We recount the foundations of Conceptual Modeling in Computer Science, Philosophy and Cognitive Science and their implications on what are concepts, conceptualizations, and conceptual models. We then review the history of the field, considering earlier work by the three co-authors, and highlight some of the contributions that made it what it is. Finally, we propose three research directions whose solutions could advance the field and will hopefully be addressed in the future. Our study is intended to help to circumscribe and characterize the field. It draws ideas from Philosophy, Cognitive Science, Engineering and the Social Sciences, as well as several areas within Computer Science, including Programming languages, Artificial Intelligence, Databases, Software Engineering, and Information Systems Engineering.
ER  - 

TY  - JOUR
T1  - Unpacking digital transformation – Constructing a framework based on industry use cases
AU  - Saeed, Khawaja Asjad
AU  - Green, Andrew William
AU  - Hedrick, Alison Brooke
JO  - Journal of Innovation & Knowledge
VL  - 10
IS  - 5
SP  - 100759
PY  - 2025
DA  - 2025/09/01/
SN  - 2444-569X
DO  - https://doi.org/10.1016/j.jik.2025.100759
UR  - https://www.sciencedirect.com/science/article/pii/S2444569X25001040
KW  - Digital transformation
KW  - Implementation
KW  - Use cases
KW  - Transformation initiatives
KW  - Competitive advantage
AB  - Based on industry use cases, we identify six distinct types of digital transformation (DT), each grounded in a specific concept or enabling technology. These DT types can be implemented individually or combined to drive transformation initiatives. While deploying a single DT type often focuses on improving operational efficiency or streamlining existing workflows, initiatives that combine multiple DT types tend to pursue more ambitious goals, such as the creation of new products, services, or business models. However, the complexity increases significantly with the integration of multiple DT types, as organizations must not only address behavioral changes but also overcome technical challenges involving systems integration, data architecture, and interoperability.
ER  - 

TY  - JOUR
T1  - Development and testing of an IoT platform with smart algorithms for building energy management systems
AU  - Islam, Fayzul
AU  - Ahmed, Ibrahim
AU  - Mihet-Popa, Lucian
JO  - Energy and Buildings
VL  - 344
SP  - 115970
PY  - 2025
DA  - 2025/10/01/
SN  - 0378-7788
DO  - https://doi.org/10.1016/j.enbuild.2025.115970
UR  - https://www.sciencedirect.com/science/article/pii/S0378778825007005
KW  - Building energy management systems
KW  - Internet of things
KW  - Reinforcement learning
KW  - Optimization
KW  - Machine learning
AB  - Buildings are a major cause of carbon emissions. The building sector is responsible for around 40 % of energy consumption and for about 30 % of CO2 emissions. Building Energy Management Systems (BEMS) are crucial for enhancing energy efficiency, and energy flexibility, and mitigating the environmental impact of buildings, which account for a significant amount of global energy use and greenhouse gas emissions. This study addresses the limitations of traditional BEMS by proposing a cloud-based IoT-BEMS with an intuitive user interface and advanced machine learning algorithms for energy optimization. The system integrates demand-side management techniques, including two main principles, load shifting through demand response and energy efficiency, allowing users to control appliances without requiring technical expertise. The results demonstrate significant energy savings, particularly in water heater optimization, with an average reduction of 24.23 % in energy consumption. Additionally, the Proximal Policy Optimization (PPO) algorithm used for electric vehicle (EV) charging resulted in an average cost saving of 30.6 % by leveraging off-peak electricity rates. The platform’s real-time data processing and user-friendly interface make it a robust solution for residential energy management, effectively balancing energy savings with user comfort. This research underscores the potential of IoT and machine learning in revolutionizing building energy management, contributing to the Sustainable Development Goals.
ER  - 

TY  - JOUR
T1  - A graph neural network approach to detect original review spammers of astroturfing campaigns
AU  - Chen, Kuanchin
AU  - Cheng, Li-Chen
AU  - Ye, Mei-Yun
AU  - Wang, Jenq-Haur
JO  - Electronic Commerce Research and Applications
VL  - 62
SP  - 101326
PY  - 2023
DA  - 2023/11/01/
SN  - 1567-4223
DO  - https://doi.org/10.1016/j.elerap.2023.101326
UR  - https://www.sciencedirect.com/science/article/pii/S1567422323000911
KW  - Astroturfing
KW  - Review spammers
KW  - Fake reviews
KW  - Collusive opinion fraud
KW  - Graph neural network
KW  - Collusive spamming
KW  - Disinformation
AB  - Online reviews affect consumers’ buying decisions. When astroturfing happens, the posted fake reviews not only cause confusion to the consumers, but in extreme cases, harm the reputation of others. Identifying the original posters of fake review in a discussion forum from the interactions with the peers in a collusive effort is essential to an overall detection strategy. However, existing detection approaches that focus on the mechanic characteristics of the posted reviews may fall short, since fake reviews cause more harm when astroturfing campaign happens involving multiple accomplices. We propose a detection framework with graph neural network, which incorporates the original perpetrator’s stylometric patterns and relationships with other accomplices. The framework was tested against the data collected from real incidents. Multiple deep learning models with fusion techniques were tested. Managerial and theoretical implications are provided.
ER  - 

TY  - JOUR
T1  - SwiftR: Cross-platform ransomware fingerprinting using hierarchical neural networks on hybrid features
AU  - Karbab, ElMouatez Billah
AU  - Debbabi, Mourad
AU  - Derhab, Abdelouahid
JO  - Expert Systems with Applications
VL  - 225
SP  - 120017
PY  - 2023
DA  - 2023/09/01/
SN  - 0957-4174
DO  - https://doi.org/10.1016/j.eswa.2023.120017
UR  - https://www.sciencedirect.com/science/article/pii/S0957417423005195
KW  - Ransomware
KW  - Cross-platform
KW  - Fingerprinting
KW  - Detection
KW  - Attribution
KW  - Neural network
AB  - Ransomware has been largely exploited by cybercriminals to target individuals and organizations. In response to the increasing number and magnitude of ransomware attacks, it is important to consider the following problems when designing a ransomware fingerprinting solution: (i) how to make the solution portable to different hardware platforms and different dynamic analysis reports, (ii) how to design a solution that considers real-world use-cases, and (iii) how to evaluate the solution under realistic and challenging evaluation scenarios. To deal with these problems, we propose SwiftR, a novel portable framework for cross-platform ransomware detection and fingerprinting. SwiftR provides an accurate ransomware detection capability that relies on raw hybrid features along with advanced deep learning techniques. SwiftR is cross-platform as it is agnostic to architectures and operating systems by leveraging two novel types of features: (1) the assembly code Intermediate Representation (IR) features that are derived from static analysis, and (2) word-based features that are derived from the behavioral analysis reports, which are produced during dynamic analysis. SwiftR is supervised, and consists of two novel components: (a) Static SwiftR that proposes a novel architecture, called Hierarchical Neural Network (HNN), and (b) Dynamic SwiftR that applies LSTM on word embedding sequences when the Static SwiftR provides a low probability confidence. SwiftR aims to address the limitations of previous works by considering real-world use cases and challenging evaluation scenarios, i.e., time-resiliency, unknown family resiliency, and production evaluation scenarios. In addition, we extensively evaluate SwiftR on a dataset of 40.3K samples, which is the largest one compared to previous works. An F1-score of 98%, 96%, and 94% is achieved for ransomware detection, segregation between ransomware and other malware, and ransomware family attribution respectively. Furthermore, SwiftR maintains its high performance when deployed in a production environment where it processes 183K samples.
ER  - 

TY  - JOUR
T1  - Production Planning and Control in Industry 4.0: Overview on Challenges of Data-Driven PPC Systems
AU  - Mäule, Johannes
AU  - Götte, Gesa
JO  - IFAC-PapersOnLine
VL  - 59
IS  - 10
SP  - 1253
EP  - 1258
PY  - 2025
DA  - 2025/01/01/
T2  - 11th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2025
SN  - 2405-8963
DO  - https://doi.org/10.1016/j.ifacol.2025.09.211
UR  - https://www.sciencedirect.com/science/article/pii/S2405896325009723
KW  - Production Planning
KW  - Control
KW  - Industry 4.0
KW  - Artificial Intelligence
AB  - The integration of Industry 4.0 technologies offers significant potential to transform production planning and control (PPC) systems, but organizations face various challenges in adopting data-driven solutions. This paper examines 51 review articles published between 2019 and February 2025 and identifies 30 different challenges categorized into entry barriers, data-related issues, complexity of PPC tasks, and operational constraints. Overcoming these challenges will require not only technical solutions, but also holistic approaches, including interdisciplinary research. Future work should involve practitioners to evaluate the relevance of these challenges, as their importance has not been assessed from an industry perspective.
ER  - 

TY  - JOUR
T1  - A hierarchical indoor spatial-semantic reasoning-based scene graph construction for elderly-centric safety warnings
AU  - Jiang, Jianwu
AU  - Zhou, Jiawei
AU  - Yan, Yushi
AU  - Wang, Yuefeng
AU  - Wang, Lina
AU  - Li, Jingwen
JO  - Results in Engineering
VL  - 26
SP  - 105397
PY  - 2025
DA  - 2025/06/01/
SN  - 2590-1230
DO  - https://doi.org/10.1016/j.rineng.2025.105397
UR  - https://www.sciencedirect.com/science/article/pii/S2590123025014677
KW  - Elderly activities
KW  - Indoor scene perception
KW  - Object detection
KW  - Scene graph generation
KW  - Graph Embedding
AB  - Indoor safety is crucial for the health and well-being of elderly individuals. However, traditional safety warning systems are hindered by high misdetection rates and delayed alerts, primarily due to their limited ability to perceive multidimensional risks in complex environments and their inadequate reasoning of spatial-semantic relationships. In this study, we propose a hierarchical spatial-semantic reasoning framework for constructing indoor scene graphs, specifically designed for elderly-centric safety monitoring. Our approach utilizes an enhanced YOLOv7-MLT network for dense detection of small objects, incorporating a multi-scale spatially adaptive feature fusion (MSAFM) module and a triplet attention mechanism. These components collaboratively build scene graphs that represent objects, spatial layouts, and associated risk factors. Additionally, graph embedding techniques are employed to extract critical cognitive information, such as element stability and floor slipperiness, while hierarchical warning rules are established to facilitate dynamic risk assessment. This research presents an effective solution for proactive safety monitoring in complex indoor environments.
ER  - 
