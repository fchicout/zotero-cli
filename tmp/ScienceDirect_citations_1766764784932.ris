TY  - JOUR
T1  - XG-NID: Dual-modality network intrusion detection using a heterogeneous graph neural network and large language model
AU  - Farrukh, Yasir Ali
AU  - Wali, Syed
AU  - Khan, Irfan
AU  - Bastian, Nathaniel D.
JO  - Expert Systems with Applications
VL  - 287
SP  - 128089
PY  - 2025
DA  - 2025/08/25/
SN  - 0957-4174
DO  - https://doi.org/10.1016/j.eswa.2025.128089
UR  - https://www.sciencedirect.com/science/article/pii/S0957417425017105
KW  - Network Intrusion detection
KW  - Graph neural network
KW  - Multi-modal fusion
KW  - Large language models
KW  - Explainable AI
AB  - In the rapidly evolving field of cybersecurity, the integration of flow-level and packet-level information for real-time intrusion detection remains a largely untapped area of research. This paper introduces “XG-NID,” a novel framework that, to the best of our knowledge, is the first to fuse flow-level and packet-level data within a heterogeneous graph structure, offering a comprehensive analysis of network traffic. Leveraging a heterogeneous graph neural network (GNN) with graph-level classification, XG-NID uniquely enables real-time inference while effectively capturing the intricate relationships between flow and packet payload data. Unlike traditional GNN-based methodologies that predominantly analyze historical data, XG-NID is designed to accommodate the heterogeneous nature of network traffic, providing a robust and real-time defense mechanism. Our framework extends beyond mere classification; it integrates Large Language Models (LLMs) to generate detailed, human-readable explanations and suggest potential remedial actions, ensuring that the insights produced are both actionable and comprehensible. Additionally, we introduce a new set of flow features based on temporal information, further enhancing the contextual and explainable inferences provided by our model. To facilitate practical application and accessibility, we developed “GNN4ID,” an open-source tool that enables the extraction and transformation of raw network traffic into the proposed heterogeneous graph structure, seamlessly integrating flow and packet-level data. Our comprehensive quantitative comparative analysis demonstrates that XG-NID achieves an F1 score of 97 % in multi-class classification, outperforming existing baseline and state-of-the-art methods. This sets a new standard in Network Intrusion Detection Systems (NIDS) by combining innovative data fusion with enhanced interpretability and real-time capabilities.
ER  - 

TY  - JOUR
T1  - SpearBot: Leveraging large language models in a generative-critique framework for spear-phishing email generation
AU  - Qi, Qinglin
AU  - Luo, Yun
AU  - Xu, Yijia
AU  - Guo, Wenbo
AU  - Fang, Yong
JO  - Information Fusion
VL  - 122
SP  - 103176
PY  - 2025
DA  - 2025/10/01/
SN  - 1566-2535
DO  - https://doi.org/10.1016/j.inffus.2025.103176
UR  - https://www.sciencedirect.com/science/article/pii/S1566253525002490
KW  - Spear phishing email
KW  - Large language model
KW  - Phishing attack
AB  - Large Language Models (LLMs) are increasingly capable of aiding in tasks such as content generation, yet they also pose risks, particularly in generating harmful spear-phishing emails. These emails, crafted to entice clicks on malicious URLs, threaten personal information security. This paper proposes an adversarial framework, SpearBot, which utilizes LLMs to generate spear-phishing emails with various phishing strategies. Through specifically crafted jailbreak prompts, SpearBot circumvents security policies and introduces other LLM instances as critics. When a phishing email is identified by the critic, SpearBot refines the generated email based on the critique feedback until it can no longer be recognized as phishing, thereby enhancing its deceptive quality. To evaluate the effectiveness of SpearBot, we implement various machine-based defenders and assess how well the phishing emails generated could deceive them. Results show these emails often evade detection to a large extent, underscoring their deceptive quality. Additionally, human evaluations of the emails’ readability and deception are conducted through questionnaires, confirming their convincing nature and the significant potential harm of the generated phishing emails. Furthermore, we propose a mixup training strategy, which could significantly improve the detection of phishing emails in SpearBot.
ER  - 

TY  - JOUR
T1  - FedLLMGuard: A federated large language model for anomaly detection in 5G networks
AU  - Rezaei, Hadiseh
AU  - Taheri, Rahim
AU  - Shojafar, Mohammad
JO  - Computer Networks
VL  - 269
SP  - 111473
PY  - 2025
DA  - 2025/09/01/
SN  - 1389-1286
DO  - https://doi.org/10.1016/j.comnet.2025.111473
UR  - https://www.sciencedirect.com/science/article/pii/S1389128625004402
KW  - 5G networks
KW  - Federated learning
KW  - Large language models
KW  - Network traffic anomalies
KW  - Privacy
AB  - The rise of 5G networks has introduced new security challenges, particularly in detecting and mitigating dynamic traffic anomalies, adversarial threats, and large-scale cyberattacks. Traditional Machine learning-based Intrusion Detection Systems (IDS) suffer from high latency, poor adaptability, and privacy risks due to centralized data aggregation. To address these issues, we propose FedLLMGuard as a novel framework integrating Federated Learning (FL) with Large Language Models (LLM) for real-time privacy-preserving and adaptive anomaly detection in 5G networks. Our method uses FL for decentralized learning while utilizing LLM for contextual traffic analysis and interoperability. Moreover, our framework introduces the CorruptNet adversarial attack, a novel poisoning strategy targeting FL-based anomaly detection, ensuring robustness evaluation under adversarial conditions. We evaluate our model against three methods – Random Forest, LSTM, and PSO Autoencoder LSTM – using three benchmark datasets: TII-SSRC-23, CICDDoS2019, and NF-UNSW-NB15. Experimental results demonstrate that FedLLMGuard outperformed all models, whether subjected to the CorruptNet adversarial attack or not. Under the CorruptNet attack, it achieves an accuracy of 98.64%, a false positive rate of only 2.16%, and ultra-low detection latency (0.0113s). These results underscore FedLLMGuard’s capacity to detect threats rapidly, mitigate attacks effectively, and sustain high accuracy while ensuring data privacy, making it a scalable and resource-efficient security solution for 5G networks.
ER  - 

TY  - JOUR
T1  - LLM-APTDS: A high-precision advanced persistent threat detection system for imbalanced data based on large language models with strong interpretabilit
AU  - Yang, Longjing
AU  - Ye, Ayong
AU  - Liu, Yuanhuang
AU  - Lu, Wenting
AU  - Huang, Chuang
JO  - Future Generation Computer Systems
VL  - 178
SP  - 108315
PY  - 2026
DA  - 2026/05/01/
SN  - 0167-739X
DO  - https://doi.org/10.1016/j.future.2025.108315
UR  - https://www.sciencedirect.com/science/article/pii/S0167739X25006090
KW  - Advanced persistent threat
KW  - Large language model
KW  - Data provenance
KW  - Model fine-tuning
KW  - ATT&CK framework
KW  - Threat intelligence generation
AB  - Advanced persistent threats (APTs) pose a significant challenge to global cybersecurity, causing substantial economic losses. Existing detection methods often rely on expert-defined rules to map anomalous events to APT tactics. Still, they are highly dependent on prior knowledge, making them unsuitable for dynamic and complex attack scenarios. This results in insufficient fine-grained activity identification and attack provenance capabilities. This study proposes LLM-APTDS, an APT detection system based on large language models (LLMs). First, a multi-model collaborative detection architecture is constructed to leverage LLMs’ semantic understanding for precise localization of log anomalies. Second, a K-nearest neighbor graph reconstruction algorithm is designed to reconstruct the relevant neighborhood graph of malicious entities, enhancing contextual awareness of attack behavior. Finally, a cyclically enhanced analysis mechanism, guided by the Adversarial Tactics, Techniques, and Common Knowledge (ATT&CK) knowledge graph, allows the LLM to iteratively reason and generate threat intelligence reports with multiple dimensions, while simultaneously providing multi-layered explanations and automated mitigation strategies. Experiments using the Defense Advanced Research Projects Agency Transparent Computing Engagement 3 (DARPA TC-E3) dataset demonstrate that, compared to baseline methods, the proposed system achieves a 5 % improvement in detection precision and a 4 % increase in F1-score, while producing high-quality, multi-dimensional threat intelligence reports.
ER  - 

TY  - JOUR
T1  - Automated graph anomaly detection with large language models
AU  - Yu, Jiaqi
AU  - Gao, Yang
AU  - Yang, Hong
AU  - Tian, Zhihong
AU  - Zhang, Peng
AU  - Zhu, Xingquan
JO  - Knowledge-Based Systems
VL  - 324
SP  - 113809
PY  - 2025
DA  - 2025/08/03/
SN  - 0950-7051
DO  - https://doi.org/10.1016/j.knosys.2025.113809
UR  - https://www.sciencedirect.com/science/article/pii/S095070512500855X
KW  - Graph anomaly detection
KW  - Automated machine learning
KW  - Graph neural architecture search
KW  - Large language models
AB  - Graph neural networks (GNNs) have emerged as powerful tools for graph anomaly detection (GAD). However, designing effective GNN architectures for GAD often demands considerable domain expertise and laborious manual tuning. Although graph neural architecture search (GNAS) has made significant progress in automating the discovery of effective deep architectures, existing GNAS methods are challenging to directly apply to GAD tasks due to the lack of a dedicated search space tailored for GAD and the difficulty in effectively incorporating domain expert knowledge into the model architecture generation process. To address these challenges, this paper proposes an automated graph anomaly detection (AutoGAD for short) framework. AutoGAD automates the generation of optimal neural network architectures through a predefined search space and an efficient search strategy. Specifically, we first design a novel search space tailored for GAD tasks based on the characteristics of the graph autoencoder framework. Then, we leverage a large language model (LLM) as the controller of GNAS, guiding the LLM to rapidly generate architectures suitable for GAD within the search space through well-designed prompts. Extensive experimental results demonstrate that AutoGAD can generate new architectures that outperform existing GAD models, and its effectiveness is consistently observed across different datasets.
ER  - 

TY  - JOUR
T1  - Building adaptative and transparent cyber agents with local language models
AU  - Rigaki, Maria
AU  - Catania, Carlos A.
AU  - García, Sebastian
JO  - Expert Systems with Applications
VL  - 299
SP  - 129987
PY  - 2026
DA  - 2026/03/01/
SN  - 0957-4174
DO  - https://doi.org/10.1016/j.eswa.2025.129987
UR  - https://www.sciencedirect.com/science/article/pii/S0957417425036024
KW  - Large language models
KW  - Agents
KW  - Reinforcement learning
KW  - Network security
AB  - Autonomous intelligent agents offer a transformative approach to cyber defense by operating independently in complex and dynamic environments. While much research has focused on defensive systems, offensive agents are equally important for testing system resilience and improving defenses through realistic adversarial interaction. Recent advances demonstrate that large language models can automate penetration testing effectively, rivaling traditional reinforcement learning methods, but their reliance on cloud-based services introduces significant concerns around privacy and reproducibility. Smaller language models provide a promising alternative for local deployment in environments with limited resources or strict confidentiality requirements. Although these models have limitations, such as smaller context windows and a higher tendency to generate incorrect information, their performance can be enhanced through domain-specific fine-tuning techniques like supervised fine-tuning and direct preference optimization. In this study, we fine-tune a seven-billion-parameter version of the Zephyr model to create an autonomous penetration testing agent called Hackphyr, which is integrated into a cognitive architecture for autonomous decision-making. We evaluate Hackphyr in a simulated network security environment designed for ethical cybersecurity research and aligned with real-world attack tactics. In extensive evaluations, Hackphyr achieved win rates above 85 % in simpler scenarios without defenders and 23–50 % in more complex scenarios. The Hackphyr-based agent outperformed all baseline agents, and it was consistently approaching the performance of the most capable commercial models, even in unfamiliar scenarios. Beyond its penetration testing performance, Hackphyr exhibits structured strategic behavior aligned with realistic attack stages such as reconnaissance, privilege escalation, lateral movement, and data exfiltration. These findings highlight the potential of locally deployed small language models to support effective and transparent offensive operations in cybersecurity.
ER  - 

TY  - JOUR
T1  - Enhancing federated intrusion detection through LLM-Driven alert enrichment and collaborative threat information sharing
AU  - Fernández Saura, Pablo
AU  - Bernal Bernabé, Jorge
AU  - Skármeta Gómez, Antonio
JO  - Future Generation Computer Systems
VL  - 178
SP  - 108319
PY  - 2026
DA  - 2026/05/01/
SN  - 0167-739X
DO  - https://doi.org/10.1016/j.future.2025.108319
UR  - https://www.sciencedirect.com/science/article/pii/S0167739X25006132
KW  - Intrusion detection systems
KW  - Federated learning
KW  - Large language models
KW  - Retrieval augmented generation
KW  - Cyber threat intelligence
KW  - Non-IID data
AB  - The potential of Federated Learning (FL) to enhance Intrusion Detection Systems (IDS) is often compromised by performance degradation in non-independent and identically distributed (non-IID) data environments. While novel and powerful solutions have been proposed to mitigate this challenge, such as tailored aggregation algorithms, the near-perfect detection performance and low false positive rate needed for many critical use cases often remain elusive. This paper proposes a novel FL-based framework where participant organizations leverage a Large Language Model (LLM) to re-evaluate and enrich in real-time the low-confidence alerts produced by their base, FL-trained detection model. The LLM context for each alert is critically enhanced by two tiers of Threat Intelligence (TI): external TI (e.g., IP reputation services) and inter-organizational TI derived from a Federated Retrieval Augmented Generation (RAG) Incident Store of collaboratively shared, verified attack summaries. Furthermore, high-confidence LLM-verified alerts are fed back to continuously re-train and update the base detection model. Our experimental results, conducted on a challenging non-IID scenario using the CIC-IDS-2017 dataset where one organization completely lacked data from one attack type, demonstrate the framework’s effectiveness. With full TI context, 2 out of the 4 analyzed LLMs achieved perfect recall in re-classifying low-confidence alerts from the missing class. The feedback loop using these LLM-verified samples boosted the base detection model recall from 75.85 % to 91.47 % (F1-score from 86.27 % to 95.54 %) on the missing attack class. Additionally, an approximate 1 % reduction in the False Positive Rate (FPR) was achieved, while inference times demonstrate potential for near-real-time applications.
ER  - 

TY  - JOUR
T1  - Pirate-GPT: A locally deployed large language model framework for reliable offline anti-piracy decision support and knowledge retrieval in maritime operations
AU  - Liu, Xiliang
AU  - Hu, Jinghong
AU  - Mei, Qiang
AU  - Wang, Shaohua
JO  - Reliability Engineering & System Safety
VL  - 267
SP  - 111891
PY  - 2026
DA  - 2026/03/01/
SN  - 0951-8320
DO  - https://doi.org/10.1016/j.ress.2025.111891
UR  - https://www.sciencedirect.com/science/article/pii/S0951832025010919
KW  - Anti-piracy decision
KW  - Localized deployment
KW  - Piracy-related knowledge
KW  - Question-answering system
AB  - Ensuring the reliability and safety of maritime transportation systems is critically impeded by the fragmentation, complexity, and redundancy of piracy-related information. This study introduces Pirate-GPT, a domain-specific question answering framework built upon large language models(LLMs), designed to advance anti-piracy decision support and maritime knowledge management. Pirate-GPT integrates a semantic sliding-window segmentation algorithm to construct a semantically complete and robust vector database, facilitating precise information retrieval. The framework further employs an Agent collaboration mechanism and a “two-stage retrieval” process, significantly improving answer relevance and explainability while reducing hallucination rates. Centered on the Qwen2.5 LLM and optimized through Generative Pre-trained Transformer Quantization (GPTQ), Pirate-GPT supports efficient, stable, and privacy-preserving local deployment in offline and resource-constrained maritime environments. The system was evaluated using 7399 piracy reports and 230 related documents, and it consistently outperformed baseline models, with LooseMatch scores exceeding 90% and marked reductions in hallucinations across various query types. Ablation studies confirmed the essential contributions of each module to the system’s overall performance. Consequently, Pirate-GPT offers a scalable, reliable, and practical solution, providing a systematic approach to integrating complex domain knowledge and supporting robust risk assessment and decision-making in real-world maritime operations.
ER  - 

TY  - JOUR
T1  - Towards secure intelligent O-RAN architecture: vulnerabilities, threats and promising technical solutions using LLMs
AU  - Karbalaee Motalleb, Mojdeh
AU  - Benzaid, Chafika
AU  - Taleb, Tarik
AU  - Katz, Marcos
AU  - Shah-Mansouri, Vahid
AU  - Kim, Jaeho
JO  - Digital Communications and Networks
PY  - 2025
DA  - 2025/05/15/
SN  - 2352-8648
DO  - https://doi.org/10.1016/j.dcan.2025.05.001
UR  - https://www.sciencedirect.com/science/article/pii/S2352864825000653
KW  - Open radio access network
KW  - O-RAN security
KW  - Large language models
KW  - Blockchain
KW  - Moving target defense
AB  - The evolution of wireless communication systems will be fundamentally impacted by an Open Radio Access Network (O-RAN), a new concept defining an intelligent architecture with enhanced flexibility, openness, and the ability to slice services more efficiently. For all its promises and like any technological advancement, O-RAN is not without risks that need to be carefully assessed and properly addressed to accelerate its wide adoption in future mobile networks. In this paper, we present an in-depth security analysis of the O-RAN architecture, discussing the potential threats that may arise in different O-RAN architecture layers and their impact on the Confidentiality, Integrity, and Availability (CIA) triad. We also promote the potential of zero trust, Moving Target Defense (MTD), blockchain, and Large Language Models (LLM) technologies in fortifying O-RAN's security posture. Furthermore, we numerically demonstrate the effectiveness of MTD in empowering robust deep reinforcement learning methods for dynamic network slice admission control in the O-RAN architecture. Moreover, we examine the effect of Explainable AI (XAI) based on Large Language Models (LLM) in securing the system.
ER  - 

TY  - JOUR
T1  - Automated Generation of Cybersecurity Response Playbooks via Large Language Models
AU  - Paduraru, Ciprian
AU  - Dumitru, Bogdan
AU  - Stefanescu, Alin
JO  - Procedia Computer Science
VL  - 270
SP  - 2987
EP  - 2996
PY  - 2025
DA  - 2025/01/01/
T2  - 29th International Conference on Knowledge-Based and Intelligent Information & Engineering Systems (KES 2025)
SN  - 1877-0509
DO  - https://doi.org/10.1016/j.procs.2025.09.423
UR  - https://www.sciencedirect.com/science/article/pii/S1877050925030960
KW  - Cybersecurity automation
KW  - Incident response
KW  - SIEM/SOAR integration
KW  - Large language models (LLMs)
KW  - CACAO playbooks
AB  - Modern cybersecurity incident response workflows remain highly reliant on manual intervention, frequently resulting in delays and inconsistencies in threat mitigation. This paper introduces an automated method that leverages compact, fine-tuned large language models (LLMs) to generate CACAO-compliant security playbooks from structured incident data, aligned with emerging cybersecurity standards. To support both model fine-tuning and empirical evaluation, we introduce a novel dataset that integrates validated real-world incidents with systematically constructed synthetic scenarios. The approach uses a JSON-based intermediate representation to facilitate the structured transformation of incident data into executable mitigation procedures. In addition, we incorporate post-processing routines and prompt optimization techniques to improve structural validity and semantic coherence. Experimental results indicate that task-adapted compact LLMs achieve performance comparable to significantly larger models. At the same time, they reduce computational requirements, enabling deployment in resource-constrained environments and integration with existing SIEM and SOAR systems.
ER  - 

TY  - JOUR
T1  - The paradigm of hallucinations in AI-driven cybersecurity systems: Understanding taxonomy, classification outcomes, and mitigations
AU  - Sood, Aditya K
AU  - Zeadally, Sherali
AU  - Hong, EenKee
JO  - Computers and Electrical Engineering
VL  - 124
SP  - 110307
PY  - 2025
DA  - 2025/05/01/
SN  - 0045-7906
DO  - https://doi.org/10.1016/j.compeleceng.2025.110307
UR  - https://www.sciencedirect.com/science/article/pii/S0045790625002502
KW  - Artificial intelligence
KW  - Cybersecurity
KW  - Threat intelligence
KW  - Generative AI
KW  - Large language models
AB  - The adoption of AI to solve cybersecurity problems is occurring exponentially. However, AI-driven cybersecurity systems face significant challenges due to the impact of hallucinations in Large Language Models (LLMs). In AI-driven cybersecurity systems, hallucinations refer to instances when an AI model generates fabricated, inaccurate, and misleading information that impacts the security posture of organizations. This failure to recognize and misreport security threats identifies benign activities as malicious, invents insights not grounded to actual cyber threats, and causes real threats to go undetected due to erroneous interpretations. Hallucinations are a critical problem in AI-driven cybersecurity because they can lead to severe vulnerabilities, erode trust in automated systems, and divert resources to address non-existent threats. In cybersecurity, where real-time, accurate insights are vital, hallucinated outputs—such as mistakenly generated alerts, can cause a misallocation of time and resources. It is crucial to address hallucinations by improving LLM accuracy, grounding outputs in real-time data, and implementing human oversight mechanisms to ensure that AI-based cybersecurity systems remain trustworthy, reliable, and capable of defending against sophisticated threats. We present a taxonomy of hallucinations in LLMs for cybersecurity, including mapping LLM responses to classification outcomes (confusion matrix components). Finally, we discuss mitigation strategies to combat hallucinations.
ER  - 

TY  - JOUR
T1  - Misinformation detection on online social networks using pretrained language models
AU  - Ahmad, Pir Noman
AU  - Shah, Adnan Muhammad
AU  - Lee, KangYoon
AU  - Muhammad, Wazir
JO  - Information Processing & Management
VL  - 63
IS  - 1
SP  - 104342
PY  - 2026
DA  - 2026/01/01/
SN  - 0306-4573
DO  - https://doi.org/10.1016/j.ipm.2025.104342
UR  - https://www.sciencedirect.com/science/article/pii/S0306457325002833
KW  - Misinformation detection
KW  - Pretrained language models
KW  - Online social networks
KW  - Generative model
AB  - The growing prevalence of online misinformation poses substantial threats, with notable examples including the undermined integrity of democratic processes and decreased effectiveness of public health efforts. The effectiveness of existing solutions, such as user education and content removal, remains unclear, primarily because confirmation bias and peer pressure hinder the identification of noncredible information by users. To address these challenges posed by online misinformation, this study proposes a state-of-the-art approach that leverages transformer-based models, including bidirectional encoder representation from transformers (BERT), GPT-2, and XLNet. These models leverage attention mechanisms to simultaneously process and capture contextual subtleties in documents, enabling highly accurate misinformation detection and classification in dynamic and complex online narratives. A transformer-based pretrained language model is used to analyze, a large corpus of tweets related to misinformation events concerning the 2020 U.S. election. Although isolated interventions are found to be ineffective, a synergistic approach is shown to reduce misinformation prevalence by 87.9 % within a 40-min delay based on a credibility interval of 80 %. These findings highlight the potential of empirical models to inform policies, enhance content moderation practices, and strengthen public resilience against misinformation.
ER  - 

TY  - JOUR
T1  - Exploring the ability of emerging large language models to detect cyberbullying in social posts through new prompt-based classification approaches
AU  - Cirillo, Stefano
AU  - Desiato, Domenico
AU  - Polese, Giuseppe
AU  - Solimando, Giandomenico
AU  - Sugumaran, Vijayan
AU  - Sundaramurthy, Shanmugam
JO  - Information Processing & Management
VL  - 62
IS  - 3
SP  - 104043
PY  - 2025
DA  - 2025/05/01/
SN  - 0306-4573
DO  - https://doi.org/10.1016/j.ipm.2024.104043
UR  - https://www.sciencedirect.com/science/article/pii/S0306457324004023
KW  - Cyberbullying detection
KW  - Large language models
KW  - Prompt-based machine learning
KW  - Prompt engineering
AB  - The spread of new social networks in recent years, especially among adolescents, has increased the spread of social posts encouraging harmful behaviors, targeting people based on factors such as race, sex, or personal beliefs. This phenomenon makes it necessary to define intelligent tools capable of efficiently analyzing social media content. Recent Large Language Models (LLMs) have demonstrated advanced text generation and comprehension capabilities, making them efficient tools for identifying harmful posts. In this paper, we perform a large-scale evaluation of 20 generative LLMs in detecting cyberbullying phenomena in real social media posts through a new ad-hoc prompt Machine Learning approach (Prompt-based ML). We evaluate LLMs on binary and multiclass classification tasks on thousands of real posts from X, Facebook, and Reddit, and also compare their performance with 24 machine learning and natural language processing models. Specifically, the comparison analysis aims to understand the cyberbullying discrimination capability of LLMs with respect to traditional models, and the obtained findings to select suitable models for identifying harmful content on social network platforms. Furthermore, we provide an evaluation of the clarity, coherence, and relevance of the explanations provided by LLMs downstream of the identification of cyberbullying in social posts involving three domain experts. Experimental results highlight high performances of LLMs, particularly Claude 3.0 and Mistral family models, in identifying different types of cyberbullying. The domain expert evaluation of explainability showed that LLMs belonging to the Claude and Mistral families had better scores for clarity, coherence and relevance in their explanations compared to other models.
ER  - 

TY  - JOUR
T1  - Integrating large language models into manufacturing execution systems: A systematic framework for photoresist manufacturing processes
AU  - Fan, Shu-Kai S.
AU  - Wang, Jiun-Tze
AU  - Jen, Chih-Hung
JO  - Computers & Industrial Engineering
VL  - 211
SP  - 111599
PY  - 2026
DA  - 2026/01/01/
SN  - 0360-8352
DO  - https://doi.org/10.1016/j.cie.2025.111599
UR  - https://www.sciencedirect.com/science/article/pii/S0360835225007454
KW  - Manufacturing execution systems (MES)
KW  - Large Language Models (LLMs)
KW  - Reinforcement learning (RL)
AB  - In the context of an increasingly dynamic and competitive marketplace, traditional manufacturers that fail to adapt face an accelerating trajectory toward obsolescence. Digital transformation, particularly through the implementation of advanced Manufacturing Execution Systems (MES), presents a critical avenue for enhancing operational competitiveness and resilience. Despite the capabilities of MES to facilitate systematic production monitoring and control, these systems continue to rely heavily on human intervention, introducing the potential for operational inefficiencies and human-induced errors. Recent advancements in generative artificial intelligence (Gen AI) offer transformative opportunities to augment MES functionality. This article proposes the integration of large language models (LLMs) within MES architectures to provide real-time alarm detection, production line monitoring, and autonomous control support for the photoresist industry. Additionally, cutting-edge reinforcement learning (RL) algorithms are incorporated to analyze complex operational data streams and to deliver optimized, AI-generated recommendations for decision-makers. The proposed LLMs-based framework aims to significantly enhance decision accuracy, operational efficiency, and adaptive capacity in modern manufacturing environments.
ER  - 

TY  - JOUR
T1  - Enhancing trust in Large Language Models for streamlined decision-making in military operations
AU  - Marasco, Emanuela
AU  - Bourlai, Thirimachos
JO  - Image and Vision Computing
VL  - 158
SP  - 105489
PY  - 2025
DA  - 2025/05/01/
SN  - 0262-8856
DO  - https://doi.org/10.1016/j.imavis.2025.105489
UR  - https://www.sciencedirect.com/science/article/pii/S0262885625000770
KW  - Machine unlearning
KW  - Military
KW  - Trustworthy AI
KW  - Large Language Models
AB  - Large Language Models (LLMs) have the potential to enhance decision-making significantly in core military operational contexts that support training, readiness, and mission execution under low-risk conditions. Still, their implementation must be approached carefully, considering the associated risks. This paper examines the integration of LLMs into military decision-making, emphasizing the LLM’s ability to improve intelligence analysis, enhance situational awareness, support strategic planning, predict threats, optimize logistics, and strengthen cybersecurity. The paper also considers misinterpretation, bias, misinformation, or overreliance on AI-generated suggestions, potentially leading to errors in routine but critical decision-making processes. Our work concludes by proposing solutions and promoting the responsible implementation of LLMs to ensure their effective and ethical use in military operations. To build trust in LLMs, this paper advocates for developing cybersecurity frameworks, transparency, and ethical oversight. It further suggests using machine unlearning (MU) to selectively remove outdated or compromised data from LLM training datasets, preserving the integrity of the insights they generate. The paper underscores the imperative for integrating LLMs in low-risk military contexts, coupled with sustained research efforts to mitigate potential hazards.
ER  - 

TY  - JOUR
T1  - Predicting inter-state cyberattacks with graph-text fusion using graph neural networks and large language models
AU  - Dong, Jiping
AU  - Hao, Mengmeng
AU  - Ding, Fangyu
AU  - Chen, Shuai
AU  - Wu, Jiajie
AU  - Zhuo, Jun
JO  - Engineering Applications of Artificial Intelligence
VL  - 165
SP  - 113465
PY  - 2026
DA  - 2026/02/01/
SN  - 0952-1976
DO  - https://doi.org/10.1016/j.engappai.2025.113465
UR  - https://www.sciencedirect.com/science/article/pii/S0952197625034967
KW  - Cyberattack prediction
KW  - Graph neural network
KW  - Large language model
KW  - Multimodal fusion
KW  - Bilateral relations
AB  - Accurate forecasting of inter-state cyberattacks is crucial for the timely prevention of security risks. However, the dynamic evolution of international relations and the heterogeneity of multi-source data make this task significant challenging in both data integration and model design. To address these issues, we propose GeoDyG-LLM (Geopolitical Dynamic Graph-Large Language Model), a unified multimodal framework for geopolitically grounded cyberattack prediction. The framework models inter-state interactions by integrating dynamic graph neural networks (GNNs) with large language models (LLMs), jointly capturing the temporal, structural, and semantic dependencies from historical cyberattack records and news events. Methodologically, the framework (i) employs a dynamic multi-view GNN with a learnable projector, whose node embeddings are layer-wise injected into the Transformer layers of the LLM; (ii) leverages LLM-based semantic refinement to construct a high-quality multimodal dataset from raw data sources; and (iii) adopts a geopolitically-aware negative sampling strategy to generate informative and balanced training pairs. Experimental results show that GeoDyG-LLM (8B) substantially outperforms baselines, achieving an F1 score of 0.888 on cyberattack prediction. Moreover, the fine-tuned model generates coherent natural-language explanations aligned with historical and contextual evidence, enhancing interpretability and supporting practical geopolitical cyber risk analysis.
ER  - 

TY  - JOUR
T1  - Sóley: Automated detection of logic vulnerabilities in Ethereum smart contracts using large language models
AU  - Soud, Majd
AU  - Nuutinen, Waltteri
AU  - Liebel, Grischa
JO  - Journal of Systems and Software
VL  - 226
SP  - 112406
PY  - 2025
DA  - 2025/08/01/
SN  - 0164-1212
DO  - https://doi.org/10.1016/j.jss.2025.112406
UR  - https://www.sciencedirect.com/science/article/pii/S0164121225000743
KW  - Smart contracts
KW  - Blockchain
KW  - Automation
KW  - Software engineering
KW  - Vulnerability
AB  - Context:
Modern blockchain, such as Ethereum, supports the deployment and execution of so-called smart contracts, autonomous digital programs with significant value of cryptocurrency. Executing smart contracts requires gas costs paid by users, which define the limits of the contract’s execution. Logic vulnerabilities in smart contracts can lead to excessive gas consumption, financial losses, and are often the root cause of high-impact cyberattacks.
Objective:
Our objective is threefold: (i) empirically investigate logic vulnerabilities in real-world smart contracts extracted from code changes on GitHub, (ii) introduce Sóley, an automated method for detecting logic vulnerabilities in smart contracts, leveraging Large Language Models (LLMs), and (iii) examine mitigation strategies employed by smart contract developers to address these vulnerabilities in real-world scenarios.
Method:
We obtained smart contracts and related code changes from GitHub. To address the first and third objectives, we qualitatively investigated available logic vulnerabilities using an open coding method. We identified these vulnerabilities and their mitigation strategies. For the second objective, we extracted various logic vulnerabilities, focusing on those containing inline assembly fragments. We then applied preprocessing techniques and trained the proposed Sóley model. We evaluated Sóley along with the performance of various LLMs and compared the results with the state-of-the-art baseline on the task of logic vulnerability detection.
Results:
Our results include the curation of a large-scale dataset comprising 50,000 Ethereum smart contracts, with a total of 428,569 labeled instances of smart contract vulnerabilities, including 171,180 logic-related vulnerabilities. Our analysis uncovered nine novel logic vulnerabilities, which we used to extend existing taxonomies. Furthermore, we introduced several mitigation strategies extracted from observed developer modifications in real-world scenarios. Experimental results show that Sóley outperforms existing approaches in automatically identifying logic vulnerabilities, achieving a 9% improvement in accuracy and a maximum improvement of 24% in F1-measure over the Baseline. Interestingly, the efficacy of LLMs in this task was evident with minimal feature engineering. Despite the positive results, Sóley struggles to identify certain classes of logic vulnerabilities, which remain for future work.
Conclusion:
Early identification of logic vulnerabilities from code changes can provide valuable insights into their detection and mitigation. Recent advancements, such as LLMs, show promise in detecting logic vulnerabilities and contributing to smart contract security and sustainability.
ER  - 

TY  - JOUR
T1  - Large Language Models Integration in Smart Grids
AU  - Madani, Seyyedreza
AU  - Tavasoli, Ahmadreza
AU  - Khoshtarash Astaneh, Zahra
AU  - Pineau, Pierre-Olivier
JO  - Energy Reports
VL  - 14
SP  - 1562
EP  - 1577
PY  - 2025
DA  - 2025/12/01/
SN  - 2352-4847
DO  - https://doi.org/10.1016/j.egyr.2025.06.051
UR  - https://www.sciencedirect.com/science/article/pii/S2352484725004445
KW  - Large Language Models
KW  - Smart grids
KW  - Artificial intelligence
KW  - Energy systems
KW  - Reinforcement learning
AB  - Large Language Models (LLMs) are changing the way we operate our society and will undoubtedly impact power systems as well—but how exactly? By integrating various data streams—including real-time grid data, market dynamics, and consumer behaviors—LLMs have the potential to make power system operations more adaptive, enhance proactive security measures, and deliver personalized energy services. This paper provides a comprehensive analysis of 30 real-world applications across eight key categories: Grid Operations and Management, Energy Markets and Trading, Personalized Energy Management and Customer Engagement, Grid Planning and Education, Grid Security and Compliance, Advanced Data Analysis and Knowledge Discovery, Emerging Applications and Societal Impact, and LLM-Enhanced Reinforcement Learning. Critical technical hurdles, such as data privacy and model reliability, are examined, along with possible solutions. Ultimately, this review illustrates how LLMs can significantly contribute to building more resilient, efficient, and sustainable energy infrastructures, underscoring the necessity of their responsible and equitable deployment.
ER  - 

TY  - JOUR
T1  - An explainable and adaptive Internet of Things intrusion detection system supported by Large Language Models
AU  - Huang, Yunfan
AU  - Ma, Maode
AU  - Raymond, Wong Jee Keen
AU  - Chow, Chee-Onn
JO  - Engineering Applications of Artificial Intelligence
VL  - 163
SP  - 112911
PY  - 2026
DA  - 2026/01/01/
SN  - 0952-1976
DO  - https://doi.org/10.1016/j.engappai.2025.112911
UR  - https://www.sciencedirect.com/science/article/pii/S0952197625029422
KW  - Intrusion Detection System
KW  - Internet of Things
KW  - Denoising Convolutional Autoencoder
KW  - Large Language Models
KW  - Retrieval-Augmented Generation
KW  - Explainable artificial intelligence
AB  - To address the increasing threats targeting Internet of Things (IoT) networks, the development of IoT Intrusion Detection Systems (IDS) has accelerated in recent years. However, existing IDSs often rely on manually labeled data and lack explainability, limiting their adaptability and practical deployment. This paper proposes an Explainable and Adaptive Internet of Things Intrusion Detection System (EADL-IDS) that combines a Denoising Convolutional Autoencoder (DCAE) for unsupervised binary anomaly detection with Large Language Models (LLMs) for multi-class attack interpretation via Retrieval-Augmented Generation (RAG). Compared to prior machine learning and deep learning based IDSs, EADL-IDS eliminates the need for labeled data while achieving higher detection accuracy. Unlike conventional LLM-based IDSs, it integrates a more effective Deep Neural Network (DNN) backbone and structured knowledge retrieval to improve explainability and reduce inference delay. Experiments on the Network Security Laboratory - Knowledge Discovery and Data Mining (NSL-KDD), Canadian Institute for Cybersecurity Internet of Things 2023 (CIC-IoT-2023), and Army Cyber Institute Internet of Things Network Traffic Dataset 2023 (ACI-IoT-2023) datasets show that EADL-IDS achieves over 95% binary accuracy and over 85% multi-class accuracy without supervision, offering a scalable and robust solution for IoT security.
ER  - 

TY  - JOUR
T1  - AttacKG+: Boosting attack graph construction with Large Language Models
AU  - Zhang, Yongheng
AU  - Du, Tingwen
AU  - Ma, Yunshan
AU  - Wang, Xiang
AU  - Xie, Yi
AU  - Yang, Guozheng
AU  - Lu, Yuliang
AU  - Chang, Ee-Chien
JO  - Computers & Security
VL  - 150
SP  - 104220
PY  - 2025
DA  - 2025/03/01/
SN  - 0167-4048
DO  - https://doi.org/10.1016/j.cose.2024.104220
UR  - https://www.sciencedirect.com/science/article/pii/S0167404824005261
KW  - Cyber threat intelligence analysis
KW  - Attack graph construction
KW  - Large Language Models
AB  - Attack graph construction seeks to convert textual cyber threat intelligence (CTI) reports into structured representations, portraying the evolutionary traces of cyber attacks. Even though previous research has proposed various methods to construct attack graphs, they generally suffer from limited generalization capability to diverse knowledge types as well as requirement of expertise in model design and tuning. Addressing these limitations, we seek to utilize Large Language Models (LLMs), which have achieved enormous success in a broad range of tasks given exceptional capabilities in both language understanding and zero-shot task fulfillment. Thus, we propose a fully automatic LLM-based framework to construct attack graphs named: AttacKG+. Our framework consists of four consecutive modules: rewriter, parser, identifier, and summarizer, each of which is implemented by instruction prompting and in-context learning empowered by LLMs. Furthermore, we upgrade the existing attack knowledge schema and propose a comprehensive version. We represent a cyber attack as a temporally unfolding event, each temporal step of which encapsulates three layers of representation, including behavior graph, MITRE TTP labels, and state summary. Extensive evaluation demonstrates that: (1) our formulation seamlessly satisfies the information needs in threat event analysis, (2) our construction framework is effective in faithfully and accurately extracting the information defined by AttacKG+. and (3) our attack graph directly benefits downstream security practices such as attack reconstruction. All the code and datasets will be released upon acceptance.
ER  - 

TY  - JOUR
T1  - LLM-AE-MP: Web Attack Detection Using a Large Language Model with Autoencoder and Multilayer Perceptron
AU  - Yang, Jing
AU  - Wu, Yuangui
AU  - Yuan, Yuping
AU  - Xue, Haozhong
AU  - Bourouis, Sami
AU  - Abdel-Salam, Mahmoud
AU  - Prajapat, Sunil
AU  - Por, Lip Yee
JO  - Expert Systems with Applications
VL  - 274
SP  - 126982
PY  - 2025
DA  - 2025/05/15/
SN  - 0957-4174
DO  - https://doi.org/10.1016/j.eswa.2025.126982
UR  - https://www.sciencedirect.com/science/article/pii/S0957417425006049
KW  - Web attack
KW  - Anomalous request
KW  - Autoencoder
KW  - Long short-term memory
KW  - Generative adversarial network
KW  - Reinforcement learning
AB  - Web applications store sensitive data, making them prime targets for cybercriminals and posing national security risks. This study introduces a new approach to distinguishing legitimate and malicious hypertext transfer protocol (HTTP) requests using an autoencoder (AE). The integration of AE allows for efficient feature distillation, enhancing the sensitivity of the model to anomalies in HTTP traffic. The AE framework is combined with a transductive long short-term memory (TLSTM) network, which is trained with an advanced generative adversarial network (GAN). Using GAN promotes an adaptive learning environment, significantly boosting the robustness and generalizability of our method against evolving web attack vectors. TLSTM uses transductive learning to focus on data points near the test set, improving the adaptability of the model to outperform traditional LSTM models. In our GAN, the generator purposely excludes gradients from the most influential batch elements, improving the ability of the model to generate diverse and generalized outputs. After training the AE, its latent representations are passed to a multilayer perceptron (MLP) for detection tasks. To address the imbalanced classification in MLP, we use a reinforcement learning (RL) strategy. The RL approach strategically adjusts incentives, enhancing the performance of the model in identifying less frequent but critical malicious instances, thereby supporting a balanced security assessment. Our evaluations using the CSIC 2010 (Spanish National Research Council 2010), FWAF (web application firewall), and HttpParams datasets show that our method outperforms existing techniques, achieving (Accuracy, F-measure, geometric mean (G-means), and area under the curve (AUC)) reaching (90.937%, 89.755%, 88.446%, 0.838), (89.055, 90.663%, 88.334%, 0.847) and (92.242%, 93.774%, 91.356%, 0.897), respectively. Moreover, our model achieves efficient runtime and memory usage across the datasets, providing a practical solution for real-time web attack detection. These results confirm the effectiveness of the model in security contexts, representing a substantial advancement in web attack detection and the improvement of investigative strategies.
ER  - 

TY  - JOUR
T1  - ExplainableDetector: Exploring transformer-based language modeling approach for SMS spam detection with explainability analysis
AU  - Uddin, Mohammad Amaz
AU  - Islam, Muhammad Nazrul
AU  - Maglaras, Leandros
AU  - Janicke, Helge
AU  - Sarker, Iqbal H.
JO  - Digital Communications and Networks
VL  - 11
IS  - 5
SP  - 1504
EP  - 1518
PY  - 2025
DA  - 2025/10/01/
SN  - 2352-8648
DO  - https://doi.org/10.1016/j.dcan.2025.07.008
UR  - https://www.sciencedirect.com/science/article/pii/S235286482500118X
KW  - Cybersecurity
KW  - Machine learning
KW  - Large language model
KW  - Spam detection
KW  - Text analytics
KW  - Explainable AI
KW  - Fine-tuning
KW  - Transformer
AB  - Short Message Service (SMS) is a widely used and cost-effective communication medium that has unfortunately become a frequent target for unsolicited messages - commonly known as SMS spam. With the rapid adoption of smartphones and increased Internet connectivity, SMS spam has emerged as a prevalent threat. Spammers have recognized the critical role SMS plays in today's modern communication, making it a prime target for abuse. As cybersecurity threats continue to evolve, the volume of SMS spam has increased substantially in recent years. Moreover, the unstructured format of SMS data creates significant challenges for SMS spam detection, making it more difficult to successfully combat spam attacks. In this paper, we present an optimized and fine-tuned transformer-based Language Model to address the problem of SMS spam detection. We use a benchmark SMS spam dataset to analyze this spam detection model. Additionally, we utilize pre-processing techniques to obtain clean and noise-free data and address class imbalance problem by leveraging text augmentation techniques. The overall experiment showed that our optimized fine-tuned BERT (Bidirectional Encoder Representations from Transformers) variant model RoBERTa obtained high accuracy with 99.84%. To further enhance model transparency, we incorporate Explainable Artificial Intelligence (XAI) techniques that compute positive and negative coefficient scores, offering insight into the model's decision-making process. Additionally, we evaluate the performance of traditional machine learning models as a baseline for comparison. This comprehensive analysis demonstrates the significant impact language models can have on addressing complex text-based challenges within the cybersecurity landscape.
ER  - 

TY  - JOUR
T1  - Anomaly detection for blockchain nodes based on eBPF and fine-tuning large language model
AU  - Su, Jincheng
AU  - Chen, Zhide
AU  - Zhu, Kexin
AU  - Feng, Chen
JO  - Journal of Information Security and Applications
VL  - 97
SP  - 104329
PY  - 2026
DA  - 2026/03/01/
SN  - 2214-2126
DO  - https://doi.org/10.1016/j.jisa.2025.104329
UR  - https://www.sciencedirect.com/science/article/pii/S2214212625003655
KW  - Blockchain
KW  - Anomaly detection
KW  - eBPF
KW  - Security monitoring
KW  - Large language models
AB  - While blockchain technology is widely used across various fields, it faces growing security challenges. Traditional blockchain anomaly detection methods, such as log analysis and fixed pattern recognition, struggle to handle complex and dynamic attack techniques. This paper proposes the Blockchain Live Anomaly Detection with eBPF and LLMs (BLAD-eLLM) scheme, which combines the efficient data capture capabilities of extended Berkeley Packet Filter (eBPF) technology for kernel-level security monitoring with the deep text understanding and semantic analysis power of large language models (LLMs) to enhance the network layer security of blockchain nodes. The proposed approach analyzes blockchain network activities comprehensively, aiming for accurate identification of potential anomalous behaviors. Furthermore, a knowledge-enhanced reasoning framework is developed, integrating Retrieval-Augmented Generation (RAG) for contextual blockchain threat intelligence and Chain-of-Thought (COT) prompting for multi-step attack analysis while employing Weight-Decomposed Low-Rank Adaptation (DoRA) based prompt fine-tuning to optimize the LLMs’ domain-specific adaptability and detection accuracy. Experimental results demonstrate that the proposed scheme significantly improves blockchain anomaly detection accuracy while maintaining minimal impact on system performance, ensuring the stability and security of the blockchain system.
ER  - 

TY  - JOUR
T1  - DroidTTP: Mapping android applications with TTP for Cyber Threat Intelligence
AU  - R. Arikkat, Dincy
AU  - P., Vinod
AU  - Rehiman K.A., Rafidha
AU  - Nicolazzo, Serena
AU  - Arazzi, Marco
AU  - Nocera, Antonino
AU  - Conti, Mauro
JO  - Journal of Information Security and Applications
VL  - 93
SP  - 104162
PY  - 2025
DA  - 2025/09/01/
SN  - 2214-2126
DO  - https://doi.org/10.1016/j.jisa.2025.104162
UR  - https://www.sciencedirect.com/science/article/pii/S2214212625001991
KW  - Cyber Threat Intelligence
KW  - Tactic technique and procedure
KW  - Feature selection
KW  - Large Language Model
KW  - Retrieval augmented generation
AB  - The widespread use of Android devices for sensitive operations has made them prime targets for sophisticated cyber threats, including Advanced Persistent Threats (APT). Traditional malware detection methods focus primarily on malware classification, often failing to reveal the Tactics, Techniques, and Procedures (TTPs) used by attackers. To address this issue, we propose DroidTTP, a novel system for mapping Android malware to attack behaviors. We curated a dataset linking Android applications to Tactics and Techniques and developed an automated mapping approach using the Problem Transformation Approach and Large Language Models (LLMs). Our pipeline includes dataset construction, feature selection, data augmentation, model training, and explainability via SHAP. Furthermore, we explored the use of LLMs for TTP prediction using both Retrieval Augmented Generation and fine-tuning strategies. The Label Powerset XGBoost model achieved the best performance, with Jaccard Similarity scores of 0.9893 for Tactic classification and 0.9753 for Technique classification. The fine-tuned LLaMa model also performed competitively, achieving 0.9583 for Tactics and 0.9348 for Techniques. Although XGBoost slightly outperformed LLMs, the narrow performance gap highlights the potential of LLM-based approaches for Tactic and Technique prediction.
ER  - 

TY  - JOUR
T1  - Enabling Real-Time, Explainable DDoS Mitigation via On-Premise Large Language Models and Flow Analysis
AU  - Wondimu, Henok
AU  - Alfatemi, Ali
AU  - Rahouti, Mohamed
AU  - Chehri, Abdellah
AU  - Weichbroth, Paweł
AU  - Ghani, Nasir
JO  - Procedia Computer Science
VL  - 270
SP  - 1390
EP  - 1399
PY  - 2025
DA  - 2025/01/01/
T2  - 29th International Conference on Knowledge-Based and Intelligent Information & Engineering Systems (KES 2025)
SN  - 1877-0509
DO  - https://doi.org/10.1016/j.procs.2025.09.260
UR  - https://www.sciencedirect.com/science/article/pii/S1877050925029333
KW  - Intrusion Detection
KW  - LLMs
KW  - DDoS
KW  - Network Security
KW  - Automated Mitigation
KW  - Explainability
AB  - Distributed Denial of Service (DDoS) attacks continue to escalate in both frequency and sophistication, often overwhelming critical network infrastructures. While deep learning methods excel at recognizing malicious patterns, their lack of transparency undermines trust and hampers effective mitigation. This paper introduces a unified, on-premise pipeline that integrates an advanced flow based attack classifier with a local large language model (LLM) to deliver explainable, real-time DDoS defense. The proposed approach detects threats at the flow level, rapidly fags suspicious traffic, and then generates human-readable analyses and device specific countermeasures ranging from firewall rules to intrusion prevention system signatures all without transmitting data of-site. Through comprehensive testing on diverse, large scale network traces, we demonstrate that this framework not only achieves near-perfect detection accuracy but also considerably reduces operational costs and privacy risks associated with external cloud services. Furthermore, evaluators confirm the clarity and correctness of the automatically generated mitigation strategies, highlighting the system’s practicality in enterprise environments. Overall, our results validate on-premise, LLM-enhanced DDoS defense as a robust, transparent, and economical solution for safeguarding modern network ecosystems.
ER  - 

TY  - JOUR
T1  - Elevating Cyber Threat Intelligence against disinformation campaigns with LLM-based concept extraction and the FakeCTI dataset
AU  - Cotroneo, Domenico
AU  - Natella, Roberto
AU  - Orbinato, Vittorio
JO  - Journal of Systems and Software
VL  - 232
SP  - 112660
PY  - 2026
DA  - 2026/02/01/
SN  - 0164-1212
DO  - https://doi.org/10.1016/j.jss.2025.112660
UR  - https://www.sciencedirect.com/science/article/pii/S0164121225003292
KW  - Cyber Threat Intelligence
KW  - Disinformation
KW  - Fake news
KW  - Large Language Models
AB  - The swift spread of fake news and disinformation campaigns poses a significant threat to public trust, political stability, and cybersecurity. Traditional Cyber Threat Intelligence (CTI) approaches, which rely on low-level indicators such as domain names and social media handles, are easily evaded by adversaries who frequently modify their online infrastructure. To address these limitations, we introduce a novel CTI framework that focuses on high-level, semantic indicators derived from recurrent narratives and relationships of disinformation campaigns. Our approach extracts structured CTI indicators from unstructured disinformation content, capturing key entities and their contextual dependencies within fake news using Large Language Models (LLMs). We further introduce FakeCTI, the first dataset that systematically links fake news to disinformation campaigns and threat actors. To evaluate the effectiveness of our CTI framework, we analyze multiple fake news attribution techniques, spanning from traditional Natural Language Processing (NLP) to fine-tuned LLMs. This work shifts the focus from low-level artifacts to persistent conceptual structures, establishing a scalable and adaptive approach to tracking and countering disinformation campaigns.
ER  - 

TY  - JOUR
T1  - An AI-Driven Thermal-Fluid Testbed for Advanced Small Modular Reactors: Integration of Digital Twin and Large Language Models
AU  - Lim, Doyeong
AU  - Ndum, Zavier Ndum
AU  - Young, Christian
AU  - Hassan, Yassin
AU  - Liu, Yang
JO  - AI Thermal Fluids
VL  - 4
SP  - 100023
PY  - 2025
DA  - 2025/12/01/
SN  - 3050-5852
DO  - https://doi.org/10.1016/j.aitf.2025.100023
UR  - https://www.sciencedirect.com/science/article/pii/S3050585225000229
KW  - Thermal-fluid testbed
KW  - Artificial intelligence
KW  - Digital twin
KW  - Large language model
KW  - Small modular reactors
AB  - This paper presents a multipurpose artificial intelligence (AI)-driven thermal-fluid testbed designed to advance Small Modular Reactor (SMR) technologies through the seamless integration of physical experimentation, high-fidelity digital twin, and sophisticated AI frameworks. The platform uniquely combines a versatile three-loop thermal-fluid testbed with a System Analysis Module (SAM)-based digital twin that is accelerated by a Gated Recurrent Unit (GRU) neural network to achieve real-time operational capabilities. The GRU model, trained on a composite dataset of experimental and simulation data, accurately forecasts future system states and corresponding control actions with a temperature prediction RMSE of 4.25 K, enabling long-term transient predictions for operational planning. The digital twin’s performance was validated through comprehensive experimental campaigns, demonstrating its ability to predict complex thermal-fluid dynamics during power transients. Furthermore, an intelligent operator assistance system powered by a large language model (LLM) was developed using context engineering techniques, which synthesizes real-time experimental data, digital twin predictions, and user queries to provide actionable operational guidance in natural language. The LLM implementation demonstrated robust performance in multi-parameter correlation analysis, predictive reasoning, and safety-aware recommendations. This integrated platform establishes a new paradigm for nuclear research infrastructure, demonstrating how the convergence of physical testbeds, AI-accelerated digital twins, and natural language interfaces can accelerate the development and deployment of next-generation intelligent nuclear systems.
ER  - 

TY  - JOUR
T1  - Generative Agent-Based Modeling with Large Language Models for insider threat detection
AU  - Ferraro, Antonino
AU  - Orlando, Gian Marco
AU  - Russo, Diego
JO  - Engineering Applications of Artificial Intelligence
VL  - 157
SP  - 111343
PY  - 2025
DA  - 2025/10/01/
SN  - 0952-1976
DO  - https://doi.org/10.1016/j.engappai.2025.111343
UR  - https://www.sciencedirect.com/science/article/pii/S0952197625013454
KW  - Insider threat detection
KW  - Generative Agent-Based Modeling
KW  - Large Language Models
KW  - Cybersecurity
KW  - Multi-agent systems
AB  - Insider threats pose a critical challenge in cybersecurity, as individuals within organizations misuse legitimate access to compromise sensitive systems and data. Traditional detection methods often struggle with the complexity of such threats, while Deep Learning (DL) approaches face issues like overfitting and lack of interpretability. To address these limitations, we propose a Generative Agent-Based Modeling (GABM) framework that integrates Large Language Models (LLMs) with a hierarchical multi-agent system. Our framework employs Specialized Agents to process categorized log files and generate detailed reports, which are synthesized by a Supervisor Agent for final activity classification. We validated this approach on both network-centric (PicoDomain) and behavior-rich (CERT r5.2) datasets, demonstrating its ability to handle diverse logs, model complex threats, and generalize across insider risk scenarios. The framework outperformed existing baselines, prioritizing high recall to minimize false negatives—crucial in cybersecurity contexts. While precision was comparatively lower, this trade-off supports early threat detection. An ablation study highlighted the importance of the Supervisor Agent, whose removal led to a significant drop in performance and increased false positives. These results demonstrate the potential of LLM-powered hierarchical multi-agent frameworks for scalable, interpretable, and reliable insider threat detection. Our contributions include the integration of GABM and LLMs, a hierarchical system for log analysis, and the use of Chain-of-Thought reasoning for enhanced interpretability.
ER  - 

TY  - JOUR
T1  - MET-LLM: Enhancing large language models for malicious encrypted traffic detection
AU  - Huang, Yongjun
AU  - Du, Pengfei
AU  - Li, Ruifan
AU  - Li, Xiaoyong
AU  - Li, Lixiang
JO  - Expert Systems with Applications
VL  - 303
SP  - 130621
PY  - 2026
DA  - 2026/03/25/
SN  - 0957-4174
DO  - https://doi.org/10.1016/j.eswa.2025.130621
UR  - https://www.sciencedirect.com/science/article/pii/S0957417425042368
KW  - Malicious encrypted traffic detection
KW  - Large language model
KW  - Traffic embedding
KW  - Domain adaptation
AB  - Modern networks have spurred growth in both legitimate and malicious activities concealed within encrypted traffic. Traditional machine learning approaches to traffic classification struggle with scalability to new protocols, diverse tasks, and adaptability to emerging threats. To address these issues, we propose MET-LLM, a novel framework for Malicious Encrypted Traffic detection that integrates domain-specific tokenization, a pretrained large language model, and a dynamic adaptive tuning adaptor. MET-LLM addresses the modal gap between natural language and heterogeneous network traffic data by partitioning each traffic sample into distinct headers and payloads and leveraging a specialized tokenizer trained on a large-scale traffic corpus to extend the base vocabulary of the underlying language model. Building on a domain-adapted pretrained model fine-tuned on extensive security-related corpora, MET-LLM captures critical contextual nuances distinguishing benign from malicious flows. Its dynamic adaptive tuning adaptor facilitates efficient parameter updates via adaptation prompt injection, adversarial training, and dynamic masking, enabling rapid adaptation to evolving network conditions and attack strategies. Extensive evaluations on benchmark datasets, including ISCX Tor 2016, ISCX VPN 2016, APP-53 2023, and CSTNET 2023, demonstrate that MET-LLM’s superior precision, recall, and F1 scores over state-of-the-art methods, affirming its efficacy and robustness in real-world cybersecurity applications. Our code is publicly available at the website, https://github.com/Superagentsys/MET-LLM.
ER  - 

TY  - JOUR
T1  - CyberLLaMA: A fine-tuned large language model for cybersecurity named entity recognition
AU  - Zhang, Hao
AU  - Wu, Tingmin
AU  - Zhu, Tianqing
AU  - Wen, Sheng
AU  - Xiang, Yang
JO  - Knowledge-Based Systems
VL  - 328
SP  - 114183
PY  - 2025
DA  - 2025/10/25/
SN  - 0950-7051
DO  - https://doi.org/10.1016/j.knosys.2025.114183
UR  - https://www.sciencedirect.com/science/article/pii/S0950705125012249
KW  - Cybersecurity
KW  - Named entity recognition (NER)
KW  - Large language models (LLMs)
KW  - Conditional random fields (CRF)
KW  - Cybersecurity dataset
AB  - Cybersecurity-specific Named-Entity Recognition (NER) is critical in addressing the escalating complexity and evolving cyber threats. While deep learning methods form the foundation of modern NER tasks, they still fall short in addressing cybersecurity-specific NER tasks due to the limited availability of up-to-date cybersecurity datasets and the unique characteristics of cybersecurity terminology (jargon, abbreviations, and rapidly evolving vocabulary). To bridge this gap we (i) compile a 42 404-article corpus and manually annotate 4788 unique security terms, and (ii) present CyberLLaMA-a framework that fine-tunes LLaMA-3.2-3B and stacks a bidirectional LSTM plus conditional-random-field layers to preserve label consistency. On the held-out test set, CyberLLaMA attains an F1 of 98.88 %, surpassing RoBERTa, SCBERT, and GPT-NER. The results indicate that CyberLLaMA, as an effective solution for Cybersecurity NER tasks, offers practical value to cybersecurity professionals and the general public by enhancing the extraction of cybersecurity information in texts.
ER  - 

TY  - JOUR
T1  - An adaptive intrusion detection system for the internet of things using large language models and post-quantum-secure blockchain
AU  - Huang, Yunfan
AU  - Ma, Maode
AU  - Raymond, Wong Jee Keen
AU  - Chow, Chee-Onn
JO  - Computer Networks
VL  - 274
SP  - 111819
PY  - 2026
DA  - 2026/01/01/
SN  - 1389-1286
DO  - https://doi.org/10.1016/j.comnet.2025.111819
UR  - https://www.sciencedirect.com/science/article/pii/S1389128625007856
KW  - Internet of things
KW  - Intrusion detection system
KW  - Denoising convolutional autoencoder
KW  - Large language models
KW  - Adaptive lifetime learning
KW  - Post-quantum-secure blockchain
KW  - Dual-switch learning
AB  - The proliferation of Internet of Things (IoT) devices makes them critical infrastructure, yet their scale and openness expose them to increasing cyberattacks. Existing dynamic IoT Intrusion Detection Systems (IDSs) can partially prevent cyberattacks. However, they typically demand extensive, high-quality labeled data, which makes updates costly and labor-intensive. Recently proposed Large-Language-Model (LLM)-based IoT IDSs address data labeling. However, they often over-rely on end-to-end LLM processing instead of hybrid LLM-Machine Learning (ML) approaches, which results in a significant detection delay. Meanwhile, the IDS’s data security is seldom considered in these works. This paper introduces the LLM and Post-Quantum-Secure Blockchain (QBc)-based adaptive Intrusion Detection System (LQB-IDS). LQB-IDS applies an adaptive detection method. It utilizes a lightweight ML model to detect known attacks that previously appeared in the training dataset. Meanwhile, all unknown attacks are handled by the LLM for dynamic analysis, using external knowledge secured by LLM managed QBc. Thus, it can continuously detect and adapt for unknown attacks without relying on pre-labeled data. Its core contributions include 1) an LLM-based unknown IoT traffic analyzer eliminating manual labeling; 2) a dual-switch learning approach combining LLM with lightweight ML models for low detection delay; and 3) a novel credit scoring algorithm with an LLM-Maintained Post-Quantum-Secure Blockchain-Based Database (LLM-QBc-DB) ensuring data security. Experimental results demonstrate that LQB-IDS achieves a 0.98 detection rate and a 0.97 average F1-score without labeled data, presenting an efficient and scalable solution for emerging IoT threats.
ER  - 

TY  - JOUR
T1  - Periodic watermarking for copyright protection of large language models in cloud computing security
AU  - Ye, Pei-Gen
AU  - Li, Zhengdao
AU  - Yang, Zuopeng
AU  - Chen, Pengyu
AU  - Zhang, Zhenxin
AU  - Li, Ning
AU  - Zheng, Jun
JO  - Computer Standards & Interfaces
VL  - 94
SP  - 103983
PY  - 2025
DA  - 2025/08/01/
SN  - 0920-5489
DO  - https://doi.org/10.1016/j.csi.2025.103983
UR  - https://www.sciencedirect.com/science/article/pii/S0920548925000121
KW  - Large language model
KW  - Watermark technology
KW  - Copyright protection
KW  - Cloud computing security
AB  - Large Language Models (LLMs) have become integral in advancing content understanding and generation, leading to the proliferation of Embedding as a Service (EaaS) within cloud computing platforms. EaaS leverages LLMs to offer scalable, on-demand linguistic embeddings, enhancing various cloud-based applications. However, the proprietary nature of EaaS makes it a target for model extraction attacks, where the timing of such infringements often remains elusive. This paper introduces TimeMarker, a novel framework that enhances temporal traceability in cloud computing environments by embedding distinct watermarks at different sub-periods, marking the first attempt to identify the timing of model extraction attacks. TimeMarker employs an adaptive watermark strength method based on information entropy and frequency domain transformations to refine the detection accuracy of model extraction attacks within cloud infrastructures. The granularity of time frame identification for theft improves as more sub-periods are used. Furthermore, our approach investigates single sub-period theft and extends to multi-sub-period theft scenarios where attackers steal data across many sub-periods to train their models in cloud settings. Validated across five widely used datasets, TimeMarker is capable of detecting model extraction over various sub-periods and assessing its impact on the accuracy and robustness of large models deployed in the cloud. The results demonstrate that TimeMarker effectively identifies different periods of extraction attacks, enhancing EaaS security within cloud computing and extending traditional watermarking to offer copyright protection for LLMs.
ER  - 

TY  - JOUR
T1  - Large language models for cyber resilience: A comprehensive review, challenges, and future perspectives
AU  - Ding, Weiping
AU  - Abdel-Basset, Mohamed
AU  - Ali, Ahmed M.
AU  - Moustafa, Nour
JO  - Applied Soft Computing
VL  - 170
SP  - 112663
PY  - 2025
DA  - 2025/02/01/
SN  - 1568-4946
DO  - https://doi.org/10.1016/j.asoc.2024.112663
UR  - https://www.sciencedirect.com/science/article/pii/S1568494624014376
KW  - Large Language Model
KW  - Cyber Resilience
KW  - Cyber Security
KW  - Data Privacy and Protection
KW  - Network and Endpoint Security
AB  - Interconnect cyber system is used by various users and organizations worldwide to perform different activities. These activities are combined with digital information and systems around the organizations to obtain higher accuracy and performance. However, these combinations of activities have faced cyber threats and attacks by single or multiple attackers. So, protecting and saving users' and organizations' sensitive data is a big challenge. So, the cyber resilience concept refers to the ability to prepare, absorb, recover, and adapt against cyberattacks and threats. It is used to mitigate cyberattacks and risks by the ability of the system to recover from threats. Artificial intelligence models enhance cyber resilience using machine learning and deep learning models. One of the most common components of artificial intelligence is large language models (LLM). It is used to understand language from text data and extract features to predict future words or missing in text datasets. LLM can enhance cyber resilience by providing various benefits for users and organizations. We divide the cyber resilience strategies into five parts. We review the LLM in each part, including security posture, data privacy and protection, security awareness, network security, and security automation. The fundamentals of LLMs are introduced as pre-trained models, transformers, encoders, and decoders. Then, we review the challenges of LLM in cyber resilience and cyber defense methods to overcome these challenges. We applied the LLM into three case studies including two for email spam text classifications and one for cyber threat detection. We obtained higher accuracy including 96.67 %, 90.70 %, and 89.94 % from three case studies respectively. Then we compared our LLM with other traditional machine learning models. The results show the LLM has higher accuracy, precision, recall, and f1 score compared with other models. Finally, the future directions of LLM in cyber resilience are provided.
ER  - 

TY  - JOUR
T1  - Adaptive Diffusion Markov-Enhanced GCN with LLM Explanations for IoT Attack Detection
AU  - Atitallah, Safa Ben
AU  - Driss, Maha
AU  - Alsehibani, Arwa
AU  - Boulila, Wadii
JO  - Procedia Computer Science
VL  - 270
SP  - 4014
EP  - 4023
PY  - 2025
DA  - 2025/01/01/
T2  - 29th International Conference on Knowledge-Based and Intelligent Information & Engineering Systems (KES 2025)
SN  - 1877-0509
DO  - https://doi.org/10.1016/j.procs.2025.09.526
UR  - https://www.sciencedirect.com/science/article/pii/S1877050925031990
KW  - Graph Neural Network
KW  - Intrusion Detection System
KW  - Internet of Things Security
KW  - Large Language Models
AB  - The increasing complexity and connectivity of Internet of Things (IoT) environments have made them targets for advanced cyber-attacks. Recently, Deep Learning (DL)-based intrusion detection systems have shown remarkable success in identifying malicious activities within IoT traffic. In particular, Graph Neural Networks (GNNs) have emerged as effective solutions. However, GNNs come with inherent challenges, including high computational complexity and a black-box nature, which limit their transparency and interoperability. In this paper, we introduce a hybrid framework that combines a GNN model, named AD-MGCN, with a Large Language Model (LLM) to address these limitations. The proposed AD-MGCN leverages a Markov-based multi-step diffusion process to enhance feature propagation, reduce noisy edges, and improve classification performance across both frequent and rare attack types. In addition, a fine-tuned instruction-based LLM (Falcon-7B Instruct) generates natural language explanations that translate model predictions into human-understandable insights. We evaluated our framework on the Edge-IIoTset dataset, which includes diverse IoT attack scenarios. The experimental results show that AD-MGCN achieves an accuracy of 97.38%, significantly outperforming the baseline GCN models. Furthermore, the LLM explanations achieve an average clarity score of 4.2/5 in expert evaluations, improving the transparency of the model for cybersecurity analysts. These results demonstrate the potential of AD-MGCN as a reliable, efficient, and interpretable solution for securing modern IoT ecosystems.
ER  - 

TY  - JOUR
T1  - SynthCTI: LLM-driven synthetic CTI generation to enhance MITRE technique mapping
AU  - Ruiz-Ródenas, Álvaro
AU  - Pujante Sáez, Jaime
AU  - García-Algora, Daniel
AU  - Rodríguez Béjar, Mario
AU  - Blasco, Jorge
AU  - Hernández-Ramos, José Luis
JO  - Future Generation Computer Systems
VL  - 177
SP  - 108232
PY  - 2026
DA  - 2026/04/01/
SN  - 0167-739X
DO  - https://doi.org/10.1016/j.future.2025.108232
UR  - https://www.sciencedirect.com/science/article/pii/S0167739X25005266
KW  - Cybersecurity
KW  - LLM
KW  - Cyber threat intelligence
KW  - MITRE ATT&CK framework
KW  - Data augmentation
KW  - Sentence classification
AB  - Cyber Threat Intelligence (CTI) mining extracts structured insights from unstructured threat data, enabling organizations to understand and respond to evolving adversarial behavior. A key task is mapping threat descriptions to MITRE ATT&CK techniques. However, this is often performed manually, requiring expert knowledge and substantial effort. Automated approaches face two major challenges: scarcity of high-quality labeled CTI data and class imbalance. While domain-specific Large Language Models (LLMs) such as SecureBERT have improved performance, most recent work focuses on model architecture rather than data limitations. We hypothesize that semantically guided synthetic CTI generation can mitigate such limitations. To test this hypothesis, we present SynthCTI, a data augmentation framework designed to generate high-quality synthetic CTI sentences for underrepresented MITRE ATT&CK techniques. The methodology converts CTI sentences into semantic vector representations, clusters them with HDBSCAN to identify semantically coherent groups, and extracts contextual features to construct prompts. These prompts guide an LLM to produce diverse and semantically faithful synthetic CTI sentences. We evaluate SynthCTI on two publicly available CTI datasets, CTI-to-MITRE and TRAM, using models with different capacities. Incorporating synthetic data leads to consistent macro-F1 improvements: for example, ALBERT improves from 0.35 to 0.52 (48.6 % relative gain), and SecureBERT from 0.4412 to 0.6558. Smaller models augmented with SynthCTI outperform larger models trained without augmentation, demonstrating the value of data generation for CTI classification. These results confirm our hypothesis and highlight the practicality of enabling smaller organizations to adopt advanced CTI analytics without requiring high-performance computing infrastructure.
ER  - 

TY  - JOUR
T1  - Java Source Code Vulnerability Detection Using Large Language Model
AU  - Anbiya, Dhika Rizki
AU  - Ferdinan, Teddy
AU  - Kołaczek, Grzegorz
JO  - Procedia Computer Science
VL  - 270
SP  - 2828
EP  - 2837
PY  - 2025
DA  - 2025/01/01/
T2  - 29th International Conference on Knowledge-Based and Intelligent Information & Engineering Systems (KES 2025)
SN  - 1877-0509
DO  - https://doi.org/10.1016/j.procs.2025.09.405
UR  - https://www.sciencedirect.com/science/article/pii/S1877050925030789
KW  - source code vulnerability detection
KW  - large language model
KW  - code representation
KW  - Abstract Syntax Tree
KW  - Code Property Graph
AB  - Vulnerability detection is one of the main focuses of the security domain. The rise of the Large Language Model (LLM) has shown promising performance compared to static code analysis and machine learning with neural architecture in vulnerability detection. These results motivate the researcher to dive deeper into the capabilities of large language models, exploring their potential to improve the performance of the existing method. This research explores vulnerability detection through the use of LLM model employing a combination of Abstract Syntax Tree (AST) and Code Property Graph (CPG) as the code representation at the granularity level. We investigate the effectiveness of concatenating both representations as well as utilizing them separately. We conducted an experiment using LLM that pre-trained with source code as well the model that trained with text corpus as a comparison. We compare the representation with the original source code in order to compare the significance of the contribution. The Software Assurance Reference Dataset (SARD) and CVEFixes are used as the dataset. The result reveals that the LLM pre-trained on a text corpus achieves effective vulnerability outcomes through combined representation, while the CPG representation delivers positive results for all pre-trained models that leverage source code.
ER  - 

TY  - JOUR
T1  - Latent space refinement for unsupervised cyber threat text classification
AU  - Wang, Yue
AU  - Nayak, Richi
AU  - Bashar, Md Abul
AU  - Chandramohan, Mahinthan
JO  - Knowledge-Based Systems
VL  - 329
SP  - 114399
PY  - 2025
DA  - 2025/11/04/
SN  - 0950-7051
DO  - https://doi.org/10.1016/j.knosys.2025.114399
UR  - https://www.sciencedirect.com/science/article/pii/S0950705125014388
KW  - Cyber threat intelligence
KW  - Text classification
KW  - Deep metric learning
KW  - Unsupervised classification
KW  - Clustering
KW  - Pretrained language models
KW  - Domain adaptation
AB  - Text classification plays a critical role in Cyber Threat Intelligence (CTI) applications, where open-source text data is mined to identify patterns such as Indicators of Compromise (IoC), Tactics, Techniques and Procedures (TTPs), Named Entities and more. However, the dynamic nature of CTI makes traditional supervised machine learning classifiers impractical due to their reliance on large number of labelled training datasets. To address this, we propose Latent Space Refinement (LSR), an unsupervised method designed for CTI text classification. LSR introduces a posterior regularisation strategy where an auxiliary distribution derived from a TF-IDF feature space serves as signals to refine latent representations derrived from Pretrained Language Models (PLMs). By iteratively refining this latent space with clustering signals, LSR enables efficient similarity-based classification using only a few user-provided seed keywords. Extensive experiments on diverse CTI tasks, including both binary and multi-class classification, demonstrate that LSR consistently outperforms state-of-the-art unsupervised and zero-shot/few-shot methods in Accuracy and Weighted F1 score, all without tuning internal PLM parameters. This makes LSR a lightweight and PLM-agnostic solution for real-world CTI applications.
ER  - 

TY  - JOUR
T1  - DLAP: A Deep Learning Augmented Large Language Model Prompting framework for software vulnerability detection
AU  - Yang, Yanjing
AU  - Zhou, Xin
AU  - Mao, Runfeng
AU  - Xu, Jinwei
AU  - Yang, Lanxin
AU  - Zhang, Yu
AU  - Shen, Haifeng
AU  - Zhang, He
JO  - Journal of Systems and Software
VL  - 219
SP  - 112234
PY  - 2025
DA  - 2025/01/01/
SN  - 0164-1212
DO  - https://doi.org/10.1016/j.jss.2024.112234
UR  - https://www.sciencedirect.com/science/article/pii/S0164121224002784
KW  - Vulnerability detection
KW  - Large Language Model
KW  - Prompting engineering
KW  - Framework
AB  - Software vulnerability detection is generally supported by automated static analysis tools, which have recently been reinforced by deep learning (DL) models. However, despite the superior performance of DL-based approaches over rule-based ones in research, applying DL approaches to software vulnerability detection in practice remains a challenge. This is due to the complex structure of source code, the black-box nature of DL, and the extensive domain knowledge required to understand and validate the black-box results for addressing tasks after detection. Conventional DL models are trained by specific projects and, hence, excel in identifying vulnerabilities in these projects but not in others. These models with poor performance in vulnerability detection would impact the downstream tasks such as location and repair. More importantly, these models do not provide explanations for developers to comprehend detection results. In contrast, Large Language Models (LLMs) with prompting techniques achieve stable performance across projects and provide explanations for results. However, using existing prompting techniques, the detection performance of LLMs is relatively low and cannot be used for real-world vulnerability detections. This paper contributes DLAP, a Deep Learning Augmented LLMs Prompting framework that combines the best of both DL models and LLMs to achieve exceptional vulnerability detection performance. Experimental evaluation results confirm that DLAP outperforms state-of-the-art prompting frameworks, including role-based prompts, auxiliary information prompts, chain-of-thought prompts, and in-context learning prompts, as well as fine-turning on multiple metrics.
ER  - 

TY  - JOUR
T1  - Evaluation of LLM-based chatbots for OSINT-based Cyber Threat Awareness
AU  - Shafee, Samaneh
AU  - Bessani, Alysson
AU  - Ferreira, Pedro M.
JO  - Expert Systems with Applications
VL  - 261
SP  - 125509
PY  - 2025
DA  - 2025/02/01/
SN  - 0957-4174
DO  - https://doi.org/10.1016/j.eswa.2024.125509
UR  - https://www.sciencedirect.com/science/article/pii/S0957417424023765
KW  - Cyber Threat Intelligence
KW  - Open-Source Intelligence
KW  - Natural Language Processing
KW  - Large Language Models
KW  - Chatbots
AB  - Knowledge sharing about emerging threats is crucial in the rapidly advancing field of cybersecurity and forms the foundation of Cyber Threat Intelligence (CTI). In this context, Large Language Models (LLMs) are becoming increasingly significant in the field of cybersecurity. This study surveys the performance of ChatGPT, GPT4all, Dolly, Stanford Alpaca, Alpaca-LoRA, Falcon, and Vicuna LLM-based chatbots in binary classification and Named Entity Recognition (NER) tasks using Open-source intelligence (OSINT) to detect and extract structured data about cybersecurity threats. We utilize data collected in previous research from Twitter to assess the competitiveness of these chatbots when compared to specialized state-of-the-art models trained for those tasks. In binary classification experiments, the commercial chatbot GPT-4 achieved an acceptable F1 score of 0.94, and the open-source GPT4all achieved an F1 score of 0.90. However, when applied for cybersecurity entity recognition, all evaluated LLM-based chatbots have limitations and are less effective. This study demonstrates the capability of LLM-based chatbots for OSINT processing and shows that they require further improvement in NER to effectively replace specially trained models. Our results highlight their strengths and limitations compared to specialized models. This provides insights for researchers to enhance LLM-based chatbots to reduce the effort required to integrate machine learning in OSINT-based CTI tools.
ER  - 

TY  - JOUR
T1  - Identification and Categorization of the Top 100 Articles and the Future of Large Language Models: Thematic Analysis Using Bibliometric Analysis
AU  - Bernstein, Ethan
AU  - Ramsamooj, Anya
AU  - Millar, Kelsey L
AU  - Lum, Zachary C
JO  - JMIR AI
VL  - 4
PY  - 2025
DA  - 2025/01/01/
SN  - 2817-1705
DO  - https://doi.org/10.2196/68603
UR  - https://www.sciencedirect.com/science/article/pii/S2817170525000638
KW  - large language models
KW  - ChatGPT
KW  - Web of Science
KW  - medicine
KW  - education
KW  - technology
KW  - research categorization
KW  - artificial intelligence
AB  - Background
Since the release of ChatGPT and other large language models (LLMs), there has been a significant increase in academic publications exploring their capabilities and implications across various fields, such as medicine, education, and technology.
Objective
This study aims to identify the most influential academic works on LLMs published in the past year, categorize their research types and thematic focuses, within different professional fields. The study also evaluates the ability of artificial intelligence (AI) tools, such as ChatGPT, to accurately classify academic research.
Methods
We conducted a bibliometric analysis using Clarivate’s Web of Science (WOS) to extract the top 100 most cited papers on LLMs. Papers were manually categorized by field, journal, author, and research type. ChatGPT-4 was used to generate categorizations for the same papers, and its performance was compared to human classifications. We summarized the distribution of research fields and assessed the concordance between AI-generated and manual classifications.
Results
Medicine emerged as the predominant field among the top 100 most cited papers, accounting for 43 (43%), followed by education 26 (26%) and technology 15 (15%). Medical literature primarily focused on clinical applications of LLMs, limitations of AI in health care, and the role of AI in medical education. In education, research was centered around ethical concerns and potential applications of AI for teaching and learning. ChatGPT demonstrated variable concordance with human reviewers, achieving an agreement rating of 47% for research types and 92% for fields of study.
Conclusions
While LLMs such as ChatGPT exhibit considerable potential in aiding research categorization, human oversight remains essential to address issues such as hallucinations, outdated information, and biases in AI-generated outputs. This study highlights the transformative potential of LLMs across multiple sectors and emphasizes the importance of continuous ethical evaluation and iterative improvement of AI systems to maximize their benefits while minimizing risks.
ER  - 

TY  - JOUR
T1  - A systematic review of transformers and large language models in the energy sector: towards agentic digital twins
AU  - Antonesi, Gabriel
AU  - Cioara, Tudor
AU  - Anghel, Ionut
AU  - Michalakopoulos, Vasilis
AU  - Sarmas, Elissaios
AU  - Toderean, Liana
JO  - Applied Energy
VL  - 401
SP  - 126670
PY  - 2025
DA  - 2025/12/15/
SN  - 0306-2619
DO  - https://doi.org/10.1016/j.apenergy.2025.126670
UR  - https://www.sciencedirect.com/science/article/pii/S030626192501400X
KW  - Artificial intelligence
KW  - Transformers
KW  - Large language models
KW  - Foundation models
KW  - GenAI
KW  - Generative AI
KW  - Agentic digital twins
KW  - Energy sector
KW  - Smart grid
AB  - Artificial intelligence (AI) has long promised to improve energy management in smart grids by enhancing situational awareness and supporting more effective decision-making. While traditional machine learning has demonstrated notable results in forecasting and optimization, it often struggles with generalization, situational awareness, and heterogeneous data integration. Recent advances in Transformer architecture, including Large Language Models (LLMs) and Foundation Models (FMs) can significantly improve the ability to model complex temporal and contextual relationships, as well as in multi-modal data fusion which is valuable for most AI applications in the energy sector. In this review, we synthesize the rapidly expanding field of AI applications in the energy domain, with a focus on Transformer Models (TMs) and LLMs, which have shown growing relevance. We examine the architectural foundations, domain-specific adaptations and practical implementations of TMs across various forecasting and grid management tasks. We then explore the emerging role of LLMs in the field: adaptation and fine tuning for the energy sector, the type of tasks they are suited for, and the new challenges they introduce. Along the way, we highlight practical implementations, innovations, and areas where the research frontier is rapidly expanding. These recent developments reviewed underscore a broader trend: Generative AI (GenAI) is beginning to augment decision-making not only in high-level planning but also in day-to-day operations, from forecasting and grid balancing to workforce training and asset onboarding. While FMs hold promises, we found limited evidence of their concrete application in energy domain to date. Therefore we introduce the concept of the Agentic Digital Twin, a next-generation model that integrates FMs to bring multi-modal situational awareness, autonomy, proactivity, and social interaction into digital twin-based energy management systems. We present the transformational impact of FMs to each phase of a digital twin and identify the open challenges that need to be addressed for their efficient and effective integration.
ER  - 

TY  - JOUR
T1  - PageLLM: Incremental approach for updating a Security Knowledge Graph by using Page ranking and Large language model
AU  - Mishra, Chinmaya
AU  - Sarma, Himangshu
AU  - M., Saravanan
JO  - Information Processing & Management
VL  - 62
IS  - 3
SP  - 104045
PY  - 2025
DA  - 2025/05/01/
SN  - 0306-4573
DO  - https://doi.org/10.1016/j.ipm.2024.104045
UR  - https://www.sciencedirect.com/science/article/pii/S0306457324004047
KW  - Security knowledge graph
KW  - Knowledge graph
KW  - Knowledge representation learning
KW  - Page ranking
KW  - Embedding
KW  - Generative AI
KW  - Large language models (LLMs)
KW  - Static knowledge graph (SKG)
KW  - Incremental knowledge graph (IKG)
KW  - Full knowledge graph (FKG)
AB  - Due to increase in cyber crime and evolution of sophisticated tools and techniques, Threat Intelligence plays a critical role. It helps defenders to stay ahead of attackers by developing the right defense mechanism to invade those attacks. In this regards security knowledge graph plays a critical role which can be used to signify complex entities and their relationship in a graphical structure. Further projecting those entities and relationships in to the lower dimension using several embedding techniques such as TransE help in many down streaming task. The learned embedding can be used to predict new cyber threat which is very helpful for defenders to stay alert and develop necessary weapons to stay ahead of an attack. One of the major challenge security knowledge graph has its dynamic nature of changing intelligence. Active learning can be used to only update the substantial portion of embedding rather than retraining the knowledge graph from scratch which has higher time and space complexity. Also given the rise in generative AI and large language models which are super rich in context, there is a scope of utilizing those for building a robust and good quality security knowledge graph. We will discuss a novel methodology called PageLLM which utilizes page ranking and LLMs to enable active learning in an incremental way and will improve the quality of knowledge graph through enriched context.
ER  - 

TY  - JOUR
T1  - DecoyPot: A large language model-driven web API honeypot for realistic attacker engagement
AU  - Sezgin, Anıl
AU  - Boyacı, Aytuğ
JO  - Computers & Security
VL  - 154
SP  - 104458
PY  - 2025
DA  - 2025/07/01/
SN  - 0167-4048
DO  - https://doi.org/10.1016/j.cose.2025.104458
UR  - https://www.sciencedirect.com/science/article/pii/S0167404825001476
KW  - Artificial intelligence
KW  - Large language model
KW  - Cyber security
KW  - Honeypot
KW  - Decoy
KW  - Deception
AB  - As cyberattacks get more sophisticated, security systems must learn to detect and deceive them. DecoyPot, a honeypot Web Application Programming Interface (API) that generates legitimate API responses, is introduced in this paper. DecoyPot's command extractor module carefully analyzes API requests to create prompt-response pairs that improve a Retrieval-Augmented Generation based (RAG) large language model (LLM). DecoyPot can instantly adjust its answers to mimic API activity in a contextually correct and convincing manner to attackers. To assess system efficacy, we used a two-phase similarity analysis. Initial queries were matched with prompt-response pairs to ensure contextually suitable responses. Second, similarity measures were used to compare generated responses to reference responses, producing an average score of 0.9780. The high score shows that the system can create API-like responses, boosting its utility. DecoyPot engaged opponents and learned their Tactics, Techniques and Procedures (TTPs). The study shows that honeypot cybersecurity effectiveness must be improved by merging AI-driven response creation with enhanced deception technologies. DecoyPot effectively adapts to incoming queries and generates API-like responses, delivering actionable cyber threat intelligence and enhancing proactive defense strategies.
ER  - 

TY  - JOUR
T1  - Deobfuscation of JavaScript code and identification of security weaknesses through large language models
AU  - Benedetti, Giacomo
AU  - Caviglione, Luca
AU  - Comito, Carmela
AU  - Falcone, Alberto
AU  - Guarascio, Massimo
JO  - Future Generation Computer Systems
VL  - 179
SP  - 108318
PY  - 2026
DA  - 2026/06/01/
SN  - 0167-739X
DO  - https://doi.org/10.1016/j.future.2025.108318
UR  - https://www.sciencedirect.com/science/article/pii/S0167739X25006120
KW  - Code deobfuscation
KW  - Large language modelss
KW  - Cybersecurity
KW  - Threat analysis
AB  - Advancements in Large Language Models (LLMs) allow solving many challenging tasks related to software security in an automatic manner, e.g., the generation of test cases. An important aspect concerns the deobfuscation of source code, especially for improving its readability or preventing the elusion of signature-based countermeasures. Although LLMs are increasingly deployed to reveal the presence of malicious payloads within obfuscated software components, a comprehensive understanding of their potential and limitations is still missing. In this work, we evaluate the effectiveness of deobfuscating JavaScript code through an LLM-based pipeline. In more detail, we investigate whether LLMs can preserve structural properties of the software, especially to enhance the identification of weaknesses. Compared to two standard tools (i.e., JSNice and js-deobfuscator), our approach provides a more readable JavaScript prose according to several metrics, while retaining information on the Common Weaknesses Enumeration plaguing the software. To support the process of explaining issues within code, we performed tests on the use of two general-purpose LLMs, i.e., ChatGPT and Google Gemini. Results indicate that advancing the security of JavaScript through LLMs requires facing several challenges, which can be largely addressed via ad-hoc models.
ER  - 

TY  - JOUR
T1  - From COBIT to ISO 42001: Evaluating cybersecurity frameworks for opportunities, risks, and regulatory compliance in commercializing large language models
AU  - McIntosh, Timothy R.
AU  - Susnjak, Teo
AU  - Liu, Tong
AU  - Watters, Paul
AU  - Xu, Dan
AU  - Liu, Dongwei
AU  - Nowrozy, Raza
AU  - Halgamuge, Malka N.
JO  - Computers & Security
VL  - 144
SP  - 103964
PY  - 2024
DA  - 2024/09/01/
SN  - 0167-4048
DO  - https://doi.org/10.1016/j.cose.2024.103964
UR  - https://www.sciencedirect.com/science/article/pii/S0167404824002694
KW  - Cybersecurity frameworks
KW  - Large language models
KW  - Risk management
KW  - AI governance
KW  - Cyber resilience
KW  - Information security
AB  - This study investigated the integration readiness of four predominant cybersecurity Governance, Risk and Compliance (GRC) frameworks – NIST CSF 2.0, COBIT 2019, ISO 27001:2022, and the latest ISO 42001:2023 – for the opportunities, risks, and regulatory compliance when adopting Large Language Models (LLMs), using qualitative content analysis and expert validation. Our analysis, with both LLMs and human experts in the loop, uncovered potential for LLM integration together with inadequacies in LLM risk oversight of those frameworks. Comparative gap analysis has highlighted that the new ISO 42001:2023, specifically designed for Artificial Intelligence (AI) management systems, provided most comprehensive facilitation for LLM opportunities, whereas COBIT 2019 aligned most closely with the European Union AI Act. Nonetheless, our findings suggested that all evaluated frameworks would benefit from enhancements to more effectively and more comprehensively address the multifaceted risks associated with LLMs, indicating a critical and time-sensitive need for their continuous evolution. We propose integrating human-expert-in-the-loop validation processes as crucial for enhancing cybersecurity frameworks to support secure and compliant LLM integration, and discuss implications for the continuous evolution of cybersecurity GRC frameworks to support the secure integration of LLMs.
ER  - 

TY  - JOUR
T1  - Optimizing prompt efficacy in large language models for fake news detection via evolutionary algorithm-based feature selection
AU  - Wu, Lei
AU  - Yang, Xinran
AU  - Shi, Xiaochuan
AU  - Ma, Chao
JO  - Information Sciences
VL  - 720
SP  - 122539
PY  - 2025
DA  - 2025/12/01/
SN  - 0020-0255
DO  - https://doi.org/10.1016/j.ins.2025.122539
UR  - https://www.sciencedirect.com/science/article/pii/S0020025525006723
KW  - Artificial intelligence
KW  - LLM
KW  - Evolutionary learning
KW  - Fake news detection
KW  - Explainable AI
AB  - This article introduces an optimization framework based on a large language model, designed for detecting fake news and using an evolutionary algorithm to optimize prompt design. Our methodology systematically incorporates a broad array of features, including linguistic style, text complexity, and psychological factors, to facilitate a comprehensive and transparent analysis of news content, thereby enabling more explainable detection outcomes. Initially, the framework augments the training phase with a preprocessing component that utilizes robust estimation techniques to refine data evaluation and enhance outcome quality. Subsequently, an evolutionary algorithm refines the original feature set, significantly enhancing the accuracy of fake news detection models. In the final stage, this optimized subset of features is integrated with a large language model, facilitating precise authenticity assessments of news articles. Empirical evaluations on benchmark datasets confirm that our approach outperforms existing models by filtering out suitable feature sets that generate effective prompts.
ER  - 

TY  - JOUR
T1  - Evaluating Three Large Language Models for Security Incident Detection and Analysis
AU  - Mudzakir, Abdullah
AU  - Ativiirya Janaprasetya, Jip Tyrone
AU  - Kayowuan, Emanuel
AU  - Lewi Engel, Ventje Jeremias
AU  - Saputro, Bagaskoro
JO  - Procedia Computer Science
VL  - 269
SP  - 465
EP  - 473
PY  - 2025
DA  - 2025/01/01/
T2  - The 10th International Conference on Computer Science and Computational Intelligence 2025
SN  - 1877-0509
DO  - https://doi.org/10.1016/j.procs.2025.08.299
UR  - https://www.sciencedirect.com/science/article/pii/S1877050925026444
KW  - Large Language Model
KW  - Retrieval Augmented Generation
KW  - Digital Forensics
KW  - Incident Respoonse
KW  - Cosine Similarity
KW  - BERTScore
AB  - As modern cyberattacks become more complex, traditional security tools are often not enough, so there is a need for smarter tools to help with Digital Forensics and Incident Response (DFIR). This study addresses that need by comparing three 8-billion-parameter Large Language Models, such as Dolphin3 8B, LLaMA3.1 8B, and Qwen3 8B, to see how effective they are in helping with DFIR tasks. Using a setup that included real Apache2 attack logs and a special knowledge base of 133 documents from sources like NIST and OWASP, the models were tested through a Retrieval-Augmented Generation (RAG) system. Their performance on forensic tasks, such as identifying attack stages and extracting Indicators of Compromise (IOCs), was measured against a ground truth from GPT-4o using Cosine Similarity and BERTScore. The results clearly show that Qwen3 8B, when helped by the knowledge base, performed much better than the other models, receiving the highest average score of 0.7233. It also achieved the top scores for both Cosine Similarity (0.6128) and BERTScore (0.8338). In time, this research shows that while LLMs have great potential in cybersecurity, not all models benefit from extra knowledge, so there isn’t a single solution that works for every case. This highlights that choosing the right model is very important; a model like Qwen3 8B, when used correctly with a RAG system, can greatly improve the accuracy of forensic analysis, proving to be a valuable tool for Blue Team operations.
ER  - 

TY  - JOUR
T1  - Automated tactics planning for cyber attack and defense based on large language model agents
AU  - Ren, Yimo
AU  - Wang, Jinfa
AU  - Zhao, Zhihui
AU  - Wen, Hui
AU  - Li, Hong
AU  - Zhu, Hongsong
JO  - Neural Networks
VL  - 191
SP  - 107842
PY  - 2025
DA  - 2025/11/01/
SN  - 0893-6080
DO  - https://doi.org/10.1016/j.neunet.2025.107842
UR  - https://www.sciencedirect.com/science/article/pii/S0893608025007221
KW  - Cyber attack and defense
KW  - Tactics planning
KW  - Large language model
KW  - Reinforcement learning
AB  - In recent years, the complexity and frequency of cyber incidents have escalated, necessitating more advanced and automated solutions for all attackers and defenders in cybersecurity, while traditional methods cannot provide timely and effective tactics planning for attackers and defenders. At this situation, this paper explores the abilities of Large Language Models (LLMs) from the Reinforcement Learning (RL) perspective to achieve automated tactics planning for cyber attack and defense. By leveraging the natural language understanding and generation capabilities of popular LLMs, this paper aims to formulate and develop more complex tactics for the specific network. This paper constructs a publicly available simulation environment of cyber attack and defense, and proposes tactic agents based on LLMs used from the RL perspective. Then, this paper conducts nearly a thousand experiments on the constructed environment. Experimental results verify that the proposed tactic agents could significantly improve the effectiveness and adaptability of automated tactics planning for cyber attack and defense, offering a promising direction for future research in cybersecurity.
ER  - 

TY  - JOUR
T1  - PhishNet: A Real-Time, Scalable Ensemble Framework for Smishing Attack Detection Using Transformers and LLMs
AU  - Alhuzali, Abeer
AU  - Al-Qahtani, Qamar
AU  - Niyazi, Asmaa
AU  - Alshehri, Lama
AU  - Alharbi, Fatemah
JO  - Computers, Materials and Continua
VL  - 86
IS  - 1
SP  - 1
EP  - 19
PY  - 2025
DA  - 2025/11/10/
SN  - 1546-2218
DO  - https://doi.org/10.32604/cmc.2025.069491
UR  - https://www.sciencedirect.com/science/article/pii/S1546221825010902
KW  - Smishing attack detection
KW  - phishing attacks
KW  - ensemble learning
KW  - cybersecurity
KW  - deep learning
KW  - transformer-based models
KW  - large language models
AB  - The surge in smishing attacks underscores the urgent need for robust, real-time detection systems powered by advanced deep learning models. This paper introduces PhishNet, a novel ensemble learning framework that integrates transformer-based models (RoBERTa) and large language models (LLMs) (GPT-OSS 120B, LLaMA3.3 70B, and Qwen3 32B) to enhance smishing detection performance significantly. To mitigate class imbalance, we apply synthetic data augmentation using T5 and leverage various text preprocessing techniques. Our system employs a dual-layer voting mechanism: weighted majority voting among LLMs and a final ensemble vote to classify messages as ham, spam, or smishing. Experimental results show an average accuracy improvement from 96% to 98.5% compared to the best standalone transformer, and from 93% to 98.5% when compared to LLMs across datasets. Furthermore, we present a real-time, user-friendly application to operationalize our detection model for practical use. PhishNet demonstrates superior scalability, usability, and detection accuracy, filling critical gaps in current smishing detection methodologies.
ER  - 

TY  - JOUR
T1  - Assessing the effectiveness of crawlers and large language models in detecting adversarial hidden link threats in meta computing
AU  - Xiong, Junjie
AU  - Wei, Mingkui
AU  - Lu, Zhuo
AU  - Liu, Yao
JO  - High-Confidence Computing
VL  - 5
IS  - 3
SP  - 100292
PY  - 2025
DA  - 2025/09/01/
SN  - 2667-2952
DO  - https://doi.org/10.1016/j.hcc.2024.100292
UR  - https://www.sciencedirect.com/science/article/pii/S2667295224000953
KW  - Meta computing
KW  - Data integration
KW  - Adversary hidden link
KW  - Web crawling
KW  - Content deception detection
KW  - Large language model
AB  - In the emerging field of Meta Computing, where data collection and integration are essential components, the threat of adversary hidden link attacks poses a significant challenge to web crawlers. In this paper, we investigate the influence of these attacks on data collection by web crawlers, which famously elude conventional detection techniques using large language models (LLMs). Empirically, we find some vulnerabilities in the current crawler mechanisms and large language model detection, especially in code inspection, and propose enhancements that will help mitigate these weaknesses. Our assessment of real-world web pages reveals the prevalence and impact of adversary hidden link attacks, emphasizing the necessity for robust countermeasures. Furthermore, we introduce a mitigation framework that integrates element visual inspection techniques. Our evaluation demonstrates the framework’s efficacy in detecting and addressing these advanced cyber threats within the evolving landscape of Meta Computing.
ER  - 

TY  - JOUR
T1  - A novel system for strengthening security in large language models against hallucination and injection attacks with effective strategies
AU  - Gokcimen, Tunahan
AU  - Das, Bihter
JO  - Alexandria Engineering Journal
VL  - 123
SP  - 71
EP  - 90
PY  - 2025
DA  - 2025/06/01/
SN  - 1110-0168
DO  - https://doi.org/10.1016/j.aej.2025.03.030
UR  - https://www.sciencedirect.com/science/article/pii/S111001682500328X
KW  - Hallucination
KW  - Injection attacks
KW  - Large language models
KW  - Retrieval-augmented generation
KW  - Security
KW  - VectorDB
AB  - To address the escalating demand for secure and trustworthy interactions with Large Language Models (LLMs), this study introduces a pioneering security framework that mitigates critical vulnerabilities, including injection attacks, hallucinations, and data privacy breaches. By incorporating advanced technologies such as VectorDB, Kernel, and Retrieval-Augmented Generation (RAG) within a cross-LLM architecture, the system delivers enhanced resilience and adaptability to adversarial scenarios. Comprehensive evaluations across leading models—including PaLM, Llama, GPT-3.5, GPT-4, Gemini, and GPT-4o—reveal the system’s exceptional performance, achieving a 98 % accuracy in eligibility scoring and outperforming conventional models in both reliability and security. This study underscores the significance of a multi-layered defense mechanism that not only detects and neutralizes threats but also ensures ethical, accurate, and contextually relevant responses. The novel cross-LLM strategy enhances system robustness by leveraging the strengths of multiple models, minimizing inconsistencies and reinforcing output integrity. With its adaptability to emerging linguistic manipulation techniques and compliance with strict ethical standards, the proposed framework establishes a secure, scalable ecosystem for LLM applications. The findings promise transformative impacts across domains such as cybersecurity, multilingual processing, and adaptive threat detection, paving the way for safer and more reliable language model deployments.
ER  - 

TY  - JOUR
T1  - ChronoSentinel: Incremental temporal embedding for Security Knowledge Graph using Dynamic Reachability Centrality and Efficient language model
AU  - Mishra, Chinmaya
AU  - Sarma, Himangshu
AU  - M., Saravanan
JO  - Information Fusion
VL  - 127
SP  - 103662
PY  - 2026
DA  - 2026/03/01/
SN  - 1566-2535
DO  - https://doi.org/10.1016/j.inffus.2025.103662
UR  - https://www.sciencedirect.com/science/article/pii/S1566253525007341
KW  - Security Knowledge Graph (SKG)
KW  - Temporal security knowledge graph (TSKG)
KW  - Historical temporal security knowledge graph (TSKG-H)
KW  - New temporal security knowledge graph (TSKG-N)
KW  - Incremental temporal security knowledge graph (TSKG-NI)
KW  - Knowledge representation learning (KRL)
KW  - Dynamic Reachability Centrality (DRC)
KW  - Large language models (LLMs)
KW  - Efficient language models (ELMs)
AB  - The increasing sophistication of cyber threats requires adaptive, real-time defenses that can evolve with dynamic attack patterns. Security Knowledge Graphs (SKGs) have become essential for representing complex interrelationships among cyber entities, which are vital for combating ongoing cybercrime. However, most existing incremental update methods rely on non-temporal strategies that fail to capture the evolution of security data. This paper presents ChronoSentinel, an innovative framework that synergistically integrates Dynamic Reachability Centrality (DRC) with Efficient Language Models (ELMs) to offer a robust and scalable solution for maintaining and enhancing Temporal Security Knowledge Graphs. By incorporating temporal dynamics, ChronoSentinel incrementally updates the graph while reducing the computational cost of full retraining, leveraging time-sensitive information to respond to emerging threats. The framework employs DRC to prioritize influential and temporally critical core nodes, ensuring the graph remains up-to-date and responsive to evolving threat landscapes. Additionally, by integrating ELMs such as BART, FLAN-T5, and DeepSeek, ChronoSentinel enriches the graph with contextual insights that improve semantic representation and enable predictive link generation. This hybrid approach supports faster threat prediction and defense while maintaining reliability, accuracy, and low computational overhead.
ER  - 

TY  - JOUR
T1  - Heterogeneous data-driven resolution generation for software systems via large language models
AU  - Liu, Wen
AU  - Sun, Degang
AU  - Yang, Haitian
AU  - Wang, Yan
AU  - Huang, Weiqing
JO  - Information Processing & Management
VL  - 63
IS  - 2, Part A
SP  - 104376
PY  - 2026
DA  - 2026/03/01/
SN  - 0306-4573
DO  - https://doi.org/10.1016/j.ipm.2025.104376
UR  - https://www.sciencedirect.com/science/article/pii/S0306457325003176
KW  - Software system
KW  - Large language models
KW  - Resolution generation
KW  - Heterogeneous data
KW  - Anomaly-related knowledge base
AB  - Modern software systems are increasingly complex and dynamic, making them particularly vulnerable to performance anomalies. Although runtime anomaly detection enhances system reliability, engineers still devote considerable time and effort to resolving errors once anomalous logs or metrics are detected. Such challenges call for intelligent automation capable of delivering targeted remediation steps based on detected anomalies. In this work, we first construct an anomaly-related knowledge base by combining heterogeneous operational data, including logs and metrics, with resolutions annotated by domain experts. Furthermore, we propose HASolver, the first Heterogeneous Anomaly Solver to generate recommended resolutions for multi-source system anomalies. The core component is a dual-view multi-vector module, designed to represent heterogeneous anomaly chunks from different modalities and to support effective multi-vector retrieval. HASolver integrates a large language model with domain knowledge to generate mitigation resolutions. We conduct extensive experiments using BLEU and ROUGE-1/2/L metrics. Compared to baseline approaches, HASolver delivers notable performance gains, improving BLEU and ROUGE-L scores by 14.6% and 19.6%, respectively. Further analyses are carried out to explore various multi-vector configurations and the effect of prompt strategies. We also release the annotated resolution dataset derived from the anomaly-related knowledge base to facilitate future research.
ER  - 

TY  - JOUR
T1  - Enhancing network security using knowledge graphs and large language models for explainable threat detection
AU  - Belcastro, Loris
AU  - Carlucci, Carmine
AU  - Cosentino, Cristian
AU  - Liò, Pietro
AU  - Marozzo, Fabrizio
JO  - Future Generation Computer Systems
VL  - 176
SP  - 108160
PY  - 2026
DA  - 2026/03/01/
SN  - 0167-739X
DO  - https://doi.org/10.1016/j.future.2025.108160
UR  - https://www.sciencedirect.com/science/article/pii/S0167739X25004546
KW  - Intrusion detection
KW  - Security log analysis
KW  - Knowledge graphs
KW  - Explainable AI
KW  - Large language models
KW  - Generative AI
KW  - Anomaly detection
AB  - Ensuring robust cybersecurity in modern network environments is increasingly challenging due to the growing complexity and volume of network traffic data. Traditional detection systems often fail to identify stealthy and sophisticated attacks, such as Distributed Denial of Service (DDoS), ARP poisoning, and reconnaissance scans. Moreover, many existing methods lack transparency and produce reports that are difficult for analysts to interpret, slowing both threat comprehension and response. This paper addresses these challenges by introducing a novel methodology that integrates Knowledge Graphs, XAI techniques and Large Language Models (LLMs) to enhance network threat detection, classification, explainability, and automated reporting. The proposed approach employs Graph-BERT to encode complex communication patterns and semantic relationships into enriched knowledge graphs constructed from network logs. To ensure model transparency and interpretability, Local Interpretable Model-Agnostic Explanations (LIME) are incorporated, while structured prompts guide report generation using Generative AI. Experimental results obtained on benchmark datasets demonstrate that the methodology achieves a classification accuracy exceeding 84 %, outperforming existing detection techniques. Additionally, a comprehensive evaluation involving ablation analysis, LLM-based assessments, and expert reviews shows that incorporating structured knowledge and explainability significantly enhances the clarity, correctness, and informativeness of generated reports. These findings confirm the system’s effectiveness both as a detection mechanism and as a practical tool that helps analysts understand threats and craft informed responses.
ER  - 

TY  - JOUR
T1  - AI-enhanced intrusion detection in smart renewable energy grids: A novel industry 4.0 cyber threat management approach
AU  - Islam, Umar
AU  - Ullah, Hanif
AU  - Khan, Naveed
AU  - Saleem, Kashif
AU  - Ahmad, Iftikhar
JO  - International Journal of Critical Infrastructure Protection
VL  - 50
SP  - 100769
PY  - 2025
DA  - 2025/09/01/
SN  - 1874-5482
DO  - https://doi.org/10.1016/j.ijcip.2025.100769
UR  - https://www.sciencedirect.com/science/article/pii/S1874548225000307
KW  - AI-enhanced intrusion detection
KW  - Smart grids
KW  - Renewable energy
KW  - Industry 4.0
KW  - Cybersecurity
KW  - Machine learning
KW  - Anomaly detection
AB  - The rapid adoption of Industry 4.0 technologies in renewable energy grids has significantly improved efficiency and scalability. However, this integration has also amplified cybersecurity risks, making conventional Intrusion Detection Systems (IDS) insufficient against evolving cyber threats. This study proposes a novel AI-enhanced Intrusion Detection System (IDS) tailored for smart renewable energy grids, leveraging a multi-stage detection framework that integrates both supervised and unsupervised learning techniques. The proposed IDS combines Random Forest for signature-based detection and Autoencoders for anomaly-based threat identification, enabling real-time detection of both known and zero-day cyber threats. A comprehensive evaluation using real-world cyberattack datasets demonstrates that the system achieves a detection accuracy of 97.8 %, significantly reducing false positives compared to traditional IDS solutions. This work not only enhances the security and resilience of smart grids but also offers a scalable and adaptable cybersecurity framework for Industry 4.0 applications. The findings contribute to the advancement of AI-driven security mechanisms, ensuring the reliability of critical energy infrastructure in the face of sophisticated cyber threats.
ER  - 

TY  - JOUR
T1  - LLM-powered threat intelligence: Proactive detection of zero-day attacks in electric vehicle cyber-physical systems
AU  - Tirulo, Aschalew
AU  - Chauhan, Siddhartha
AU  - Shafie-khah, Miadreza
JO  - Sustainable Energy, Grids and Networks
VL  - 43
SP  - 101877
PY  - 2025
DA  - 2025/09/01/
SN  - 2352-4677
DO  - https://doi.org/10.1016/j.segan.2025.101877
UR  - https://www.sciencedirect.com/science/article/pii/S2352467725002590
KW  - Electric vehicles
KW  - Cyber-physical systems
KW  - Zero-day attacks
KW  - Generative AI
KW  - Large language models
KW  - Intrusion detection
KW  - Automotive cybersecurity
AB  - Context: Modern electric vehicles (EVs) are sophisticated cyber-physical systems (CPSs) comprising interconnected electronic control units (ECU), sensors, and communication interfaces. This connectivity enhances functionality while expanding the attack surface, exposing EVs to cyber threats. Problem: Zero-day attacks on EV systems pose severe risks by exploiting unknown vulnerabilities and bypassing traditional signature-based intrusion detection systems (IDS). Conventional detection mechanisms struggle with novel attack patterns and dynamic EV environments. Solution: We propose a generative AI-driven framework that leverages large language models (LLMs) to proactively identify zero-day attacks in EV ecosystems. The framework integrates generative threat intelligence with anomaly detection, enabling real-time synthesis and detection of emerging attack vectors. Methods: A curated EV threat intelligence corpus was used to fine-tune an LLM that monitors in-vehicle network traffic and sensor telemetry for anomalies. We evaluate detection capabilities through prompt-based zero-day exploit simulation. Results: The LLM-based system achieves high detection accuracy (>98 %) with significantly reduced false positives compared to classical IDS approaches. It successfully identified complex attack scenarios (charging infrastructure ransomware, sensor spoofing in autonomous systems) that evaded baseline methods. Case studies demonstrate real-time detection with automated mitigation recommendations. Contributions: (1) a novel EV cybersecurity architecture combining LLMs with generative attack simulation; (2) a comprehensive EV attack dataset including synthetic zero-days; (3) extensive evaluation against baseline IDS with performance, latency, and interpretability analysis; (4) insights on deploying generative AI for automotive cybersecurity aligned with industry standards. The approach provides proactive defense for next-generation vehicles.
ER  - 

TY  - JOUR
T1  - Improving online anti-phishing training using cognitive large language models
AU  - Malloy, Tailia
AU  - Fang, Fei
AU  - Gonzalez, Cleotilde
JO  - Computers in Human Behavior
VL  - 176
SP  - 108760
PY  - 2026
DA  - 2026/03/01/
SN  - 0747-5632
DO  - https://doi.org/10.1016/j.chb.2025.108760
UR  - https://www.sciencedirect.com/science/article/pii/S0747563225002079
KW  - Online training
KW  - Cybersecurity
KW  - Large Language Models
KW  - Cognitive models
AB  - Training the public to recognize phishing emails is challenging because attackers increasingly use Large Language Models (LLMs) to craft more convincing scams. Although previous research suggests that fully LLM-generated phishing emails are easier for humans to identify, recent models like GPT-4 used in tandem with prompt engineering methods could generate phishing emails that are more difficult to detect. Various training techniques have been used to educate end users about the risks of phishing emails, but these methods may not address the challenges posed by LLM-generated phishing emails. To investigate these challenges, we conducted an experiment using multiple different methods that cyber attackers can use to develop phishing emails. We found that the most challenging emails to detect were those written by humans and then edited and stylized using code generated by an LLM. We also created an anti-phishing training platform to study a personalized learning approach using cognitive models. The basic idea, borrowed from tutoring systems, is to use a cognitive model based on Instance-Based Learning (IBL) Theory to trace the student’s actions and predict each email’s classification decision. The training protocol then selects educational examples that maximize the student’s classification improvement. We also used the predictions of the IBL model to prompt the LLM to generate written feedback for the students. Our study shows that our training methods significantly improved the student’s classification decisions. These results are highly relevant to researchers interested in the social impacts of LLM, their misuse, and mitigation strategies and to practitioners interested in novel anti-phishing training techniques.
ER  - 

TY  - JOUR
T1  - ContextualGraph-LLM: A multimodal framework for enhanced Darknet traffic analysis
AU  - Hwang, Yujung
AU  - Kurt, Furkan
AU  - Curebal, Faruk
AU  - Keskin, Omer
AU  - Subasi, Abdulhamit
JO  - Expert Systems with Applications
VL  - 297
SP  - 129298
PY  - 2026
DA  - 2026/02/01/
SN  - 0957-4174
DO  - https://doi.org/10.1016/j.eswa.2025.129298
UR  - https://www.sciencedirect.com/science/article/pii/S0957417425029136
KW  - Intrusion detection
KW  - Darknet Traffic
KW  - Graph-Based Machine Learning
KW  - Lightweight Language Models
KW  - ContextualGraph-LLM
AB  - Darknet environments present a dual challenge for cybersecurity, and they offer privacy for legitimate users yet also serve as a hub for illicit activities. Traditional detection methods struggle to cope with the sophisticated tactics employed by threat actors in this space, necessitating innovative solutions that can uncover nuanced malicious behaviors. The primary objective of this work is to overcome the limitations of traditional detection methods against nuanced malicious behaviors in Darknet environments by introducing ContextualGraph-LLM (CG-LLM). This novel framework leverages the combined power of Graph Neural Networks (GNNs) and Large Language Models (LLMs) to enhance multi-label intrusion detection accuracy across various real-world network traffic scenarios, as demonstrated by its superior performance on benchmark datasets. This paper proposes CG-LLM, a novel framework that integrates Graph Neural Networks (GNNs) with Large Language Models (LLMs) to achieve robust multi-label intrusion detection. Our approach capitalizes on the CIC-Darknet2020 dataset, along with the ToN_IoT and UNSW-NB15 datasets, which cover a variety of real-world scenarios reflective of actual Darknet traffic and broader network threats. We first transform raw network flow records into a bipartite graph structure, where nodes correspond to distinct endpoints (source–destination pairs) and edges capture flow-level features such as packet sizes and transaction volumes. These graph-based embeddings are then enriched with textual node embeddings derived from TinyLlama. By pairing structural insights with contextual semantics, CG-LLM is poised to detect subtle malicious patterns often overlooked by more conventional, single-modality methods. Our experimental results reveal that CG-LLM consistently surpasses state of the arts studies in weighted average F1-score (0.9035), precision, (0.9368) and recall (0.8803) on the CIC-Darknet2020 dataset. Furthermore, it achieved impressive results on ToN_IoT (F1-score: 0.9712, precision: 0.9750, recall: 0.9714) and UNSW-NB15 (F1-score: 0.9797, precision: 0.9809, recall: 0.9802), highlighting its capacity to tackle data imbalance and underrepresented attack categories. Notably, the model excels in detecting hard-to-spot malicious behaviors, underscoring the effectiveness of fusing graph topology with linguistic context. While the incorporation of LLM embeddings elevates detection accuracy, it also introduces computational overhead, pointing to the importance of future research on scalability and optimization. In conclusion, this study demonstrates the potential of multimodal architectures that harness the complementary strengths of GNNs and LLMs, offering an adaptable and resilient framework for safeguarding networks against ever-evolving Darknet threats.
ER  - 

TY  - JOUR
T1  - Enhancing cyber risk identification in the construction industry using language models
AU  - Yao, Dongchi
AU  - García de Soto, Borja
JO  - Automation in Construction
VL  - 165
SP  - 105565
PY  - 2024
DA  - 2024/09/01/
SN  - 0926-5805
DO  - https://doi.org/10.1016/j.autcon.2024.105565
UR  - https://www.sciencedirect.com/science/article/pii/S0926580524003017
KW  - Cybersecurity
KW  - Risk identification
KW  - Deep learning
KW  - Language model
KW  - Construction industry
AB  - Modern construction projects are vulnerable to cyber-attacks due to insufficient attention to cybersecurity. Cyber risks in construction projects are not fully recognized, and the relevant literature is limited. To address this gap, the capabilities of a language model were leveraged to analyze extensive text, tailored to identify cyber risks. The model was trained using a curated corpus related to construction cybersecurity, enhanced by Supervised Fine-Tuning and Reinforcement Learning from Human Feedback techniques. The findings demonstrate advancements in the model's ability to understand cybersecurity and generate responses to cybersecurity questions. Using this model, a prioritized checklist of cyber risks across project phases was developed, establishing a new industry benchmark. This checklist can be utilized by various groups, including project managers and risk analysts. The model allows for updates with new data, ensuring the checklist remains current. The upgraded model holds significant promise for industry-wide applications, serving as an intelligent cybersecurity consultant.
ER  - 

TY  - JOUR
T1  - The Three Sides of the Moon LLMs in Cybersecurity: Guardians, Enablers and Targets
AU  - Marulli, Fiammetta
AU  - Paganini, Pierluigi
AU  - Lancellotti, Fabio
JO  - Procedia Computer Science
VL  - 246
SP  - 5340
EP  - 5348
PY  - 2024
DA  - 2024/01/01/
T2  - 28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)
SN  - 1877-0509
DO  - https://doi.org/10.1016/j.procs.2024.09.653
UR  - https://www.sciencedirect.com/science/article/pii/S187705092402708X
KW  - Large Language Models
KW  - Cyber-security
KW  - Generative Models Vulnerabilities
KW  - Prompt Injection
AB  - Large Language Models (LLMs) are rapidly evolving, demonstrating impressive capabilities in multimedia objects generation, ranging from text and image generation from scratch to programming code and efficient conversational agents. From the perspective of cyber-security challenges, LLMs and cyber-security are in a controversial relationship: it can be observed that LLMs, as a type of AI, play a mainfold role: that of security guardians, that of security breachs ”unaware” enablers and that of victims of cyber attacks. In fact, LLMs are able to enhance security of several tasks and applications but they are also attractive for malicious users to be exploited as means to perform novel attacks and, finally they represent challenging assets for targeting attacks. In this work, we discuss this mainfold key reading by providing a brief landscape of both the current defence applications of LLMs against cyber attacks and the currently known LLMs security vulnerabilities along with potential cyber-attacks targeting and involving LLMs. The final aim of study is intended to provide a guideline to further explore specific cyber-security scenarios involving LLMs.
ER  - 

TY  - JOUR
T1  - Leveraging MDS2 and SBOM data for LLM-assisted vulnerability analysis of medical devices
AU  - Stein, Stefan
AU  - Pilgermann, Michael
AU  - Weber, Simon
AU  - Sedlmayr, Martin
JO  - Computational and Structural Biotechnology Journal
VL  - 28
SP  - 267
EP  - 280
PY  - 2025
DA  - 2025/01/01/
SN  - 2001-0370
DO  - https://doi.org/10.1016/j.csbj.2025.07.012
UR  - https://www.sciencedirect.com/science/article/pii/S2001037025002788
KW  - Cyber security
KW  - IT security
KW  - Large Language Model (LLM)
KW  - Manufacturer disclosure statement for medical device security
KW  - MDS2
KW  - Medical cyber-physical systems
KW  - Medical device
KW  - Medical information system
KW  - Operational technology (OT) security
KW  - Vulnerabilities
AB  - This study investigated the use of a semi-automated, Retrieval-Augmented Generation (RAG)-based multi-agent architecture to analyze security-relevant data and assemble specialized exploitation paths targeting medical devices. The input dataset comprised device-specific sources, namely, the Manufacturer Disclosure Statement for Medical Device Security (MDS2) documents and Software Bills of Materials (SBOMs), enriched with public vulnerability databases, including Common Vulnerabilities and Exposures (CVE), Known Exploited Vulnerabilities (KEV), and Metasploit exploit records. The objective was to assess whether a modular, Large Language Model (LLM)-driven agent system could autonomously correlate device metadata with known vulnerabilities and existing exploit information to support structured threat modeling. The architecture follows a static RAG design based on predefined prompts and fixed retrieval logic, without autonomous agent planning or dynamic query adaptation. The developed Vulnerability Intelligence for Threat Analysis in Medical Security (VITAMedSec) system operates under human-prompted supervision and successfully synthesizes actionable insights and exploitation paths without requiring manual step-by-step input during execution. Although technically coherent results were obtained under controlled conditions, real-world validation remains a critical avenue for future research. This study further discusses the dual-use implications of such an agent-based framework, its relevance to patient safety in medical device cybersecurity, and the broader applicability of the proposed architecture to other critical infrastructure sectors. These findings emphasize both the technical potential and ethical responsibility for applying semi-automated AI workflows in medical cybersecurity contexts.
ER  - 

TY  - JOUR
T1  - An integrated IDS for the Internet of Vehicles using a Large Language Model framework
AU  - R., Aishwarya
AU  - V., Vetriselvi
AU  - Srinivas, Naveen
AU  - A., Ashwin Muthuraman
JO  - Internet of Things
VL  - 33
SP  - 101666
PY  - 2025
DA  - 2025/09/01/
SN  - 2542-6605
DO  - https://doi.org/10.1016/j.iot.2025.101666
UR  - https://www.sciencedirect.com/science/article/pii/S2542660525001805
KW  - Integrated intrusion detection system
KW  - Internet of Vehicle
KW  - VANET
KW  - Large Language Model
AB  - The Internet of Vehicles (IoV) has expanded through the integration of VANET (Vehicular Ad hoc Network) and IoT (Internet of Things) technologies within the Intelligent Transportation System domain, facilitated by the advancement of Beyond 5G communication technology. Recently, smart vehicles, such as connected vehicles, have become increasingly prevalent due to technological advancements. These vehicles engage in communication with other IoV components, rendering them susceptible to various attacks. Ensuring the security of connected vehicles is crucial to mitigate vulnerabilities within the IoV environment, as cyber–physical threats could pose life-threatening consequences. Therefore, anomaly and attack detection mechanisms are imperative to safeguard the IoV environment. This paper proposes a Generative AI-based Intelligent Integrated Intrusion Detection System tailored for IoV, considering multiple communication dimensions. Typically, attackers may target both the in-vehicle network such as CAN (Controller Area Network) BUS, and various external networks such as DSRC (Dedicated Short-Range Communication), CV2X (Cellular Vehicle-to-Everything) of smart connected vehicles. Thus, an Intrusion Detection System (IDS) focusing on multi-communication dimension attacks on smart vehicles is developed to enhance safety, thereby preventing collisions and chaos. The TON_IoT dataset and CICIoV2024 dataset are used in this study to assess the proposed approach for detecting intra and inter-vehicular network attacks. The proposed work achieves promising results, having a high accuracy of 98% with a 96% detection rate and a high F1 score of 0.97.
ER  - 

TY  - JOUR
T1  - LLM-LADE: Large language model-based log anomaly detection with explanation
AU  - Zhang, Zhiwei
AU  - Li, Saifei
AU  - Zhang, Lijie
AU  - Ye, Jianbin
AU  - Hu, Chunduo
AU  - Yan, Lianshan
JO  - Knowledge-Based Systems
VL  - 326
SP  - 114064
PY  - 2025
DA  - 2025/09/27/
SN  - 0950-7051
DO  - https://doi.org/10.1016/j.knosys.2025.114064
UR  - https://www.sciencedirect.com/science/article/pii/S0950705125011098
KW  - Log anomaly detection
KW  - Log analysis
KW  - Large language model
KW  - Fine-tuning
KW  - Online optimization
AB  - The increasing complexity of software systems and the exponential growth of log data have made anomaly detection and fault localization from massive logs a critical research area. However, existing log anomaly detection methods struggle to capture context-dependent semantic relationships present in unstructured logs and lack explainable decision-making mechanisms, substantially restricting their applicability in complex systems. The present study proposes LLM-LADE, a novel large language model (LLM)-based framework for log anomaly detection and explanation. LLM-LADE introduces a three-stage coordinated architecture encompassing ‘data augmentation - parameter-efficient fine-tuning (PEFT) - online optimization’, representing the first implementation of LLMs for simultaneous anomaly discrimination and explanation generation in log sequences. The framework overcomes the interpretability limitations of conventional approaches through a joint learning objective of label-cause pairs. An incrementally matched knowledge base mechanism is designed to enable zero-parameter-update online optimization via real-time capture of misjudged samples, ensuring continuous adaptability to dynamic logging environments. Experimental results on three benchmark datasets demonstrate that LLM-LADE outperforms current state-of-the-art algorithms in detection performance, with explanations generated exhibiting professional-grade usability in terms of both informativeness and readability.
ER  - 

TY  - JOUR
T1  - Integrative innovation of large language models in industries: technologies, applications, and challenges
AU  - Wang, Shikai
AU  - Shao, Yiwen
JO  - Data Science and Management
PY  - 2025
DA  - 2025/07/02/
SN  - 2666-7649
DO  - https://doi.org/10.1016/j.dsm.2025.06.005
UR  - https://www.sciencedirect.com/science/article/pii/S2666764925000323
KW  - Large Language Models (LLMs)
KW  - Natural Language Processing (NLP)
KW  - Human-Centric AI
KW  - Artificial General Intelligence (AGI)
AB  - This paper examines the transformative potential of large language models (LLMs) across diverse industries, emphasizing their ability to enhance natural language processing tasks through pre-training on extensive datasets. Although LLMs offer significant opportunities to automate customer interactions, improve decision-making, and optimize workflows, their rapid adoption also presents challenges such as information security, data quality, model interpretability, ethical implications, and regulatory compliance. To address these issues, this review proposes integrative strategies, including scenario-based applications, methodological innovation, and data-model integration, to boost LLMs’ performance and adaptability. It also explores the evolution of LLMs toward multimodal and multitask general-purpose models, highlighting future trends in sustainable development and human-centric AI. Future research directions include achieving a balance between enhancing model capabilities and managing energy consumption, as well as improving transparency and explainability to strengthen user trust.
ER  - 

TY  - JOUR
T1  - Automatic synthesis of econometric empirical research results using large language model: A case study of digitalization-greening relationships
AU  - Guo, Zitong
AU  - Yang, Guangfei
AU  - Wu, Wenjun
JO  - Technological Forecasting and Social Change
VL  - 222
SP  - 124370
PY  - 2026
DA  - 2026/01/01/
SN  - 0040-1625
DO  - https://doi.org/10.1016/j.techfore.2025.124370
UR  - https://www.sciencedirect.com/science/article/pii/S0040162525004019
KW  - Large language model
KW  - Econometrics
KW  - Systematic literature review
KW  - Prompt engineering
KW  - Neo4j
AB  - Econometrics is widely used in economics and social sciences research, generating abundant research results. While meta-analysis, systematic literature review (SLR) and bibliometrics are widely used in hotspot identification and knowledge synthesizing, deeper knowledge extraction, integration and utilization, a broader range of topics, a higher degree of automation and labor demands are expected. To maximize the utility of empirical findings and meet the above expectations, a Large Language Model (LLM)-assisted protocol is proposed in this paper. This protocol utilizes LLM to screen literature records, discriminate econometric empirical studies, and extract structured information from abstracts. Neo4j is used for storage, visualization, and subsequent utilization and mining of the extracted structured information. An empirical case of relationships between digitalization and greening is used to validate the feasibility of the protocol. The findings indicate the protocol accomplishes the verification of the mediating paths in existing research, the exploration of potential mediating paths, and the identification of collider factors, which offer valuable insights for future empirical studies related to digitalization-greening relationships. Future research directions of LLM-driven econometric modeling and knowledge hypernetworks construction via SLR methodologies are finally proposed based on this protocol.
ER  - 

TY  - JOUR
T1  - Cross-Model Evaluation of LLMs for Generating Formal Specification of Distributed Industrial Control Systems
AU  - Raptis, George E.
AU  - Khan, Muhammad Taimoor
AU  - Koulamas, Christos
AU  - Serpanos, Dimitrios
JO  - Procedia Computer Science
VL  - 270
SP  - 5846
EP  - 5854
PY  - 2025
DA  - 2025/01/01/
T2  - 29th International Conference on Knowledge-Based and Intelligent Information & Engineering Systems (KES 2025)
SN  - 1877-0509
DO  - https://doi.org/10.1016/j.procs.2025.10.053
UR  - https://www.sciencedirect.com/science/article/pii/S1877050925033861
KW  - inline security monitoring
KW  - industrial control systems
KW  - distributed ICS
KW  - water distribution systems
KW  - JML specifications
KW  - generative AI
KW  - large language models
KW  - cybersecurity
KW  - formal verification
KW  - runtime anomaly detection
AB  - The increasing complexity and decentralization of Industrial Control Systems (ICS) have expanded the attack surface for cyber-security threats, particularly in critical infrastructure domains. Inline monitoring using formal annotations like the Java Modeling Language (JML) offers a lightweight yet precise method to detect behavioral anomalies. However, the manual creation of such specifications is resource-intensive and requires domain expertise. This paper explores generative artificial intelligence (AI), specifically large language models (LLMs), to automate the synthesis of inline formal security monitors. We benchmark three state-of-the-art LLMs (GPT-4o, DeepSeek-V3, and Gemini 2.5 Flash) on their ability to generate JML annotations for ICS software drawn from the ASM2S water distribution system. Our evaluation across five dimensions (syntax, semantics, property coverage, alarm semantics, and effort savings) reveals distinct trade-offs. GPT-4o demonstrates strong syntactic and structural alignment with ASM2S, while DeepSeek-V3 offers richer behavioral modeling. Gemini 2.5 Flash showcases conceptual depth but introduces non-verifiable constructs. These findings demonstrate the potential of LLMs as co-pilots in secure-by-design ICS development and underscore the need for syntax-aware fine-tuning and interactive verification workflows.
ER  - 

TY  - JOUR
T1  - Enhancing password security with honeywords and LLMs
AU  - Umejiaku, Afamefuna P.
AU  - Sonde, Moroti
AU  - Sheng, Victor S.
JO  - Journal of Information Security and Applications
VL  - 93
SP  - 104129
PY  - 2025
DA  - 2025/09/01/
SN  - 2214-2126
DO  - https://doi.org/10.1016/j.jisa.2025.104129
UR  - https://www.sciencedirect.com/science/article/pii/S2214212625001668
KW  - Honeywords
KW  - Passwords security
KW  - Password strength meters
KW  - Large language models
KW  - Cyber security
KW  - Credential authentication
AB  - The increasing sophistication of cyber threats has amplified the need for innovative solutions to secure authentication systems. Honeywords, disguised credentials designed to detect unauthorized access, play a crucial role in cybersecurity by serving as early warning mechanisms. However, traditional honeyword generation methods often struggle with high false-positive and False-Negative Probability, limiting their effectiveness against advanced attackers. This study explores the integration of Large Language Models (LLMs) into password and honeyword generation systems. Leveraging LLMs’ natural language processing capabilities, we propose a novel framework for creating secure, user-friendly passwords and realistic honeywords. Our approach introduces multi-word, context-aware decoy generation, enhancing the indistinguishability of honeywords from genuine credentials. Empirical evaluations demonstrate significant improvements in performance metrics. Our model achieves a false negative probability (FNP(B)) of 0.33944, outperforming existing methods such as the Tweaking Path Model (0.54), the Deep Tweak Model (0.56), and the Chunk-Level GPT3 (0.58). Furthermore, it achieves a near-perfect false positive probability (FPP(A)) of <0.01, surpassing all compared algorithms. This research highlights the transformative potential of LLMs in enhancing authentication security. By addressing the limitations of traditional honeyword systems and introducing scalable, customizable solutions, this work contributes to the development of next-generation robust cybersecurity frameworks capable of countering evolving threats.
ER  - 

TY  - JOUR
T1  - Large Language Models for chatbot applications handling sensitive information
AU  - de Oliveira, Hilário Tomaz Alves
AU  - César Sobrinho, Álvaro Alvares De Carvalho
AU  - Dias, Andrey dos Reis Cadima
AU  - de Araújo, André Magno Costa
AU  - Araújo, Rafael Dias
AU  - Matos, Diego Dermeval Medeiros da Cunha
AU  - Munoz-Najar Galvez, Sebastian
JO  - Expert Systems with Applications
VL  - 299
SP  - 130145
PY  - 2026
DA  - 2026/03/01/
SN  - 0957-4174
DO  - https://doi.org/10.1016/j.eswa.2025.130145
UR  - https://www.sciencedirect.com/science/article/pii/S0957417425037601
KW  - RAG
KW  - Chatbot
KW  - LLMs
KW  - Accountability
KW  - Sensitive data
AB  - Recent advances in Large Language Models (LLMs) can enhance the performance of chatbot applications. However, LLMs rely on training data, which may limit their ability to answer questions based on up-to-date information. Moreover, many domains must address security and privacy threats to sensitive portions of this information. This paper presents an approach for designing high-performance and low-cost LLM-based chatbots that handle sensitive information. We used information retrieval techniques with the framework Rasa and integrated them into a retrieval-augmented generation architecture, leveraging low-cost online and offline LLMs. This setup enables up-to-date information and an additional layer of protection for internal documents, which may contain sensitive or proprietary content. Our case study considers an educational public policy in which we curated and annotated a dataset of 591 question–answer pairs written in Brazilian Portuguese extracted from institutional sources. We compared retrieval models, including BM25, E5 variants, OpenAI embeddings, MiniLM-L12-V2, and SBERT-MPNET, and the LLMs Sabiazinho-3 (specialized in Portuguese), Llama 3 (1B, 3B, 8B, and 70B), Gemma 3 (1B, 4B, and 12B), and DeepSeek-R1-Distill-Llama-70B. The evaluation relied on traditional metrics, such as ROUGE-L and BERTScore, as well as metrics computed using the LLM as a judge, including fidelity and answer relevance. The hybrid approach, integrating Sabiazinho-3 and Gemma 3 12B, achieved high performance, with a mean BERTScore of 0.758 for 96.672 % of questions answered. The results reflected substantial semantic similarity, factual consistency, correctness, and relevance to nearly all reference answers.
ER  - 

TY  - JOUR
T1  - Assessing the cybersecurity of connected 3D printers using large language models (LLMs)
AU  - Goh, Shi Yong
AU  - Mishra, Ankush
AU  - Govindarasu, Manimaran
AU  - Ganapathysubramanian, Baskar
AU  - Krishnamurthy, Adarsh
JO  - Manufacturing Letters
VL  - 44
SP  - 1187
EP  - 1197
PY  - 2025
DA  - 2025/08/01/
T2  - 53rd SME North American Manufacturing Research Conference (NAMRC 53)
SN  - 2213-8463
DO  - https://doi.org/10.1016/j.mfglet.2025.06.138
UR  - https://www.sciencedirect.com/science/article/pii/S2213846325001701
KW  - Cybersecurity
KW  - Large language model
KW  - Man-in-the-middle attack
KW  - Connected 3D printer
AB  - Additive manufacturing (AM) is transforming industries by enabling complex, cost-effective production, yet its integration with Industry 4.0 introduces significant cybersecurity vulnerabilities. This paper examines potential cyber threats in AM workflows, focusing on attacks that manipulate G-code instructions, such as Man-in-the-Middle (MITM) exploits, which could compromise the integrity of critical components. We introduce Large Language Models (LLMs) as a novel tool for detecting malicious alterations in G-code, showcasing their potential for real-time threat detection. Our findings underscore both the risks within AM and the promise of AI-driven cybersecurity solutions to protect evolving intelligent manufacturing systems.
ER  - 

TY  - JOUR
T1  - A comparative benchmark study of LLM-based threat elicitation tools
AU  - Van Landuyt, Dimitri
AU  - Mollaeefar, Majid
AU  - Raciti, Mario
AU  - Verreydt, Stef
AU  - Kalash, Abdulaziz
AU  - Bissoli, Andrea
AU  - Preuveneers, Davy
AU  - Bella, Giampaolo
AU  - Ranise, Silvio
JO  - Future Generation Computer Systems
VL  - 177
SP  - 108243
PY  - 2026
DA  - 2026/04/01/
SN  - 0167-739X
DO  - https://doi.org/10.1016/j.future.2025.108243
UR  - https://www.sciencedirect.com/science/article/pii/S0167739X25005370
KW  - Threat modeling
KW  - Threat elicitation
KW  - LLM Threat modeling tools
KW  - Empirical study
KW  - Semantic similarity
KW  - Threat modeling benchmark
AB  - Threat modeling refers to the software design activity that involves the proactive identification, evaluation, and mitigation of specific potential threat scenarios. Recently, attention has been growing for the potential to automate the threat elicitation process using Large Language Models (llms), and different tools have emerged that are capable of generating threats based on system models and other descriptive system documentation. This paper presents the outcomes of an experimental evaluation study of llm-based threat elicitation tools, which we apply to two complex and contemporary application cases that involve biometric authentication. The comparative benchmark is based on a grounded approach to establish four distinct baselines which are representative of the results of human threat modelers, both novices and experts. In support of scale and reproducibility, the evaluation approach itself is maximally automated using sentence transformer models to perform threat mapping. Our study evaluates 56 distinct threat models generated by 6 llm-based threat elicitation tools. While the generated threats are somewhat similar to the threats documented by human threats modelers, relative performance is low. The evaluated llm-based threat elicitation tools prove to be particularly inefficient in eliciting the threats on the expert level. Furthermore, we show that performance differences between these tools can be attributed on a similar level to both the prompting approach (e.g., multi-shot, knowledge pre-prompting, role prompting) and the actual reasoning capabilities of the underlying llms being used.
ER  - 

TY  - JOUR
T1  - LLM-TIKG: Threat intelligence knowledge graph construction utilizing large language model
AU  - Hu, Yuelin
AU  - Zou, Futai
AU  - Han, Jiajia
AU  - Sun, Xin
AU  - Wang, Yilei
JO  - Computers & Security
VL  - 145
SP  - 103999
PY  - 2024
DA  - 2024/10/01/
SN  - 0167-4048
DO  - https://doi.org/10.1016/j.cose.2024.103999
UR  - https://www.sciencedirect.com/science/article/pii/S0167404824003043
KW  - Threat intelligence
KW  - Large language model
KW  - Knowledge graph
KW  - TTP classification
AB  - Open-source threat intelligence is often unstructured and cannot be directly applied to the next detection and defense. By constructing a knowledge graph through open-source threat intelligence, we can better apply this information to intrusion detection. However, the current methods for constructing knowledge graphs face limitations due to the domain-specific attributes of entities and the analysis of lengthy texts, and they require large amounts of labeled data. Furthermore, there is a lack of authoritative open-source annotated threat intelligence datasets, which require significant manual effort. Moreover, it is noteworthy that current research often neglects the textual descriptions of attack behaviors, resulting in the loss of vital information to understand intricate cyber threats. To address these issues, we propose LLM-TIKG that applies the large language model to construct a knowledge graph from unstructured open-source threat intelligence. The few-shot learning capability of GPT is leveraged to achieve data annotation and augmentation, thereby creating the datasets for fine-tuning a smaller language model (7B). Using the fine-tuned model, we perform topic classification on the collected reports, extract entities and relationships, and extract TTPs from the attack description. This process results in the construction of a threat intelligence knowledge graph, enabling automated and universal analysis of textualized threat intelligence. The experimental results demonstrate improved performance in both named entity recognition and TTP classification, achieving the precision of 87.88% and 96.53%, respectively.
ER  - 

TY  - JOUR
T1  - DistilXIDS: Efficient, lightweight and explainable transformer-based language model for real-time network intrusion detection
AU  - Ajayan, Amal
AU  - Kirubavathi, G.
AU  - Sarker, Iqbal H.
JO  - Neurocomputing
VL  - 668
SP  - 132398
PY  - 2026
DA  - 2026/03/01/
SN  - 0925-2312
DO  - https://doi.org/10.1016/j.neucom.2025.132398
UR  - https://www.sciencedirect.com/science/article/pii/S092523122503070X
KW  - Network security
KW  - Transformers
KW  - Lightweight models
KW  - Security LLM
KW  - Fine tuning
KW  - DDoS
KW  - NIDS
AB  - The increasing complexity of network environments and the prevalence of sophisticated cyberattacks underscore the urgent need for advanced, efficient, and lightweight Intrusion Detection Systems (IDS). In response to this demand, we introduce DistilXIDS, a transformer-based model designed specifically for intrusion detection. DistilXIDS is built on DistilBERT, a distilled variant of Bidirectional Encoder Representations from Transformers (BERT), and is fine-tuned to identify Distributed Denial of Service (DDoS) attacks. Extensive experimentation on three widely recognized cybersecurity benchmark datasets, CIC-IDS2017, CIC-DDoS2019, and UNSW-NB15, highlights DistilXIDS’s outstanding classification performance. The model achieves accuracy, precision, recall, and F1-scores exceeding 99 %, with AUC-ROC scores surpassing 0.99. Importantly, it maintains low rates of false positives and false negatives, demonstrating its reliability for practical applications. In addition to classification capabilities, DistilXIDS features comprehensive real-time performance evaluations, assessing its deployability in high-throughput environments. The model exhibits low inference latency and high throughput alongside efficient GPU utilization. Scalability analyses reveal that optimal performance is achieved at moderate batch sizes, reinforcing the model’s suitability for real-time intrusion detection in operational settings. To investigate the impact of class imbalance, we first evaluated a baseline model on unbalanced data. This was followed by class-weighted fine-tuning and the application of the Synthetic Minority Oversampling Technique (SMOTE), both of which significantly enhanced recall for underrepresented attack samples. Furthermore, explainability is embedded within DistilXIDS through the use of SHapley Additive exPlanations (SHAP) and Local Interpretable Model-agnostic Explanations (LIME). This integration facilitates feature attribution and interpretability, thereby fostering analyst trust and operational transparency. In summary, these findings illustrate the adaptability of transformer-based architectures to structured network telemetry, paving the way for robust, interpretable, and efficient IDS solutions. DistilXIDS effectively addresses the balance among computational efficiency, real-time performance, interpretability, and predictive accuracy in cybersecurity applications.
ER  - 

TY  - JOUR
T1  - eX-NIDS: A framework for explainable network intrusion detection leveraging Large Language Models
AU  - Houssel, Paul R.B.
AU  - Layeghy, Siamak
AU  - Singh, Priyanka
AU  - Portmann, Marius
JO  - Computers and Electrical Engineering
VL  - 129
SP  - 110826
PY  - 2026
DA  - 2026/01/01/
SN  - 0045-7906
DO  - https://doi.org/10.1016/j.compeleceng.2025.110826
UR  - https://www.sciencedirect.com/science/article/pii/S0045790625007694
KW  - Network Intrusion Detection System
KW  - Large Language Model
KW  - Explainable AI
KW  - Prompt
KW  - Cyber Threat Intelligence
KW  - NetFlow
AB  - This paper introduces eX-NIDS, a framework designed to enhance interpretability in flow-based Network Intrusion Detection Systems (NIDS) by leveraging Large Language Models (LLMs). In our proposed framework, flows labelled as malicious by NIDS are initially processed through a module called the Prompt Augmenter. This module extracts contextual information and Cyber Threat Intelligence (CTI)-related knowledge from these flows. This enriched, context-specific data is then integrated with an input prompt for an LLM, enabling it to generate detailed explanations and interpretations of why the flow was identified as malicious by NIDS. We compare the generated interpretations against a Basic-Prompt Explainer baseline, which does not incorporate any contextual information into the LLM’s input prompt. Our framework is quantitatively evaluated using the Llama 3 and GPT-4 models, employing a novel evaluation method tailored for natural language explanations, focusing on their correctness and consistency. The results demonstrate that augmented LLMs can produce accurate and consistent explanations, serving as valuable complementary tools in NIDS to explain the classification of malicious flows. The use of augmented prompts enhances performance by over 20% compared to the Basic-Prompt Explainer.
ER  - 

TY  - JOUR
T1  - RLFE-IDS: A framework of Intrusion Detection System based on Retrieval Augmented Generation and Large Language Model
AU  - Li, Xuewei
AU  - Zheng, Zengyang
AU  - Zhao, Mankun
AU  - Zhao, Yue
AU  - Shi, Lifeng
AU  - Wang, Baoliang
JO  - Computer Networks
VL  - 268
SP  - 111341
PY  - 2025
DA  - 2025/08/01/
SN  - 1389-1286
DO  - https://doi.org/10.1016/j.comnet.2025.111341
UR  - https://www.sciencedirect.com/science/article/pii/S1389128625003081
KW  - Intrusion detection
KW  - Large Language Model
KW  - Retrieval-Augmented Generation
AB  - Intrusion Detection Systems (IDS) play a critical role in network security as a key defense measure, often struggle to effectively handle unknown attacks or variations of known attacks. This challenge is exacerbated by the poor generalization of deep learning models. To enhance the adaptability of IDS, this article introduces an innovative framework called LLM-IDS, which explores the feasibility of leveraging Large Language Model (LLMs) for intrusion detection, due to its strong generalization capabilities. However, there is a significant difficulty in deploying LLMs. Moreover, since most LLMs are primarily designed for Natural Language Processing (NLP) tasks, significant differences arise when naively adapting them to intrusion detection tasks. To address them, this article introduces a novel framework called RLFE-IDS, comprising two key modules: Retrieval-Augmented Generation (RAG) and an embedding model called FE-Net. RAG employs a vector database to store network data alongside their corresponding vector representations. Based on the RAG framework, LLMs can be directly called through an Application Programming Interface (API), alleviating the difficulties in its deployment. The embedding model FE-Net, bridges the semantic gap between text data and network data. Upon receiving new network data, RLFE-IDS employs RAG to query the database for the most relevant network data, which is then fed into the LLM to classify. This article validates approach through experiments on four datasets, and deploys RLFE-IDS into the real network environment. Experiments show that the optimal accuracy of LLM-IDS is 99.36%, and that of RLFE-Net is 98.56%. The results demonstrate not only the feasibility of applying LLMs to intrusion detection, but also the robustness and superior performance of RLFE-IDS.
ER  - 

TY  - JOUR
T1  - Leveraging Large Language Models for Malware Detection Through PE Header and Library Analysis
AU  - Tane, Matthew Maximillian
AU  - Wijaya, Nicholas Daniel
AU  - Hidayaturrahman, 
JO  - Procedia Computer Science
VL  - 269
SP  - 1043
EP  - 1056
PY  - 2025
DA  - 2025/01/01/
T2  - The 10th International Conference on Computer Science and Computational Intelligence 2025
SN  - 1877-0509
DO  - https://doi.org/10.1016/j.procs.2025.09.046
UR  - https://www.sciencedirect.com/science/article/pii/S1877050925027073
KW  - Malware detection
KW  - Large Language Model
KW  - Attention
KW  - Imbalanced dataset
KW  - Gradient-based attribution
AB  - In 2024, ransomware attacks reached unprecedented levels globally, with over 5,400 incidents reported. Malware attacks continue to cause critical damage, causing high business and organization losses. As time goes on, our reliance on Large Language Models for everyday tasks continues to grow due to its high speed and accuracy. In this research, we investigate the accuracy and effectiveness of Large Language Models as malware detectors by training 4 open-source models—BERT, GPT-2, TinyLlama and Qwen 2—to classify malware and benign samples on PE file headers and libraries extracted from an imbalanced combination of the BODMAS and Dike Dataset, where 98.35% of the samples are malware and 1.65% are benign. Bayesian optimization was used to find optimal parameters. TinyLlama performed best with an accuracy of 99.69% and a balanced accuracy of 91.69%. All models achieved accuracy and balanced accuracy greater than 99.6% and 90.3% respectively. TinyLlama, whose attention weights and gradients were analyzed, was able to recognize structural patterns and identify unusual headers and libraries within the input text. In general, the model relied more on libraries and structural patterns compared to the section headers. In this paper, we demonstrate large language models’ relatively high performance in detecting malware despite an imbalanced dataset.
ER  - 

TY  - JOUR
T1  - A survey of large language models for cyber threat detection
AU  - Chen, Yiren
AU  - Cui, Mengjiao
AU  - Wang, Ding
AU  - Cao, Yiyang
AU  - Yang, Peian
AU  - Jiang, Bo
AU  - Lu, Zhigang
AU  - Liu, Baoxu
JO  - Computers & Security
VL  - 145
SP  - 104016
PY  - 2024
DA  - 2024/10/01/
SN  - 0167-4048
DO  - https://doi.org/10.1016/j.cose.2024.104016
UR  - https://www.sciencedirect.com/science/article/pii/S0167404824003213
KW  - Large language models
KW  - Cyber security
KW  - Threat detection
KW  - Literature review
AB  - With the increasing complexity of cyber threats and the expanding scope of cyberspace, there exist progressively more challenges in cyber threat detection. It is proven that most previous threat detection models may become inadequate due to the escalation of hacker attacks. However, recent research has shown that some of these problems can be effectively addressed by Large Language Models (LLMs) directly or indirectly. Nowadays, a growing number of security researchers are adopting LLMs for analyzing various cyber threats. According to the investigation, we found that while there are numerous emerging reviews on the utilization of LLMs in some fields of cyber security, there is currently a lack of a comprehensive review on the application of LLMs in the threat detection stage. Through retrieving and collating existing works in recent years, we examined various threat detection and monitoring tasks for which LLMs may be well-suited, including cyber threat intelligence, phishing email detection, threat prediction, logs analysis, and so on. Additionally, the review explored the specific stages of different detection tasks in which LLMs are involved, evaluating the points at which LLMs are optimized. For instance, LLMs have been found to enhance the interpretability of log analysis in real-time anomaly event discovery. Additionally, we discussed some tasks where LLMs may not be suitable and explored future directions and challenges in this field. By providing a detailed status update and comprehensive insights, this review aims to assist security researchers in leveraging LLMs to enhance existing detection frameworks or develop domain-specific LLMs.
ER  - 

TY  - JOUR
T1  - Exploring large language models for indoor occupancy measurement in smart office buildings
AU  - Qaisar, Irfan
AU  - Sun, Kailai
AU  - Zhao, Qianchuan
JO  - Building and Environment
VL  - 287
SP  - 113860
PY  - 2026
DA  - 2026/01/01/
SN  - 0360-1323
DO  - https://doi.org/10.1016/j.buildenv.2025.113860
UR  - https://www.sciencedirect.com/science/article/pii/S0360132325013307
KW  - Energy saving
KW  - Occupancy detection and estimation
KW  - Artificial intelligence
KW  - Large language model
KW  - Office buildings
AB  - Accurately measuring building occupancy is essential for optimizing Heating, Ventilation, and Air Conditioning control and enhancing energy efficiency in smart buildings. However, existing machine learning models often struggle to generalize across diverse occupancy patterns with limited data. Recent advances in large language models present new opportunities by leveraging contextual reasoning and few-shot learning to enhance performance in smart building systems. This study proposes an LLM-based framework for real-time indoor occupancy measurement, incorporating few-shot learning, chain-of-thought reasoning, and in-context learning techniques. This study explores how LLMs can enable accurate and data-efficient occupancy measurement for indoor occupant-centric control and energy optimization. We evaluate LLMs’ performance against traditional models across two case studies: binary occupancy detection and multi-level occupancy estimation. Experiments are conducted using two real-world datasets collected from office buildings in China and Singapore. Results indicate that LLMs consistently outperform traditional models across various time intervals and training/testing configurations. Under a 4-day training/1-day testing setup, DeepSeek-R1 achieves 95.92% accuracy and a 96.1% F1-score, while Gemini-Pro attains 94.14% accuracy in multi-level estimation with only 1 day of training. An occupant-centric control (OCC) simulation and ablation study were implemented in EnergyPlus with real data to improve energy efficiency and comfort. These findings highlight the adaptability and robustness of LLMs, positioning them as promising tools for real-time occupancy measurement in smart office environments. Code and implementation details are available at: https://github.com/kailaisun/LLM-occupancy.
ER  - 

TY  - JOUR
T1  - Assessing LLMs in malicious code deobfuscation of real-world malware campaigns
AU  - Patsakis, Constantinos
AU  - Casino, Fran
AU  - Lykousas, Nikolaos
JO  - Expert Systems with Applications
VL  - 256
SP  - 124912
PY  - 2024
DA  - 2024/12/05/
SN  - 0957-4174
DO  - https://doi.org/10.1016/j.eswa.2024.124912
UR  - https://www.sciencedirect.com/science/article/pii/S0957417424017792
KW  - Malware analysis
KW  - Code deobfuscation
KW  - Large language models
KW  - Cybersecurity
AB  - The integration of large language models (LLMs) into various cybersecurity pipelines has become increasingly prevalent, enabling the automation of numerous manual tasks and often surpassing human performance. Recognising this potential, cybersecurity researchers and practitioners are actively investigating the application of LLMs to process vast volumes of heterogeneous data for anomaly detection, potential bypass identification, attack mitigation, and fraud prevention. Moreover, LLMs’ advanced capabilities in generating functional code, interpreting code context, and code summarisation present significant opportunities for reverse engineering and malware deobfuscation. In this work, we comprehensively examine the deobfuscation capabilities of state-of-the-art LLMs. Specifically, we conducted a detailed evaluation of four prominent LLMs using real-world malicious scripts from the notorious Emotet malware campaign. Our findings reveal that while current LLMs are not yet perfectly accurate, they demonstrate substantial potential in efficiently deobfuscating payloads. This study highlights the importance of fine-tuning LLMs for specialised tasks, suggesting that such optimisation could pave the way for future AI-powered threat intelligence pipelines to combat obfuscated malware. Our contributions include a thorough analysis of LLM performance in malware deobfuscation, identifying strengths and limitations, and discussing the potential for integrating LLMs into cybersecurity frameworks for enhanced threat detection and mitigation. Our experiments illustrate that LLMs can automatically and accurately extract the necessary indicators of compromise from a real-world campaign with an accuracy of 69.56% and 88.78% for the URLs and the corresponding domains of the droppers, respectively.
ER  - 

TY  - JOUR
T1  - Uncovering the risks of digital supply chains: A large language model framework for semantic identification and validation
AU  - Fang, Jiaqi
AU  - Su, Bixiang
AU  - Wang, Shuzhen
AU  - Wang, Bin
JO  - International Journal of Production Economics
VL  - 291
SP  - 109858
PY  - 2026
DA  - 2026/01/01/
SN  - 0925-5273
DO  - https://doi.org/10.1016/j.ijpe.2025.109858
UR  - https://www.sciencedirect.com/science/article/pii/S0925527325003433
KW  - LLMs
KW  - Supply chains risk
KW  - Digital supply chain transformation
KW  - Generative AI
AB  - This study proposes a semantic framework powered by Large Language Models (LLMs) to detect and quantify latent, technology-induced risks embedded in unstructured corporate disclosures under ongoing digital transformation. While existing methods often fail to capture key supply chain risks such as external shocks, operational disruptions, and digital system failures, structured prompts guide GPT-based models to extract standardized and interpretable risk indicators. These indicators are validated through panel regressions and behavioral inventory simulations using the classical (s, S) policy. Empirical results show that LLM-derived measures explain 10 %–15 % more variance in firm-level market volatility, digital investment, and inventory decisions than traditional Bag-of-Words (BoW) methods. External risks significantly predict stock return volatility (β = 0.173) and international revenue share (β = −0.477), while digital risks are associated with higher management costs and increased patent output. Simulation results show that inventory buffers expand and total inventory costs rise by up to 18 % under elevated risk exposure. The findings provide a theoretically grounded and practically viable framework for semantic risk identification, advancing intelligent supply chain analytics.
ER  - 

TY  - JOUR
T1  - Security concerns for Large Language Models: A survey
AU  - Li, Miles Q.
AU  - Fung, Benjamin C.M.
JO  - Journal of Information Security and Applications
VL  - 95
SP  - 104284
PY  - 2025
DA  - 2025/12/01/
SN  - 2214-2126
DO  - https://doi.org/10.1016/j.jisa.2025.104284
UR  - https://www.sciencedirect.com/science/article/pii/S2214212625003217
KW  - Large language models
KW  - Adversarial attacks
KW  - Data poisoning
KW  - AI safety
KW  - Agentic risks
AB  - Large Language Models (LLMs) such as ChatGPT and its competitors have caused a revolution in natural language processing, but their capabilities also introduce new security vulnerabilities. This survey provides a comprehensive overview of these emerging concerns, categorizing threats into several key areas: inference-time attacks via prompt manipulation; training-time attacks; misuse by malicious actors; and the inherent risks in autonomous LLM agents. Recently, a significant focus is increasingly being placed on the latter. We summarize recent academic and industrial studies from 2022 to 2025 that exemplify each threat, analyze existing defense mechanisms and their limitations, and identify open challenges in securing LLM-based applications. We conclude by emphasizing the importance of advancing robust, multi-layered security strategies to ensure LLMs are safe and beneficial.
ER  - 

TY  - JOUR
T1  - Enhancing large language model for fake news video detection via cross-modal retrieval
AU  - Han, Linfeng
AU  - Zhang, Xiaoming
AU  - Wang, Tianbo
AU  - Liu, Yun
AU  - Dong, Zhiqiang
JO  - Information Processing & Management
VL  - 63
IS  - 2, Part B
SP  - 104471
PY  - 2026
DA  - 2026/03/01/
SN  - 0306-4573
DO  - https://doi.org/10.1016/j.ipm.2025.104471
UR  - https://www.sciencedirect.com/science/article/pii/S0306457325004121
KW  - Multimodal
KW  - Fake news video
KW  - Large language model
KW  - Prompt learning
KW  - Cross-modal knowledge
AB  - Fake news video detection aims to analyze and verify the authenticity of video-based news content using multimodal data, including visual, audio, and textual cues. Fake news videos typically contain subtle, misleading alterations, often limited to specific frames, or feature genuine video coupled with fabricated narratives, complicating authenticity assessment through video content alone. Existing approaches predominantly depend on intrinsic features within the video or utilize external knowledge derived from a single modality, limiting their capability to exploit multi-source cross-modal information effectively and rendering them vulnerable to substantial content noise. To address these issues, we propose an Enhanced Method for Fake Video Detection based on Cross-modal Retrieval utilizing Large Models (FVDLM). We first collect relevant video and text news to augment external knowledge. Given the sparsity of informative features in video content, the information bottleneck theory is employed to denoise irrelevant information. Furthermore, to effectively integrate cross-modal knowledge and enrich external context, we introduce a prompt learning approach utilizing large models to generate contextual knowledge. Three specialized prompts are crafted to assess video authenticity from multiple viewpoints. Comprehensive experiments validate the effectiveness and superiority of our proposed model.
ER  - 

TY  - JOUR
T1  - Unknown web attack threat detection based on large language model
AU  - Xu, Yijia
AU  - Zhang, Qiang
AU  - Deng, Huaxin
AU  - Liu, Zhonglin
AU  - Yang, Cheng
AU  - Fang, Yong
JO  - Applied Soft Computing
VL  - 173
SP  - 112905
PY  - 2025
DA  - 2025/04/01/
SN  - 1568-4946
DO  - https://doi.org/10.1016/j.asoc.2025.112905
UR  - https://www.sciencedirect.com/science/article/pii/S1568494625002169
KW  - Unknown threat
KW  - Abnormal behavior
KW  - Graph contrastive learning
KW  - Large language model
KW  - Causal analysis
AB  - Unknown attacks pose a significant threat to current cyber defenses. Traditional methods for detecting abnormal user behaviors rely on explicit associations and content information, often overlooking implicit causal relationships. Additionally, the frequent emergence of new attack types and the scarcity of training data limit their effectiveness. The paper proposes a novel approach for detecting abnormal user behaviors using large language models (LLMs), addressing these challenges under low-resource conditions. Our method extracts implicit causal relationships from system logs to build behavior graphs and employs label-free graph contrastive invariant learning to generate causal feature vectors. A multi-agent framework, including narrator and decision-maker agents, is used to improve descriptive text generation, while the Translator more efficiently converts causal vectors into meaningful descriptions. Experimental results on the WAB-dataset demonstrate that implicit causal relationships enhance the graph structure’s ability to represent abnormal behaviors. The integration of LLMs enables superior behavior analysis with fewer resources compared to traditional methods. Additionally, the comprehensibility of the generated texts and the efficiency of the Translator provide a strong foundation for supporting security professionals in understanding and analyzing abnormal behaviors in real-world scenarios.
ER  - 

TY  - JOUR
T1  - Large language model-assisted digital twin for remote monitoring and control of advanced reactors
AU  - Ndum, Zavier Ndum
AU  - Lim, Doyeong
AU  - Ford, John
AU  - Adu, Simon
AU  - Tao, Jian
AU  - Hassan, Yassin
AU  - Liu, Yang
JO  - Progress in Nuclear Energy
VL  - 192
SP  - 106172
PY  - 2026
DA  - 2026/02/01/
SN  - 0149-1970
DO  - https://doi.org/10.1016/j.pnucene.2025.106172
UR  - https://www.sciencedirect.com/science/article/pii/S0149197025005700
KW  - Digital twin
KW  - AI-driven control
KW  - Intelligent control
KW  - LLM agent
KW  - Reference governor
AB  - In this paper, we present a novel framework for real-time autonomous monitoring and control of advanced small-scale nuclear reactors by integrating a simulator-based digital twin (DT) with a domain-knowledge-enhanced large language model (LLM). The framework’s architecture, featuring robust bi-directional connectivity via the OPC-UA industrial protocol, was demonstrated on a lead-cooled fast reactor (LFR) case study. The LFR demonstration leverages a customized, MATLAB-Simulink-based simulator with natural circulation and burnup modeling capabilities, which has been validated against the System Analysis Module (SAM) code. The system employs a hybrid supervisory methodology that incorporates both a traditional rate-limiting Reference Governor (rl-RG) and a novel, tool-centric LLM agent. The LLM agent’s primary capability involves processing incoming sensor data streams, invoking the simulator and related customized tools to obtain physics-based results, and interpreting this data to understand the current system status. Augmented with custom-built retrieval and diagnostic subroutines, the LLM agent then explains the reactor’s state and generates context-aware, quantitative recommendations for management actions, such as reactivity control and power maneuvers. We found that the control signals recommended by the LLM agent successfully matched those generated by the traditional rl-RG. Furthermore, the framework includes an intuitive, multi-tab graphical user interface that enhances human-machine interaction by enabling real-time transient visualization, interactive LLM queries, and detailed transient analysis. This research highlights the transformative potential of AI-enhanced, DT-based control frameworks for advanced reactors and demonstrates its utility as a robust educational and training tool.
ER  - 

TY  - JOUR
T1  - volGPT: Evaluation on triaging ransomware process in memory forensics with Large Language Model
AU  - Oh, Dong Bin
AU  - Kim, Donghyun
AU  - Kim, Donghyun
AU  - Kim, Huy Kang
JO  - Forensic Science International: Digital Investigation
VL  - 49
SP  - 301756
PY  - 2024
DA  - 2024/07/01/
T2  - DFRWS USA 2024 - Selected Papers from the 24th Annual Digital Forensics Research Conference USA
SN  - 2666-2817
DO  - https://doi.org/10.1016/j.fsidi.2024.301756
UR  - https://www.sciencedirect.com/science/article/pii/S2666281724000751
KW  - digital forensics
KW  - Memory forensics
KW  - Ransomware
KW  - Volatility
KW  - ChatGPT
KW  - Large language model (LLM)
AB  - In the face of the harm that ransomware can inflict upon users’ computers, the imperative to efficiently and accurately triage its processes within memory forensics becomes increasingly crucial. However, ransomware perpetrators employ sophisticated techniques, such as process masquerading, to evade detection and analysis. In response to these challenges, we propose a novel ransomware triage method leveraging a Large Language Model (LLM) in conjunction with the Volatility framework, the de-facto standard in memory forensics. We conducted experiments on memory dumps infected by five different ransomware families, utilizing LLM-based approaches. Through extensive experiments, our method named volGPT demonstrated high accuracy in identifying ransomware-related processes within memory dumps. Additionally, our approach exhibited greater efficiency and provided more comprehensive explanations during ransomware triage than other state-of-the-art methods.
ER  - 

TY  - JOUR
T1  - AI for cyber-security risk: harnessing AI for automatic generation of company-specific cybersecurity risk profiles
AU  - Schreiber, Amir
AU  - Schreiber, Ilan
JO  - Information and Computer Security
VL  - 33
IS  - 4
SP  - 520
EP  - 546
PY  - 2025
DA  - 2025/02/17/
SN  - 2056-4961
DO  - https://doi.org/10.1108/ICS-08-2024-0177
UR  - https://www.sciencedirect.com/science/article/pii/S2056496125000066
KW  - Cybersecurity risks
KW  - AI
KW  - Gen-AI
KW  - Generative AI
KW  - AI risks
AB  - Purpose
In the modern digital realm, artificial intelligence (AI) technologies create unprecedented opportunities and enhance tactical security operations. This study aims to address the gap in using AI to strategically produce holistic cybersecurity risk profiles.
Design/methodology/approach
This paper uses a rigorous AI-powered method to conduct cybersecurity risk profiles tailored to individual enterprises, investigating sources of threat and guiding defense strategies. This paper built a real working demo application based on real security databases and used it to build company-specific cybersecurity risk profiles.
Findings
This paper demonstrated a robust, automated process for developing tailored cybersecurity risk profiles in three case studies across different industries. The AI application produced coherent outputs, validated by experts as accurate.
Research limitations/implications
This study lays the groundwork for further research, allowing for refinement by integrating additional resources, such as near-real-time alerts from external or internal sources.
Practical implications
The escalating threat landscape highlights the need for organizations to adopt AI for cybersecurity management, leveraging tools that assist in defining and refining cybersecurity risk profiles to enhance defense measures.
Social implications
Using AI-generated cybersecurity risk profiles supports efforts to create a safer digital environment for organizations, their employees and their customers, aligning with the growing reliance on AI in daily life.
Originality/value
Unlike most papers, this paper uses an AI application to address contemporary challenges in creating holistic, non-tactical profiles that can be refined and contextualized by the organizations while achieving automation in key processes and integrating multiple resources.
ER  - 

TY  - JOUR
T1  - AECR: Automatic attack technique intelligence extraction based on fine-tuned large language model
AU  - Chen, Minghao
AU  - Zhu, Kaijie
AU  - Lu, Bin
AU  - Li, Ding
AU  - Yuan, Qingjun
AU  - Zhu, Yuefei
JO  - Computers & Security
VL  - 150
SP  - 104213
PY  - 2025
DA  - 2025/03/01/
SN  - 0167-4048
DO  - https://doi.org/10.1016/j.cose.2024.104213
UR  - https://www.sciencedirect.com/science/article/pii/S0167404824005194
KW  - Cyber threat intelligence (CTI)
KW  - Attack technique extraction
KW  - Prompt engineering
KW  - Large language model (LLM)
KW  - Advanced persistent threat (APT)
AB  - Cyber Threat Intelligence (CTI) reports contain resourceful intelligence on cyber-attack campaigns, which provides great help for security analysts to infer attack trends and enhance their defenses. However, due to the diversity of report content and writing styles, current intelligence extraction is mostly based on time-consuming manual efforts. Moreover, existing automatic methods generally neglect the importance of background knowledge and produce inexact extraction results. These problems prevent the effective utilization and sharing of intelligence from CTI reports. In this paper, we primarily focus on the automatic extraction of attack technique (AT) intelligence, which reveals patterns of attack behaviors and hardly changes over time. We propose a novel automatic AT extraction pipeline for CTI reports (AECR). AECR explores the feasibility of extracting AT intelligence based on a fined-tuned large language model (LLM). Particularly, we endow the selected LLM with enhanced domain-specific knowledge to improve its comprehension of AT-relevant content and alleviate the hallucination problem. Experimental results demonstrate that AECR outperforms state-of-the-art methods by a wide margin with a reasonable time cost. Specifically, we improve the accuracy, precision, recall, and F1-score by 108%, 37.2%, 22.4%, and 67.5% respectively. To the best of our knowledge, AECR is the first to perform AT extraction based on fine-tuned LLM.
ER  - 

TY  - JOUR
T1  - Red teaming large language models: A comprehensive review and critical analysis
AU  - Jabbar, Muhammad Shahid
AU  - Al-Azani, Sadam
AU  - Alotaibi, Abrar
AU  - Ahmed, Moataz
JO  - Information Processing & Management
VL  - 62
IS  - 6
SP  - 104239
PY  - 2025
DA  - 2025/11/01/
SN  - 0306-4573
DO  - https://doi.org/10.1016/j.ipm.2025.104239
UR  - https://www.sciencedirect.com/science/article/pii/S0306457325001803
KW  - Large language models (LLMs)
KW  - AI security
KW  - Red teaming attacks
KW  - Red teaming evaluation
KW  - Trustworthy AI
KW  - Model exploitation
AB  - Securing large language models (LLMs) remains a critical challenge as their adoption across various sectors rapidly grows. While advancements in LLM development have enhanced their capabilities, inherent vulnerabilities continue to pose significant risks, exposing these models to various forms of attack. This study provides a comprehensive review of LLMs’ red teaming, distinguished by its broad coverage and intuitive organization. It systematically explores a range of red teaming attacks, including prompt-based attacks, data manipulation attacks, model exploitation attacks, information extraction attacks, and model degradation attacks. Additionally, it provides a critical review and analysis of evaluation methods and benchmarks, focusing on datasets, evaluation metrics, and benchmarking techniques used in LLM red teaming and risk assessment. Our review reflects the current state of LLM security and provides new insights alongside established methods by integrating recent and impactful research. The structured presentation of our findings offers a comprehensive and actionable resource, facilitating a deeper understanding of the complexities involved. This review highlights the proactive assessment of risk and exploitation potential, and contributes to the development of more secure and responsible LLMs, serving as a valuable guide for researchers, practitioners, and policymakers.
ER  - 

TY  - JOUR
T1  - Transforming cybersecurity with agentic AI to combat emerging cyber threats
AU  - Kshetri, Nir
JO  - Telecommunications Policy
VL  - 49
IS  - 6
SP  - 102976
PY  - 2025
DA  - 2025/07/01/
SN  - 0308-5961
DO  - https://doi.org/10.1016/j.telpol.2025.102976
UR  - https://www.sciencedirect.com/science/article/pii/S0308596125000734
AB  - This paper investigates the transformative potential of agentic AI in cybersecurity, specifically addressing how it can enhance practices in response to emerging threats. It aims to explore how agentic AI can transform cybersecurity practices, particularly in addressing new and evolving threats, while also examining the cybersecurity risks associated with its integration. The research explores the possibilities for agentic AI to automate critical tasks within Security Operations Centers (SOCs), such as decision-making, incident response, and threat detection. It also emphasizes the risks associated with AI integration, including the introduction of new vulnerabilities and challenges in managing automated systems, which call for a reassessment of existing cybersecurity frameworks to effectively address these risks.
ER  - 

TY  - JOUR
T1  - The machines are watching: Exploring the potential of Large Language Models for detecting Algorithmically Generated Domains
AU  - Pelayo-Benedet, Tomás
AU  - Rodríguez, Ricardo J.
AU  - Gañán, Carlos H.
JO  - Journal of Information Security and Applications
VL  - 93
SP  - 104176
PY  - 2025
DA  - 2025/09/01/
SN  - 2214-2126
DO  - https://doi.org/10.1016/j.jisa.2025.104176
UR  - https://www.sciencedirect.com/science/article/pii/S2214212625002133
KW  - Large Language Models
KW  - Algorithmically Generated Domains
KW  - DNS traffic analysis
KW  - Malware detection
AB  - Algorithmically Generated Domains (AGDs) are integral to many modern malware campaigns, allowing adversaries to establish resilient command and control channels. While machine learning techniques are increasingly employed to detect AGDs, the potential of Large Language Models (LLMs) in this domain remains largely underexplored. In this paper, we examine the ability of nine commercial LLMs to identify malicious AGDs, without parameter tuning or domain-specific training. We evaluate zero-shot approaches and few-shot learning approaches, using minimal labeled examples and diverse datasets with multiple prompt strategies. Our results show that certain LLMs can achieve detection accuracy between 77.3% and 89.3%. In a 10-shot classification setting, the largest models excel at distinguishing between malware families, particularly those employing hash-based generation schemes, underscoring the promise of LLMs for advanced threat detection. However, significant limitations arise when these models encounter real-world DNS traffic. Performance degradation on benign but structurally suspect domains highlights the risk of false positives in operational environments. This shortcoming has real-world consequences for security practitioners, given the need to avoid erroneous domain blocking that disrupt legitimate services. Our findings underscore the practicality of LLM-driven AGD detection, while emphasizing key areas where future research is needed (such as more robust warning design and model refinement) to ensure reliability in production environments.
ER  - 

TY  - JOUR
T1  - Evaluating various Finetuned LLMs for Cybersecurity Named Entity Recognition
AU  - G, Shanmukha Aditya
AU  - B, Kruthika
AU  - G, Venkata Sneha
AU  - Gupta, Deepa
AU  - Srivastava, Smita
JO  - Procedia Computer Science
VL  - 258
SP  - 1933
EP  - 1943
PY  - 2025
DA  - 2025/01/01/
T2  - International Conference on Machine Learning and Data Engineering
SN  - 1877-0509
DO  - https://doi.org/10.1016/j.procs.2025.04.444
UR  - https://www.sciencedirect.com/science/article/pii/S1877050925015479
KW  - BART
KW  - Cybersecurity
KW  - Deep Learning Techniques
KW  - FLAN T5
KW  - Large Language Models
KW  - Mistral
KW  - Named Entity Recognition
AB  - In an era dominated by escalating cybersecurity threats, the rapid and accurate identification of named entities within textual data is crucial. Named Entity Recognition (NER) serves as a cornerstone in this endeavor, facilitating the extraction of vital information from unstructured sources. Our study explores the development of a robust NER model tailored for cybersecurity text analysis, leveraging advanced deep learning techniques and state-of-the-art Large Language Models (LLMs). Utilizing an English Cyber Security NER dataset with 24 tags, we interrogate the performance of diverse fine-tuned LLMs, including Mistral 7B, BART 406M, FLAN T5 Small 580M, and FLAN T5 Base 2.1B. By evaluating key metrics such as accuracy, precision, recall, and F1-score, we assess each model’s efficacy in domain-specific entity recognition. Our research demonstrates an achieved F1 score of approximately 74% after 100 training epochs, highlighting the potential of these models in enhancing cybersecurity defenses. The comparative analysis of these fine-tuned LLMs provides insights into their respective strengths in tackling cybersecurity NER tasks, contributing to the ongoing efforts in identifying and mitigating threats effectively.
ER  - 

TY  - JOUR
T1  - “Digital Camouflage”: The LLVM challenge in LLM-based malware detection
AU  - Böke, Ekin
AU  - Torka, Simon
JO  - Journal of Systems and Software
VL  - 231
SP  - 112646
PY  - 2026
DA  - 2026/01/01/
SN  - 0164-1212
DO  - https://doi.org/10.1016/j.jss.2025.112646
UR  - https://www.sciencedirect.com/science/article/pii/S0164121225003152
KW  - Malware detection
KW  - Large Language Models
KW  - Cybersecurity
KW  - LLVM
AB  - Large Language Models (LLMs) have emerged as promising tools for malware detection by analyzing code semantics, identifying vulnerabilities, and adapting to evolving threats. However, their reliability under adversarial compiler-level obfuscation is yet to be discovered. In this study, we empirically evaluate the robustness of three state-of-the-art LLMs: ChatGPT-4o, Gemini Flash 2.5, and Claude Sonnet 4 against compiler-level obfuscation techniques implemented via the LLVM infrastructure. These include control flow flattening, bogus control flow injection, instruction substitution, and split basic blocks, which are widely used to evade detection while preserving malicious behavior. We perform a structured evaluation on 40 C functions (20 vulnerable, 20 secure) sourced from the Devign dataset and obfuscated using LLVM passes. Our results show that these models often fail to correctly classify obfuscated code, with precision, recall, and F1-score dropping significantly after transformation. This reveals a critical limitation: LLMs, despite their language understanding capabilities, can be easily misled by compiler-based obfuscation strategies. To promote reproducibility, we release all evaluation scripts, prompts, and obfuscated code samples in a public repository. We also discuss the implications of these findings for adversarial threat modeling, and outline future directions such as software watermarking, compiler-aware defenses, and obfuscation-resilient model design.
ER  - 

TY  - JOUR
T1  - Betastack: Enhancing base station traffic prediction with network-specific Large Language Models
AU  - Lv, Quanfeng
AU  - Chang, Yuan
AU  - Li, Tong
AU  - Ge, Jingguo
JO  - Computer Networks
VL  - 270
SP  - 111557
PY  - 2025
DA  - 2025/10/01/
SN  - 1389-1286
DO  - https://doi.org/10.1016/j.comnet.2025.111557
UR  - https://www.sciencedirect.com/science/article/pii/S1389128625005249
KW  - Large Language Models in networking
KW  - Network traffic prediction
KW  - Base station network
AB  - Accurate traffic forecasting in base station networks is crucial for efficient network management, resource allocation, and ensuring quality of service. This paper introduces BetaStack, a novel network-specific Large Language Model (LLM) designed to enhance base station traffic prediction. Unlike existing approaches, BetaStack incorporates physical constraints and a specialized network protocol embedding layer that captures the hierarchical structure of network traffic data. Through fine-tuning with these network-specific adaptations and a self-regressive prediction mechanism, BetaStack effectively leverages the powerful sequence modeling capabilities of LLMs to address the intricacies of network traffic. Extensive experiments on real-world data from base station cells in Guangdong, China demonstrate that BetaStack achieves significant performance improvements over both state-of-the-art time-series forecasting models and specialized network traffic prediction models. These results underscore the potential of BetaStack to improve the accuracy of network traffic prediction, enabling more efficient network management. The code can be found in https://github.com/lqf0624/BetaStack.git.
ER  - 

TY  - JOUR
T1  - Can LLM-generated misinformation be detected: A study on Cyber Threat Intelligence
AU  - Huang, He
AU  - Sun, Nan
AU  - Tani, Massimiliano
AU  - Zhang, Yu
AU  - Jiang, Jiaojiao
AU  - Jha, Sanjay
JO  - Future Generation Computer Systems
VL  - 173
SP  - 107877
PY  - 2025
DA  - 2025/12/01/
SN  - 0167-739X
DO  - https://doi.org/10.1016/j.future.2025.107877
UR  - https://www.sciencedirect.com/science/article/pii/S0167739X25001724
KW  - Cyber security
KW  - Artificial intelligence
KW  - Human-centric
AB  - Given the increasing number and severity of cyber attacks, there has been a surge in cybersecurity information across various mediums such as posts, news articles, reports, and other resources. Cyber Threat Intelligence (CTI) involves processing data from these cybersecurity sources, enabling professionals and organizations to gain valuable insights. However, with the rapid dissemination of cybersecurity information, the inclusion of fake CTI can lead to severe consequences, including data poisoning attacks. To address this challenge, we have implemented a three-step strategy: generating synthetic CTI, evaluating the quality of the generated CTI, and detecting fake CTI. Unlike other subdomains, such as fake COVID news detection, there is currently no publicly available dataset specifically tailored for fake CTI detection research. To address this gap, we first establish a reliable groundtruth dataset by utilizing domain-specific cybersecurity data to fine-tune a Large Language Model (LLM) for synthetic CTI generation. We then employ crowdsourcing techniques and advanced synthetic data verification methods to evaluate the quality of the generated dataset, introducing a novel evaluation methodology that combines quantitative and qualitative approaches. Our comprehensive evaluation reveals that the generated CTI cannot be distinguished from genuine CTI by human annotators, regardless of their computer science background, demonstrating the effectiveness of our generation approach. We benchmark various misinformation detection techniques against our groundtruth dataset to establish baseline performance metrics for identifying fake CTI. By leveraging existing techniques and adapting them to the context of fake CTI detection, we provide a foundation for future research in this critical field. To facilitate further research, we make our code, dataset, and experimental results publicly available on GitHub.
ER  - 

TY  - JOUR
T1  - Optimising AI models for intelligence extraction in the life cycle of Cybersecurity Threat Landscape generation
AU  - Zacharis, Alexandros
AU  - Gavrila, Razvan
AU  - Patsakis, Constantinos
AU  - Douligeris, Christos
JO  - Journal of Information Security and Applications
VL  - 90
SP  - 104037
PY  - 2025
DA  - 2025/05/01/
SN  - 2214-2126
DO  - https://doi.org/10.1016/j.jisa.2025.104037
UR  - https://www.sciencedirect.com/science/article/pii/S2214212625000754
KW  - Machine learning
KW  - Cybersecurity Threat Landscape
KW  - Threat intelligence
KW  - Named entity recognition
KW  - Large language model
AB  - The increasing complexity and frequency of cyber attacks in the modern digital environment demand continuous vigilance and proactive strategies to manage risks effectively. Conventional approaches to generating intelligence for Cybersecurity Threat Landscape (CTL) reports are often resource-intensive and time-consuming, as they depend on manual identification, collection, and analysis of relevant electronically stored information (ESI). This study investigates the potential of artificial intelligence (AI) to transform CTL generation, reducing manual classification and tagging while improving efficiency and accuracy. We focus on evaluating the classification performance of several Large Language Models (LLMs), including Gemini 1.5 Pro, GPT-4o, but also Bidirectional Encoder Representations from Transformers (BERT) based models like TRAM and TTPHunter along with custom Named Entity Recognition (NER) models, using a dataset previously annotated by human experts. Our findings demonstrate the promising results of AI-driven intelligence extraction for CTL report generation, streamlining cybersecurity operations by automating routine tasks and providing precise and timely threat intelligence. However, the variability in model performance suggests the importance of hybrid approaches needed to achieve the accuracy of human annotation. Therefore, we propose a novel voting agreement-based methodology, harvesting the most from the combined AI model capabilities to effectively address the complexities of cybersecurity threat intelligence extraction.
ER  - 

TY  - JOUR
T1  - Making waves: A conceptual framework exploring how large language model-based multi-agent systems could reshape water engineering
AU  - Hosseini, Seyed Hossein
AU  - Zolghadr-Asli, Babak
AU  - Tenkanen, Henrikki
AU  - Madani, Kaveh
AU  - Matin, Mir A.
AU  - Demir, Ibrahim
AU  - Ostfeld, Avi
AU  - Singh, Vijay P.
AU  - Savic, Dragan
JO  - Water Research
VL  - 291
SP  - 125157
PY  - 2026
DA  - 2026/03/01/
SN  - 0043-1354
DO  - https://doi.org/10.1016/j.watres.2025.125157
UR  - https://www.sciencedirect.com/science/article/pii/S0043135425020603
KW  - Large language model-based multi-agent (LLM-MA)
KW  - Water engineering
KW  - Decision support systems
KW  - Adaptive AI systems
KW  - Generative AI
AB  - Large Language Model-based Multi-Agents (LLM-MAs) are emerging systems that manage complex tasks with specialized and coordinated agents. In this paper, we present new perspectives on the integration of LLM-MA systems into enhancing water engineering practices. Water engineering typically involves data integration, analysis, modeling, decision-making, and cross-disciplinary collaboration, which often present significant difficulties. To address these domain-specific complexities, we explore how LLM-MA systems can support advanced operations in water engineering and facilitate them. By pointing out the linguistic capabilities of LLMs and the modular, scalable, and collaborative architecture of LLM-MA systems, we investigate the role of intelligent agents in enabling timely, adaptive, and traceable solutions. Various practical applications were identified, e.g., LLM-MA for pressure drop detection in water distribution networks, flood management, or in their role as potential negotiating agents to find a balanced solution considering differing goals. Our investigation highlights both the capabilities and limitations of LLM-MAs in water engineering and proposes practical recommendations for their effective implementation within the field. This study seeks to develop a foundational framework for understanding how LLM-MAs can shape the future of water engineering processes.
ER  - 

TY  - JOUR
T1  - Backdoor samples detection based on perturbation discrepancy consistency in pre-trained language models
AU  - Peng, Zuquan
AU  - Fu, Jianming
AU  - Zou, Lixin
AU  - Zheng, Li
AU  - Ren, Yanzhen
AU  - Peng, Guojun
JO  - Neural Networks
VL  - 193
SP  - 108025
PY  - 2026
DA  - 2026/01/01/
SN  - 0893-6080
DO  - https://doi.org/10.1016/j.neunet.2025.108025
UR  - https://www.sciencedirect.com/science/article/pii/S0893608025009050
KW  - Backdoor attacks
KW  - Backdoor samples detection
KW  - Pre-trained language models
KW  - Black-box
AB  - The use of unvetted third-party and internet data renders pre-trained models susceptible to backdoor attacks. Detecting backdoor samples is critical to prevent backdoor activation during inference or injection during training. However, existing detection methods often require the defender to have access to the poisoned models, extra clean samples, or significant computational resources to detect backdoor samples, limiting their practicality. To address this limitation, we propose a backdoor sample detection method based on perturbatioN discrEpancy consisTency Evaluation (NETE). This is a novel detection method that can be used both pre-training and post-training phases. In the detection process, it only requires an off-the-shelf pre-trained model to compute the log probability of samples and an automated function based on a mask-filling strategy to generate perturbations. Our method is based on the interesting phenomenon that the change in perturbation discrepancy for backdoor samples is smaller than that for clean samples. Based on this phenomenon, we use curvature to measure the discrepancy in log probabilities between different perturbed samples and input samples, thereby evaluating the consistency of the perturbation discrepancy to determine whether the input sample is a backdoor sample. Experiments conducted on four typical backdoor attacks and five types of large language model backdoor attacks demonstrate that our detection strategy outperforms existing zero-shot black-box detection methods.
ER  - 

TY  - JOUR
T1  - A Text Classification Approach to Enhancing Cyber Threat Detection
AU  - Căpîlnaş, Matei-Vasile
AU  - Coroiu, Adriana Mihaela
JO  - Procedia Computer Science
VL  - 270
SP  - 1689
EP  - 1695
PY  - 2025
DA  - 2025/01/01/
T2  - 29th International Conference on Knowledge-Based and Intelligent Information & Engineering Systems (KES 2025)
SN  - 1877-0509
DO  - https://doi.org/10.1016/j.procs.2025.09.289
UR  - https://www.sciencedirect.com/science/article/pii/S187705092502962X
KW  - text clustering
KW  - cybersecurity
KW  - LLM
AB  - The number of cyberattacks has increased at an alarming rate in recent years, and cybersecurity is becoming a priority for many companies. This article aims to present a solution based on text classification for managing information related to cyber attacks and to facilitate the information and education of both regular users and IT specialists. In the experiments that will be described, a dataset containing articles related to the field of cybersecurity extracted from the Hacker News platform was used. The obtained results can serve as a starting point in the development of a Large Language Model (LLM) system to facilitate the search and dissemination of informational content.
ER  - 

TY  - JOUR
T1  - SecLMNER: A framework for enhanced named entity recognition in multi-source cybersecurity data using large language models
AU  - Zhang, Yunlong
AU  - Liu, Jingju
AU  - Zhong, Xiaofeng
AU  - Wu, Lei
JO  - Expert Systems with Applications
VL  - 271
SP  - 126651
PY  - 2025
DA  - 2025/05/01/
SN  - 0957-4174
DO  - https://doi.org/10.1016/j.eswa.2025.126651
UR  - https://www.sciencedirect.com/science/article/pii/S0957417425002738
KW  - Cybersecurity
KW  - Named entity recognition
KW  - Large language models
KW  - Open-source intelligence
AB  - In the realm of cybersecurity, Named Entity Recognition (NER) has predominantly centered on cyber threat intelligence. However, cybersecurity-related information often resides in open-source intelligence and unprocessed tool outputs. With structured, semi-structured, and unstructured text data coexisting, traditional BERT-based NER methods encounter challenges in handling diverse data formats and technical terminology richness. To tackle these obstacles, this paper presents a framework that integrates large language models for NER in multi-source cybersecurity data. Leveraging decoder-based large language models’ generative capabilities, the framework intelligently crafts natural language segments containing data information for enhanced adaptability. Subsequently, the SecureBERT model, excelling as the premier open-source solution for NER in the network security domain, leverages a pre-trained encoder tailored for cybersecurity text to proficiently detect diverse entities and associated information embedded within natural language segments. The experimental findings demonstrate that our proposed method outperforms traditional BERT and its variants in fine-tuning across five distinct cybersecurity text data sources. The results reveal that by integrating three generative language models, each with parameters under 10 billion, we achieve an improvement in Recall ranging from 8.1% to 21.81% over the state-of-the-art open-source SecureBERT, and an increase in F1 score from 6.19% to 16.7%. Moreover, with the enhancements to our framework, the NER performance can match that of open-source large-parameter language models.
ER  - 

TY  - JOUR
T1  - Utilizing Fine-Tuning of Large Language Models for Generating Synthetic Payloads: Enhancing Web Application Cybersecurity through Innovative Penetration Testing Techniques
AU  - Ćirković, Stefan
AU  - Mladenović, Vladimir
AU  - Tomić, Siniša
AU  - Drljača, Dalibor
AU  - Ristić, Olga
JO  - Computers, Materials and Continua
VL  - 82
IS  - 3
SP  - 4409
EP  - 4430
PY  - 2025
DA  - 2025/03/06/
SN  - 1546-2218
DO  - https://doi.org/10.32604/cmc.2025.059696
UR  - https://www.sciencedirect.com/science/article/pii/S1546221825002401
KW  - LLM
KW  - GPT-2
KW  - XSS
KW  - SQL injection
KW  - command injection
KW  - evaluation loss perplexity
AB  - With the increasing use of web applications, challenges in the field of cybersecurity are becoming more complex. This paper explores the application of fine-tuned large language models (LLMs) for the automatic generation of synthetic attacks, including XSS (Cross-Site Scripting), SQL Injections, and Command Injections. A web application has been developed that allows penetration testers to quickly generate high-quality payloads without the need for in-depth knowledge of artificial intelligence. The fine-tuned language model demonstrates the capability to produce synthetic payloads that closely resemble real-world attacks. This approach not only improves the model’s precision and dependability but also serves as a practical resource for cybersecurity professionals to enhance the security of web applications. The methodology and structured implementation underscore the importance and potential of advanced language models in cybersecurity, illustrating their effectiveness in generating high-quality synthetic data for penetration testing purposes. The research results demonstrate that this approach enables the identification of vulnerabilities that traditional methods may not uncover, providing deeper insights into potential threats and enhancing overall security measures. The performance evaluation of the model indicated satisfactory results, while further hyperparameter optimization could improve accuracy and generalization capabilities. This research represents a significant step forward in improving web application security and opens new opportunities for the use of LLMs in security testing, thereby contributing to the development of more effective cybersecurity strategies.
ER  - 

TY  - JOUR
T1  - Securing large language models: A quantitative assurance framework approach
AU  - Stamnes Karlsen, Sander
AU  - Yamin, Muhammad Mudassar
AU  - Hashmi, Ehtesham
AU  - Katt, Basel
AU  - Ullah, Mohib
JO  - Journal of Information Security and Applications
VL  - 97
SP  - 104351
PY  - 2026
DA  - 2026/03/01/
SN  - 2214-2126
DO  - https://doi.org/10.1016/j.jisa.2025.104351
UR  - https://www.sciencedirect.com/science/article/pii/S2214212625003874
KW  - Large language models
KW  - Security assurance framework
KW  - Quantitative security assessment
KW  - LLM vulnerabilities
KW  - Security architecture
AB  - Large Language Models (LLMs) are increasingly integrated into sensitive domains such as healthcare and autonomous systems, yet adoption is constrained by security risks that conventional assurance methods do not capture. Traditional software assurance techniques are inadequate for LLM-specific vulnerabilities, including prompt injection, insecure output handling, and training data poisoning. We introduce a quantitative security assurance framework for LLM applications that translates security requirements and vulnerabilities into measurable scores. The framework computes an Assurance Metric (AM) as AM=RM−VM, where VM is weighted using CVSS v4.0, and maps results to five security assurance levels, making security posture comparable, auditable, and actionable. Requirements span input/output validation, training data, development and deployment, access control, third-party services, and security procedures; vulnerability tests align with the OWASP Top 10 for LLMs (prompt injection, insecure output handling, training data poisoning, denial of service, sensitive information disclosure, overreliance, and model theft). Case study results show uncensored models (e.g., Llama2-uncensored) exhibit significantly higher exposure, especially to prompt injection and output-handling attacks–while censored and fine-tuned models attain higher assurance levels. Significance and impact: the framework provides transparent, quantitative scoring to compare systems, prioritize mitigations, and support evidence-based deployment and governance in high-takes environments, with continuous human oversight emphasized.
ER  - 
