TY  - JOUR
T1  - Large language models for conceptual modeling: Assessment and application potential
AU  - Storey, Veda C.
AU  - Pastor, Oscar
AU  - Guizzardi, Giancarlo
AU  - Liddle, Stephen W.
AU  - Maaß, Wolfgang
AU  - Parsons, Jeffrey
AU  - Ralyté, Jolita
AU  - Santos, Maribel Yasmina
JO  - Data & Knowledge Engineering
VL  - 160
SP  - 102480
PY  - 2025
DA  - 2025/11/01/
SN  - 0169-023X
DO  - https://doi.org/10.1016/j.datak.2025.102480
UR  - https://www.sciencedirect.com/science/article/pii/S0169023X25000758
KW  - Conceptual modeling
KW  - Large language models
KW  - Artificial intelligence
KW  - Semantics
AB  - Large Language Models (LLMs) are being rapidly adopted for many activities in organizations, business, and education. Included in their applications are capabilities to generate text, code, and models. This leads to questions about their potential role in the conceptual modeling part of information systems development. This paper reports on a panel presented at the 43rd International Conference on Conceptual Modeling where researchers discussed the current and potential role of LLMs in conceptual modeling. The panelists discussed applications and interest levels and expressed both optimism and caution in the adoption of LLMs. Suggested is a need for much continued research by the conceptual modeling community on LLM development and their role in research and teaching.
ER  - 

TY  - JOUR
T1  - Traffic scene perception via multimodal large language model with data augmentation and efficient training strategy
AU  - Liu, Shuo
AU  - Shi, Lei
AU  - Shi, Yucheng
AU  - Gao, Yufei
AU  - Sun, Xiaole
JO  - Applied Soft Computing
VL  - 177
SP  - 113210
PY  - 2025
DA  - 2025/06/01/
SN  - 1568-4946
DO  - https://doi.org/10.1016/j.asoc.2025.113210
UR  - https://www.sciencedirect.com/science/article/pii/S1568494625005216
KW  - Traffic scenes understanding and reasoning
KW  - Multimodal large language model
KW  - Staged training strategy
AB  - Intelligent mobility, driven by advancements in deep learning and computing power, enhances transportation efficiency and societal connectivity, fostering economic and urban development. Current computer vision solutions often struggle to capture the complex details or understand the context within traffic scenes, limiting advanced intelligent mobility and raising safety concerns. Multimodal Large Language Models (MLLMs), by integrating linguistic and visual data, can aid vehicles and transportation systems in gaining a deeper understanding of the real-world traffic scenes, offering solutions to current challenges. Nevertheless, existing approaches predominantly employ MLLMs as instruments for querying and engaging with traffic infrastructure, rather than empowering these models to genuinely comprehend the traffic environment. This limitation curtails the potential of MLLMs and may even pose safety risks. In this paper, we first introduce a data augmentation framework designed to transform raw data into datasets suited for specific training objectives, thereby addressing issues related to data scarcity. Secondly, we propose a learning rate-based staged training strategy that segments the training process into distinct stages. This strategy involves deploying datasets targeted at various training objectives according to the patterns of parameter changes observed in different stages, thereby enhancing the training efficiency of the model. Utilizing these methods, we present InsightGPT, a model endowed with robust understanding and reasoning capabilities in traffic scenarios. In experiments conducted across six tasks, InsightGPT consistently outperforms baseline MLLMs in evaluating both the overall traffic scenes and individual objects within it, demonstrating its superior traffic comprehension and reasoning abilities. InsightGPT’s parameters and deployment details are available at https://github.com/JinleLiu/InsightGPT.
ER  - 

TY  - JOUR
T1  - Longitudinal abuse and sentiment analysis of Hollywood movie dialogues using language models
AU  - Chandra, Rohitash
AU  - Ren, Guoxiang
JO  - Machine Learning with Applications
VL  - 22
SP  - 100749
PY  - 2025
DA  - 2025/12/01/
SN  - 2666-8270
DO  - https://doi.org/10.1016/j.mlwa.2025.100749
UR  - https://www.sciencedirect.com/science/article/pii/S266682702500132X
KW  - Language models
KW  - Sentiment analysis
KW  - Abusive detection
KW  - Hollywood
KW  - Oscar
KW  - Blockbusters
KW  - Dialogues
AB  - Over the past decades, there has been an increase in the prevalence of abusive and violent content in Hollywood movies. In this study, we use language models to explore the longitudinal abuse and sentiment analysis of Hollywood Oscar and blockbuster movie dialogues from 1950 to 2024. We provide an analysis of subtitles for over a thousand movies, which are categorised into four genres. We employ fine-tuned language models to examine the trends and shifts in emotional and abusive content over the past seven decades. Findings reveal significant temporal changes in movie dialogues, which reflect broader social and cultural influences. Overall, the emotional tendencies in the films are diverse, and the detection of abusive content also exhibits significant fluctuations. The results show a gradual rise in abusive content in recent decades, reflecting social norms and regulatory policy changes. Genres such as thrillers still present a higher frequency of abusive content that emphasises the ongoing narrative role of violence and conflict. At the same time, underlying positive emotions such as humour and optimism remain prevalent in most of the movies. Furthermore, the gradual increase of abusive content in movie dialogues has been significant over the last two decades, where Oscar-nominated movies overtook the top ten blockbusters.
ER  - 

TY  - JOUR
T1  - Security of LLM-based agents regarding attacks, defenses, and applications: A comprehensive survey
AU  - Tang, Yaxin
AU  - Liu, Yijia
AU  - Lan, Jiahe
AU  - Yan, Zheng
AU  - Gelenbe, Erol
JO  - Information Fusion
VL  - 127
SP  - 103941
PY  - 2026
DA  - 2026/03/01/
SN  - 1566-2535
DO  - https://doi.org/10.1016/j.inffus.2025.103941
UR  - https://www.sciencedirect.com/science/article/pii/S1566253525010036
KW  - LLM-based agent
KW  - Security
KW  - Attack
KW  - Defense
AB  - Large Language Model (LLM) based agents that employ an LLM as a core reasoning engine, are autonomous or semi-autonomous systems. Equipped with dedicated perception and action modules, they can sense their environment and take autonomous actions to execute complex tasks. Increasingly used for automated decision-making and real-world interactions that need multi-step planning capabilities and persistent engagement with external tools and environments, these agents face substantial and complex security risks. Compared to single-turn LLM inference, the agentic execution of complex tasks significantly expands the attack surface. Although prior work has surveyed security threats and defenses for LLM-based agents, two key issues remain to be addressed: (i) lack of a comprehensive review on attack and defense methods under unified evaluation criteria, and (ii) under-exploration of the security applications of LLM-based agents, ranging from enabling cyberattacks to strengthening cyber defense. This paper addresses these gaps with a comprehensive survey of attacks against, and defenses of, the LLM-based agents, as well as a review of the security-related applications enabled by LLM-based agents. We first introduce the foundations of LLM-based agents, and describe the structure and scope of this review. We then propose two complementary sets of evaluation criteria for rigorously evaluating the performance of attacks and defenses. Using these criteria, we analyze the strengths and limitations of the work presented in the relevant literature. In addition, we propose a taxonomy of security-related applications enabled by LLM-based agents and summarize the existing work from two perspectives: cyber offense and cyber defense. Finally, we identify key open challenges and propose future research directions to advance the secure development and deployment of LLM-based agents.
ER  - 

TY  - JOUR
T1  - Generative AI in minimizing cyber-attacks: Developing the Vehicular Threat Intelligence Flowchart
AU  - Guntuka, Sony
AU  - Shakshuki, Elhadi
AU  - Malik, Haroon
JO  - Procedia Computer Science
VL  - 257
SP  - 215
EP  - 224
PY  - 2025
DA  - 2025/01/01/
T2  - The 16th International Conference on Ambient Systems, Networks and Technologies Networks (ANT)/ the 8th International Conference on Emerging Data and Industry 4.0 (EDI40)
SN  - 1877-0509
DO  - https://doi.org/10.1016/j.procs.2025.03.030
UR  - https://www.sciencedirect.com/science/article/pii/S1877050925007665
KW  - Cyber attacks
KW  - GenAI
KW  - Vehicular Networks
KW  - Cybersecurity
KW  - Threat Detection
AB  - This paper delves into the innovative applications of Generative Artificial Intelligence (GenAI) in enhancing the cybersecurity of vehicular networks, a critical area given the increasing integration of intelligent transport systems and autonomous vehicles. As vehicular networks become more sophisticated, they also become more susceptible to cyber-attacks that can compromise vehicle control systems, endangering public safety and personal privacy. GenAI offers advanced capabilities for automating defences, improving threat intelligence, and creating dynamic security frameworks that can adapt to emerging threats. This research is a comprehensive overview of the current state of GenAI in the context of vehicular network cybersecurity, highlighting the development and implementation of the Vehicular Threat Intelligence Flowchart (VTIF). The VTIF features a threat detection rule algorithm that automates the identification of cyber threats, significantly improving detection accuracy. While the integration of GenAI presents substantial benefits, it also introduces new risks, necessitating robust ethical, legal, and technical oversight. This paper outlines the potential advantages and challenges of employing GenAI in vehicular cybersecurity and proposes future research directions aimed at developing resilient and ethical cybersecurity mechanisms.
ER  - 

TY  - JOUR
T1  - A NLP Approach to Quantify Resilience in Cyber-Socio-Technical Systems with LLM Agents
AU  - De Nicola, Antonio
AU  - Migliore, Maria Guariglia
AU  - Mele, Ida
AU  - Villani, Maria Luisa
JO  - Procedia Computer Science
VL  - 253
SP  - 1943
EP  - 1950
PY  - 2025
DA  - 2025/01/01/
T2  - 6th International Conference on Industry 4.0 and Smart Manufacturing
SN  - 1877-0509
DO  - https://doi.org/10.1016/j.procs.2025.01.256
UR  - https://www.sciencedirect.com/science/article/pii/S1877050925002649
KW  - Cyber-Socio-Technical System
KW  - Large Language Model
KW  - Natural Language Processing
KW  - Resilience
KW  - Safety
AB  - Incorporating cyber artifacts into Cyber-Socio-Technical Systems (CSTSs) poses new challenges to human safety due to their increasingly unpredictable behavior. Resilience is the ability to adapt, recover, and bounce back from challenges, setbacks, or adversity. Quantifying the resilience of CSTSs requires the identification of leading or lagging indicators. Among the leading indicators, allostatic load measures the level of systemic tension accumulated from misalignments in the perspectives of system actors regarding how work should be performed. In this paper, we propose a novel approach based on Natural Language Processing (NLP) to measure allostatic load. This approach involves lightweight modelling of process perspectives, extraction of token vectors from process function descriptions, and computing vector similarity by using the Dice similarity algorithm. Then, allostatic load is defined as the complement to one of the similarity value. An example application concerning a chemical spill in a hospital laboratory demonstrates the method’s practical use.
ER  - 

TY  - JOUR
T1  - SecureQwen: Leveraging LLMs for vulnerability detection in python codebases
AU  - Mechri, Abdechakour
AU  - Ferrag, Mohamed Amine
AU  - Debbah, Merouane
JO  - Computers & Security
VL  - 148
SP  - 104151
PY  - 2025
DA  - 2025/01/01/
SN  - 0167-4048
DO  - https://doi.org/10.1016/j.cose.2024.104151
UR  - https://www.sciencedirect.com/science/article/pii/S0167404824004565
KW  - Static analysis
KW  - Vulnerability detection
KW  - Codebase
KW  - Large language model
KW  - Software security
KW  - Security
KW  - Generative pre-trained transformers
AB  - Identifying vulnerabilities in software code is crucial for ensuring the security of modern systems. However, manual detection requires expert knowledge and is time-consuming, underscoring the need for automated techniques. In this paper, we present SecureQwen, a novel vulnerability detection tool leveraging large language models (LLMs) with a context length of 64K tokens to identify potential security threats in large-scale Python codebases. Utilizing a decoder-only transformer architecture, SecureQwen captures complex relationships between code tokens, enabling accurate classification of vulnerable code sequences across 14 common weakness enumerations (CWEs), including OS Command Injection, SQL Injection, Improper Check or Handling of Exceptional Conditions, Path Traversal, Broken or Risky Cryptographic Algorithm, Deserialization of Untrusted Data, and Cleartext Transmission of Sensitive Information. Therefore, we evaluate SecureQwen on a large Python dataset with over 1.875 million function-level code snippets from different sources, including GitHub repositories, Codeparrot’s dataset, and synthetic data generated by GPT4-o. The experimental evaluation demonstrates high accuracy, with F1 scores ranging from 84% to 99%. The results indicate that SecureQwen effectively detects vulnerabilities in human-written and AI-generated code.
ER  - 

TY  - JOUR
T1  - Modeling and executing cyber security exercise scenarios in cyber ranges
AU  - Yamin, Muhammad Mudassar
AU  - Katt, Basel
JO  - Computers & Security
VL  - 116
SP  - 102635
PY  - 2022
DA  - 2022/05/01/
SN  - 0167-4048
DO  - https://doi.org/10.1016/j.cose.2022.102635
UR  - https://www.sciencedirect.com/science/article/pii/S0167404822000347
KW  - Cyber range
KW  - Security
KW  - Exercises
KW  - Scenario
KW  - Modeling
AB  - The skill shortage in global cybersecurity is a well-known problem; to overcome this issue, cyber ranges have been developed. These ranges provide a platform for conducting cybersecurity exercises; however, conducting such exercises is a complex process because they involve people with different skill sets for the scenario modeling, infrastructure preparation, dry run, execution, and evaluation. This process is very complex and inefficient in terms of time and resources. Moreover, the exercise infrastructure created in current cyber ranges does not reflect the dynamic environment of real-world systems and does not provide adaptability for changing requirements. To tackle these issues, we developed a system that can automate many tasks of the cybersecurity exercise life cycle. We used model-driven approaches to (1) model the roles of the different teams present in the cybersecurity exercises and (2) generate automation artifacts to execute their functions efficiently in an autonomous manner. By executing different team roles such as attackers and defenders, we can add friction in the environment, making it dynamic and realistic. We conducted case studies in the form of operational cybersecurity exercises involving national-level cybersecurity competitions and a university class setting in Norway to evaluate our developed system for its efficiency, adaptability, autonomy, and skill improvement of the exercise participants. In the right conditions, our proposed system could create a complex cybersecurity exercise infrastructure involving 400 nodes with customized vulnerabilities, emulated attackers, defenders, and traffic generators under 40 minutes. It provided a realistic environment for cybersecurity exercises and positively affected the exercise participants’ skill sets.
ER  - 

TY  - JOUR
T1  - Generative AI of things for sustainable smart cities: Synergizing cognitive augmentation, resource efficiency, network traffic, cybersecurity, and anomaly detection for environmental performance
AU  - Bibri, Simon Elias
AU  - Huang, Jeffrey
JO  - Sustainable Cities and Society
VL  - 133
SP  - 106826
PY  - 2025
DA  - 2025/10/01/
SN  - 2210-6707
DO  - https://doi.org/10.1016/j.scs.2025.106826
UR  - https://www.sciencedirect.com/science/article/pii/S2210670725006997
KW  - Generative artificial intelligence
KW  - Generative internet of things
KW  - Generative artificial intelligence of things
KW  - Sustainable smart cities
KW  - Environmental sustainability
KW  - Cognitive augmentation
KW  - Resource efficiency
KW  - Cybersecurity
KW  - Network traffic
KW  - Anomaly detection
AB  - Artificial Intelligence of Things (AIoT) has emerged as a transformative technology driving environmental sustainability in smart city development. However, the integration of Generative Artificial Intelligence (GenAI) within AIoT ecosystems remains largely unexplored. Current research predominantly addresses conventional AIoT frameworks, overlooking the innovative potential of generative models, such as Generative Adversarial Networks, Variational Autoencoders, Diffusion Models, Transformers, and hybrid architectures, to significantly enhance situational awareness, system optimization, operational robustness, real-time responsiveness, and adaptive decision-making in complex urban environments. AIoT systems continue to face persistent challenges, including data scarcity, poor data quality, limited adaptability, imbalanced datasets, and inadequate context-awareness. This study addresses these gaps by systematically exploring how GenAI can enhance AIoT functionalities across key domains—namely cognitive augmentation, resource efficiency, network traffic, cybersecurity, and anomaly detection—while examining their synergistic potential to improve system-level environmental performance across two interconnected layers in sustainable smart cities. At the operational layer, key findings reveal that integrating GenAI with AIoT systems enhances urban efficiency, adaptability, autonomy, robustness, and resilience by conserving resources, optimizing network traffic flows, securing infrastructures, and detecting anomalies before they escalate. Specifically, the fusion of generative intelligence with federated learning promotes sustainable, energy-efficient AIoT deployments by reducing data transmission, thereby lowering communication overhead and safeguarding user privacy. In networked environments, generative models improve synthetic traffic realism and communication efficiency. They also strengthen cybersecurity through enhanced intrusion prevention and threat detection. Additionally, they enable early identification and mitigation of anomalies, boosting operational efficiency and system robustness. These improvements stabilize sustainable smart city system functioning and prevent disruptive failures. At the environmental layer, as key findings indicate, these operational gains cascade into indirect but tangible ecological benefits, while generative models advance the core pillars of AIoT by enabling proactive, autonomous, context-aware, and self-adaptive systems that further enhance the environmental performance of sustainable smart cities. Thus, while the five domains primarily underpin the operational backbone of urban systems, their cascading effects extend to ecological outcomes. The proposed conceptual framework, distilled from key findings, integrates GenAI and AIoT and highlights both domain-specific advancements and their synergistic interactions. This framework holds significant potential to drive sustainable smart city development by fostering AIoT ecosystems that are more intelligent, resource-efficient, adaptive, secure, robust, and autonomous through the strategic application of generative intelligence. The insights gained from this study provide policymakers, urban planners, system designers, and technology developers with practical guidance to harness GAIoT for enhancing smart city resilience, sustainability, and operational intelligence.
ER  - 

TY  - JOUR
T1  - Generative AI and large language models: A new frontier in reverse vaccinology
AU  - Hayawi, Kadhim
AU  - Shahriar, Sakib
AU  - Alashwal, Hany
AU  - Serhani, Mohamed Adel
JO  - Informatics in Medicine Unlocked
VL  - 48
SP  - 101533
PY  - 2024
DA  - 2024/01/01/
SN  - 2352-9148
DO  - https://doi.org/10.1016/j.imu.2024.101533
UR  - https://www.sciencedirect.com/science/article/pii/S2352914824000893
KW  - Reverse vaccinology
KW  - Large language models (LLMs)
KW  - AI
KW  - Generative AI
KW  - Vaccine candidate identification
KW  - AI ethics
KW  - Vaccines
AB  - Reverse vaccinology is an emerging concept in the field of vaccine development as it facilitates the identification of potential vaccine candidates. Biomedical research has been revolutionized with the recent innovations in Generative Artificial Intelligence (AI) and Large Language Models (LLMs). The intersection of these two technologies is explored in this study. In this study, the impact of Generative AI and LLMs in the field of vaccinology is explored. Through a comprehensive analysis of existing research, prospective use cases, and an experimental case study, this research highlights that LLMs and Generative AI have the potential to enhance the efficiency and accuracy of vaccine candidate identification. This work also discusses the ethical and privacy challenges, such as data consent and potential biases, raised by such applications that require careful consideration. This study paves the way for experts, researchers, and policymakers to further investigate the role and impact of Generative AI and LLM in vaccinology and medicine.
ER  - 

TY  - JOUR
T1  - An Adversarial Machine Learning Approach on Securing Large Language Model with Vigil, an Open-Source Initiative
AU  - Pokhrel, Kushal
AU  - Sanin, Cesar
AU  - Hossain Sakib, Md. Kowssar
AU  - Islam, Md Rafiqul
AU  - Szczerbicki, Edward
JO  - Procedia Computer Science
VL  - 246
SP  - 686
EP  - 695
PY  - 2024
DA  - 2024/01/01/
T2  - 28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)
SN  - 1877-0509
DO  - https://doi.org/10.1016/j.procs.2024.09.486
UR  - https://www.sciencedirect.com/science/article/pii/S1877050924025316
KW  - Adversarial Machine Learning
KW  - Large Language Model
KW  - Information Security
KW  - Prompt Injection
KW  - Synthetic Prompt Generation
KW  - Natural Language Processing
AB  - Several security concerns and efforts to breach system security and prompt safety concerns have been brought to light as a result of the expanding use of LLMs. These vulnerabilities are evident and LLM models have been showing many signs of hallucination, repetitive content generation, and biases, which makes them vulnerable to malicious prompts that raise substantial concerns in regard to the dependability and efficiency of such models. It is vital to have a complete grasp of the complex behaviours of malicious attackers in order to build effective strategies for protecting modern artificial intelligence (AI) systems through the development of effective tactics. The purpose of this study is to look into some of these aspects and propose a method for preventing devastating possibilities and protecting LLMs from potential threats that attackers may pose. Vigil is an open-source LLM prompt security scanner, that is accessible as a Python library and REST API, specifically to solve these problems by employing a sophisticated adversarial machine-learning algorithm. The entire objective of this study is to make use of Vigil as a security scanner. and asses its efficiency. In this case study, we shed some light on Vigil, which effectively recognises and helps LLM prompts by identifying two varieties of threats: malicious and benign.
ER  - 

TY  - JOUR
T1  - LPASS: Linear Probes as Stepping Stones for vulnerability detection using compressed LLMs
AU  - Ibanez-Lissen, Luis
AU  - Gonzalez-Manzano, Lorena
AU  - de Fuentes, Jose Maria
AU  - Anciaux, Nicolas
JO  - Journal of Information Security and Applications
VL  - 93
SP  - 104125
PY  - 2025
DA  - 2025/09/01/
SN  - 2214-2126
DO  - https://doi.org/10.1016/j.jisa.2025.104125
UR  - https://www.sciencedirect.com/science/article/pii/S2214212625001620
KW  - Vulnerability detection
KW  - LLM
KW  - Model compression
KW  - Linear classification probes
AB  - Large Language Models (LLMs) are being extensively used for cybersecurity purposes. One of them is the detection of vulnerable codes. For the sake of efficiency and effectiveness, compression and fine-tuning techniques are being developed, respectively. However, they involve spending substantial computational efforts. In this vein, we analyze how Linear Probes (LPs) can be used to provide an estimation on the performance of a compressed LLM at an early phase — before fine-tuning. We also show their suitability to set the cut-off point when applying layer pruning compression. Our approach, dubbed LPASS, is applied in BERT and Gemma for the detection of 12 of MITRE’s Top 25 most dangerous vulnerabilities on 480k C/C++ samples. LPs can be computed in 142.97 s. and provide key findings: (1) 33.3 % and 72.2% of layers can be removed, respectively, with no precision loss; (2) they provide an early estimate of the post-fine-tuning and post-compression model effectiveness, with 3% and 8.68% as the lowest and average precision errors, respectively. LPASS-based LLMs outperform the state of the art, reaching 86.9% of accuracy in multi-class vulnerability detection. Interestingly, LPASS-based compressed versions of Gemma outperform the original ones by 1.6% of F1-score at a maximum while saving 29.4 % and 23.8% of training and inference time and 42.98% of model size.
ER  - 

TY  - JOUR
T1  - When Large Language Models and Machine Learning Meet Multi-Criteria Decision Making: Fully Integrated Approach for Social Media Moderation
AU  - Fuentes, Noreen
AU  - Ugang, Janeth
AU  - Galamiton, Narcisan
AU  - Bacus, Suzette
AU  - Evangelista, Samantha Shane
AU  - Maturan, Fatima
AU  - Ocampo, Lanndon
JO  - Computers, Materials and Continua
VL  - 86
IS  - 1
SP  - 1
EP  - 26
PY  - 2025
DA  - 2025/11/10/
SN  - 1546-2218
DO  - https://doi.org/10.32604/cmc.2025.068104
UR  - https://www.sciencedirect.com/science/article/pii/S154622182501046X
KW  - Self-moderation
KW  - user-generated content
KW  - k-means clustering
KW  - TODIM
KW  - large language models
AB  - This study demonstrates a novel integration of large language models, machine learning, and multi-criteria decision-making to investigate self-moderation in small online communities, a topic under-explored compared to user behavior and platform-driven moderation on social media. The proposed methodological framework (1) utilizes large language models for social media post analysis and categorization, (2) employs k-means clustering for content characterization, and (3) incorporates the TODIM (Tomada de Decisão Interativa Multicritério) method to determine moderation strategies based on expert judgments. In general, the fully integrated framework leverages the strengths of these intelligent systems in a more systematic evaluation of large-scale decision problems. When applied in social media moderation, this approach promotes nuanced and context-sensitive self-moderation by taking into account factors such as cultural background and geographic location. The application of this framework is demonstrated within Facebook groups. Eight distinct content clusters encompassing safety, harassment, diversity, and misinformation are identified. Analysis revealed a preference for content removal across all clusters, suggesting a cautious approach towards potentially harmful content. However, the framework also highlights the use of other moderation actions, like account suspension, depending on the content category. These findings contribute to the growing body of research on self-moderation and offer valuable insights for creating safer and more inclusive online spaces within smaller communities.
ER  - 

TY  - JOUR
T1  - Reinforcement learning for an efficient and effective malware investigation during cyber incident response
AU  - Dunsin, Dipo
AU  - Ghanem, Mohamed Chahine
AU  - Ouazzane, Karim
AU  - Vassilev, Vassil
JO  - High-Confidence Computing
VL  - 5
IS  - 3
SP  - 100299
PY  - 2025
DA  - 2025/09/01/
SN  - 2667-2952
DO  - https://doi.org/10.1016/j.hcc.2025.100299
UR  - https://www.sciencedirect.com/science/article/pii/S2667295225000030
KW  - Cyber incident
KW  - Digital forensics
KW  - Artificial intelligence
KW  - Reinforcement learning
KW  - Markov Chain
KW  - MDP
KW  - DFIR
KW  - Malware
KW  - Incident response
AB  - The ever-escalating prevalence of malware is a serious cybersecurity threat, often requiring advanced post-incident forensic investigation techniques. This paper proposes a framework to enhance malware forensics by leveraging reinforcement learning (RL). The approach combines heuristic and signature-based methods, supported by RL through a unified MDP model, which breaks down malware analysis into distinct states and actions. This optimisation enhances the identification and classification of malware variants. The framework employs Q-learning and other techniques to boost the speed and accuracy of detecting new and unknown malware, outperforming traditional methods. We tested the experimental framework across multiple virtual environments infected with various malware types. The RL agent collected forensic evidence and improved its performance through Q-tables and temporal difference learning. The epsilon-greedy exploration strategy, in conjunction with Q-learning updates, effectively facilitated transitions. The learning rate depended on the complexity of the MDP environment: higher in simpler ones for quicker convergence and lower in more complex ones for stability. This RL-enhanced model significantly reduced the time required for post-incident malware investigations, achieving a high accuracy rate of 94% in identifying malware. These results indicate RL’s potential to revolutionise post-incident forensics investigations in cybersecurity. Future work will incorporate more advanced RL algorithms and large language models (LLMs) to further enhance the effectiveness of malware forensic analysis.
ER  - 

TY  - JOUR
T1  - Integrating human expertise & automated methods for a dynamic and multi-parametric evaluation of large language models’ feasibility in clinical decision-making
AU  - Sblendorio, Elena
AU  - Dentamaro, Vincenzo
AU  - Lo Cascio, Alessio
AU  - Germini, Francesco
AU  - Piredda, Michela
AU  - Cicolini, Giancarlo
JO  - International Journal of Medical Informatics
VL  - 188
SP  - 105501
PY  - 2024
DA  - 2024/08/01/
SN  - 1386-5056
DO  - https://doi.org/10.1016/j.ijmedinf.2024.105501
UR  - https://www.sciencedirect.com/science/article/pii/S1386505624001643
KW  - LLM’s feasibility
KW  - AI routine integration
KW  - Methodology
KW  - Clinical decision making
KW  - Healthcare innovation
KW  - Nursing informatics
KW  - Safety
KW  - Multidisciplinary approach
KW  - Multi-parametric analysis
AB  - Background
Recent enhancements in Large Language Models (LLMs) such as ChatGPT have exponentially increased user adoption. These models are accessible on mobile devices and support multimodal interactions, including conversations, code generation, and patient image uploads, broadening their utility in providing healthcare professionals with real-time support for clinical decision-making. Nevertheless, many authors have highlighted serious risks that may arise from the adoption of LLMs, principally related to safety and alignment with ethical guidelines.
Objective
To address these challenges, we introduce a novel methodological approach designed to assess the specific feasibility of adopting LLMs within a healthcare area, with a focus on clinical nursing, evaluating their performance and thereby directing their choice. Emphasizing LLMs’ adherence to scientific advancements, this approach prioritizes safety and care personalization, according to the “Organization for Economic Co-operation and Development” frameworks for responsible AI. Moreover, its dynamic nature is designed to adapt to future evolutions of LLMs.
Method
Through integrating advanced multidisciplinary knowledge, including Nursing Informatics, and aided by a prospective literature review, seven key domains and specific evaluation items were identified as follows:1.State of the Art Alignment & Safety.2.Focus, Accuracy & Management of Prompt Ambiguity.3.Data Integrity, Data Security, Ethics & Sustainability, in accordance with OECD Recommendations for Responsible AI.4.Temporal Variability of Responses (Consistency)5.Adaptation to specific standardized terminology and Classifications for healthcare professionals.6.General Capabilities: Post User Feedback Self-Evolution Capability and Organization in Chapters.7.Ability to Drive Evolution in Healthcare.A Peer Review by experts in Nursing and AI was performed, ensuring scientific rigor and breadth of insights for an essential, reproducible, and coherent methodological approach. By means of a 7-point Likert scale, thresholds are defined in order to classify LLMs as “unusable”, “usable with high caution”, and “recommended” categories. Nine state of the art LLMs were evaluated using this methodology in clinical oncology nursing decision-making, producing preliminary results. Gemini Advanced, Anthropic Claude 3 and ChatGPT 4 achieved the minimum score of the State of the Art Alignment & Safety domain for classification as “recommended”, being also endorsed across all domains. LLAMA 3 70B and ChatGPT 3.5 were classified as “usable with high caution.” Others were classified as unusable in this domain.
Conclusion
The identification of a recommended LLM for a specific healthcare area, combined with its critical, prudent, and integrative use, can support healthcare professionals in decision-making processes.
ER  - 

TY  - JOUR
T1  - Chinese Cyber Threat Intelligence Named Entity Recognition via RoBERTa-wwm-RDCNN-CRF
AU  - Zhen, Zhen
AU  - Gao, Jian
JO  - Computers, Materials and Continua
VL  - 77
IS  - 1
SP  - 299
EP  - 323
PY  - 2023
DA  - 2023/10/31/
SN  - 1546-2218
DO  - https://doi.org/10.32604/cmc.2023.042090
UR  - https://www.sciencedirect.com/science/article/pii/S1546221823001212
KW  - Cybersecurity
KW  - cyber threat intelligence
KW  - named entity recognition
AB  - In recent years, cyber attacks have been intensifying and causing great harm to individuals, companies, and countries. The mining of cyber threat intelligence (CTI) can facilitate intelligence integration and serve well in combating cyber attacks. Named Entity Recognition (NER), as a crucial component of text mining, can structure complex CTI text and aid cybersecurity professionals in effectively countering threats. However, current CTI NER research has mainly focused on studying English CTI. In the limited studies conducted on Chinese text, existing models have shown poor performance. To fully utilize the power of Chinese pre-trained language models (PLMs) and conquer the problem of lengthy infrequent English words mixing in the Chinese CTIs, we propose a residual dilated convolutional neural network (RDCNN) with a conditional random field (CRF) based on a robustly optimized bidirectional encoder representation from transformers pre-training approach with whole word masking (RoBERTa-wwm), abbreviated as RoBERTa-wwm-RDCNN-CRF. We are the first to experiment on the relevant open source dataset and achieve an F1-score of 82.35%, which exceeds the common baseline model bidirectional encoder representation from transformers (BERT)-bidirectional long short-term memory (BiLSTM)-CRF in this field by about 19.52% and exceeds the current state-of-the-art model, BERT-RDCNN-CRF, by about 3.53%. In addition, we conducted an ablation study on the encoder part of the model to verify the effectiveness of the proposed model and an in-depth investigation of the PLMs and encoder part of the model to verify the effectiveness of the proposed model. The RoBERTa-wwm-RDCNN-CRF model, the shared pre-processing, and augmentation methods can serve the subsequent fundamental tasks such as cybersecurity information extraction and knowledge graph construction, contributing to important applications in downstream tasks such as intrusion detection and advanced persistent threat (APT) attack detection.
ER  - 

TY  - JOUR
T1  - MADONNA: Browser-based malicious domain detection using Optimized Neural Network by leveraging AI and feature analysis
AU  - Senanayake, Janaka
AU  - Rajapaksha, Sampath
AU  - Yanai, Naoto
AU  - Kalutarage, Harsha
AU  - Komiya, Chika
JO  - Computers & Security
VL  - 152
SP  - 104371
PY  - 2025
DA  - 2025/05/01/
SN  - 0167-4048
DO  - https://doi.org/10.1016/j.cose.2025.104371
UR  - https://www.sciencedirect.com/science/article/pii/S0167404825000604
KW  - Malicious domain detection
KW  - Artificial intelligence
KW  - Feature engineering
KW  - Browser extension
AB  - Detecting malicious domains is a critical aspect of cybersecurity, with recent advancements leveraging Artificial Intelligence (AI) to enhance accuracy and speed. However, existing browser-based solutions often struggle to achieve both high accuracy and efficient throughput. In this paper, we present MADONNA, a novel browser-based malicious domain detector that exceeds the current state-of-the-art in both accuracy and throughput. MADONNA utilizes feature selection through correlation analysis and model optimization techniques, including pruning and quantization, to significantly enhance detection speed without compromising accuracy. Our approach employs a Shallow Neural Network (SNN) architecture, outperforming Large Language Models (LLMs) and state-of-the-art methods by improving accuracy by 6% (reaching 0.94) and F1-score by 4% (reaching 0.92). We further integrated MADONNA into a Google Chrome extension, demonstrating its practical application with a real-time domain detection accuracy of 94% and an average inference time of 0.87 s. These results highlight MADONNA’s effectiveness in balancing speed and accuracy, providing a scalable, real-world solution for malicious domain detection.
ER  - 

TY  - JOUR
T1  - ForensicLLM: A local large language model for digital forensics
AU  - Sharma, Binaya
AU  - Ghawaly, James
AU  - McCleary, Kyle
AU  - Webb, Andrew M.
AU  - Baggili, Ibrahim
JO  - Forensic Science International: Digital Investigation
VL  - 52
SP  - 301872
PY  - 2025
DA  - 2025/03/01/
T2  - DFRWS EU 2025 - Selected Papers from the 12th Annual Digital Forensics Research Conference Europe
SN  - 2666-2817
DO  - https://doi.org/10.1016/j.fsidi.2025.301872
UR  - https://www.sciencedirect.com/science/article/pii/S2666281725000113
KW  - Digital forensics
KW  - Digital investigations
KW  - Generative AI
KW  - Large language model (LLM)
KW  - Admissibility of evidence
KW  - ChatGPT
KW  - LLaMA
KW  - Retrieval augmented generation (RAG)
KW  - Fine-tuning
AB  - Large Language Models (LLMs) excel in diverse natural language tasks but often lack specialization for fields like digital forensics. Their reliance on cloud-based APIs or high-performance computers restricts use in resource-limited environments, and response hallucinations could compromise their applicability in forensic contexts. We introduce ForensicLLM, a 4-bit quantized LLaMA-3.1–8B model fine-tuned on Q&A samples extracted from digital forensic research articles and curated digital artifacts. Quantitative evaluation showed that ForensicLLM outperformed both the base LLaMA-3.1–8B model and the Retrieval Augmented Generation (RAG) model. ForensicLLM accurately attributes sources 86.6 % of the time, with 81.2 % of the responses including both authors and title. Additionally, a user survey conducted with digital forensics professionals confirmed significant improvements of ForensicLLM and RAG model over the base model. ForensicLLM showed strength in “correctness” and “relevance” metrics, while the RAG model was appreciated for providing more detailed responses. These advancements mark ForensicLLM as a transformative tool in digital forensics, elevating model performance and source attribution in critical investigative contexts.
ER  - 

TY  - JOUR
T1  - Intelligent Fault Diagnosis for CNC Through the Integration of Large Language Models and Domain Knowledge Graphs
AU  - Liu, Yuhan
AU  - Zhou, Yuan
AU  - Liu, Yufei
AU  - Xu, Zhen
AU  - He, Yixin
JO  - Engineering
VL  - 53
SP  - 311
EP  - 322
PY  - 2025
DA  - 2025/10/01/
SN  - 2095-8099
DO  - https://doi.org/10.1016/j.eng.2025.04.003
UR  - https://www.sciencedirect.com/science/article/pii/S2095809925001948
KW  - Large language model
KW  - Domain knowledge graph
KW  - Knowledge graph-based retrieval augmented generation
KW  - Learning mechanism
KW  - Decision support system
AB  - As large language models (LLMs) continue to demonstrate their potential in handling complex tasks, their value in knowledge-intensive industrial scenarios is becoming increasingly evident. Fault diagnosis, a critical domain in the industrial sector, has long faced the dual challenges of managing vast amounts of experiential knowledge and improving human–machine collaboration efficiency. Traditional fault diagnosis systems, which are primarily based on expert systems, suffer from three major limitations: ① ineffective organization of fault diagnosis knowledge, ② lack of adaptability between static knowledge frameworks and dynamic engineering environments, and ③ difficulties in integrating expert knowledge with real-time data streams. These systemic shortcomings restrict the ability of conventional approaches to handle uncertainty. In this study, we proposed an intelligent computer numerical control (CNC) fault diagnosis system, integrating LLMs with knowledge graph (KG). First, we constructed a comprehensive KG that consolidated multi-source data for structured representation. Second, we designed a retrieval-augmented generation (RAG) framework leveraging the KG to support multi-turn interactive fault diagnosis while incorporating real-time engineering data into the decision-making process. Finally, we introduced a learning mechanism to facilitate dynamic knowledge updates. The experimental results demonstrated that our system significantly improved fault diagnosis accuracy, outperforming engineers with two years of professional experience on our constructed benchmark datasets. By integrating LLMs and KG, our framework surpassed the limitations of traditional expert systems rooted in symbolic reasoning, offering a novel approach to addressing the cognitive paradox of unstructured knowledge modeling and dynamic environment adaptation in industrial settings.
ER  - 

TY  - JOUR
T1  - Zero-shot load forecasting for integrated energy systems: A large language model-based framework with multi-task learning
AU  - Li, Jiaheng
AU  - Li, Donghe
AU  - Yang, Ye
AU  - Xi, Huan
AU  - Yu, Wanju
AU  - Xiao, Yu
AU  - Yang, Qingyu
JO  - Neurocomputing
VL  - 654
SP  - 131288
PY  - 2025
DA  - 2025/11/14/
SN  - 0925-2312
DO  - https://doi.org/10.1016/j.neucom.2025.131288
UR  - https://www.sciencedirect.com/science/article/pii/S0925231225019605
KW  - Zero-shot forecasting
KW  - Large language models
KW  - Time series prompt generation
KW  - Multi-task learning
KW  - Similarity alignment
AB  - The growing penetration of renewable energy sources in power systems has increased the complexity and uncertainty of load forecasting, especially for integrated energy systems with multiple energy carriers. Traditional forecasting methods heavily rely on historical data and demonstrate limited transferability across different scenarios, creating significant challenges for emerging applications in smart grids and energy internet. This paper proposes the TSLLM-Load Forecasting Mechanism, a novel zero-shot load forecasting framework based on large language models (LLMs) to address these challenges. The framework consists of three key components: a data preprocessing module that handles multi-source energy load data, a time series prompt generation module that bridges the semantic gap between energy data and LLMs through multi-task learning and similarity alignment, and a prediction module that leverages pre-trained LLMs for accurate forecasting. We validate the framework’s effectiveness on real-world datasets comprising load profiles from Australian solar-powered households. In conventional testing, our method achieves competitive performance, ranking third among nine methods. In zero-shot prediction, our framework demonstrates 10.8 % MSE improvement and 12.5 % MAE improvement compared to Informer, ranking second overall after TimeMixer. Most significantly, few-shot learning experiments reveal exceptional capabilities under extreme data constraints, with our method achieving optimal performance when trained with only 1 % of available data, representing 40.8 % MSE improvement compared to the best conventional method and 78.9 % improvement compared to existing LLM-based approaches. Large-scale transferability analysis shows our model outperforms baselines for 82 % of households when trained with minimal data from a single household. These results demonstrate the framework’s potential for accurate and transferable load forecasting in integrated energy systems, particularly beneficial for renewable energy integration and smart grid applications where historical data availability is limited.
ER  - 

TY  - JOUR
T1  - Scalable automation for IoT cyberSecurity compliance: Ontology-driven reasoning for real-time assessment
AU  - Oranekwu, Ikechukwu
AU  - Elluri, Lavanya
AU  - Yus, Roberto
AU  - Kotal, Anantaa
JO  - Computers & Security
VL  - 161
SP  - 104711
PY  - 2026
DA  - 2026/02/01/
SN  - 0167-4048
DO  - https://doi.org/10.1016/j.cose.2025.104711
UR  - https://www.sciencedirect.com/science/article/pii/S0167404825004006
KW  - NISTIR
KW  - Standards
KW  - Guidelines
KW  - Internet of things
KW  - Cybersecurity compliance
KW  - Large language model
KW  - Retrieval augmented generation
KW  - Knowledge graphs
KW  - Semantic web technologies
KW  - Ontology reasoning
KW  - Privacy policy analysis
KW  - SPARQL
KW  - SWRL
KW  - Security
KW  - Governance
AB  - In recent years, the rapid expansion of the Internet of Things (IoT) has introduced significant cybersecurity challenges, requiring manufacturers to comply with various regulatory frameworks and cybersecurity standards. Hence, to protect user data and privacy, all organizations providing IoT devices must adhere to complex guidelines such as the National Institute of Standards and Technology Inter-Agency Report (NISTIR) 8259, which defines essential cybersecurity guidelines for IoT manufacturers. However, interpreting and applying these rules from these guidelines remains a significant challenge for companies. Previously, our Automated Knowledge Framework for IoT Cybersecurity Compliance, leveraged SWRL, SPARQL queries, Web Ontology Language and Visualization (OWL Viz), Semantic Web technologies, Large Language Models (LLMs), and Retrieval Augmented Generation (RAG) pipeline to automate compliance assessment of multiple Functional requirement documents (FRDs), while systematically cross-checking Business requirement documents (BRDs) against them [Oranekwu et al., 2024]. However, these efforts primarily focused on mapping NISTIR 8259 guidelines into a structured ontology laying the foundation for us to build, expand on, and then integrate the IoT Cybersecurity Improvement Act of 2020 into the compliance framework. Furthermore, exploring its big data capability, the Knowledge Graph (KG) has been expanded and populated with more than 800 manufacturer privacy policy instances, allowing direct comparison between manufacturer-defined data properties, object properties, and regulatory compliance expectations. The primary objective is to evaluate the effectiveness of this enhanced version of the framework in identifying policy non-compliance by comparing triples extracted from privacy policies against the structured knowledge representation. Through this approach, our goal is to automate compliance verification by examining the relationships between manufacturers, security requirements, and regulatory obligations, offering a scalable solution for the security governance of IoT.
ER  - 

TY  - JOUR
T1  - A formal framework for LLM-assisted automated generation of Zeek signatures from binary artifacts
AU  - Greco, Claudia
AU  - Ianni, Michele
JO  - Future Generation Computer Systems
VL  - 175
SP  - 108086
PY  - 2026
DA  - 2026/02/01/
SN  - 0167-739X
DO  - https://doi.org/10.1016/j.future.2025.108086
UR  - https://www.sciencedirect.com/science/article/pii/S0167739X25003802
KW  - Large language models
KW  - Zeek
KW  - Binary analysis
KW  - Malware
AB  - Designing semantically meaningful and operationally effective intrusion detection signatures remains a labor-intensive and expertise-driven task, particularly within the Zeek network monitoring framework. In this paper, we introduce a formalized and modular system for automating Zeek signature generation using Large Language Models (LLMs). Our pipeline begins with static analysis of binary artifacts, extracts salient behavioral features, and transforms them into structured prompts for an LLM tasked with synthesizing Zeek scripts. We provide a rigorous formal framework that defines each stage of this transformation, along with theoretical models for prompt distortion, injection resilience, and sanitization. Furthermore, we explore the adversarial surface exposed by LLMs—introducing a taxonomy of injection attacks, prompt inversion risks, and behavioral feedback loops—and propose mitigations grounded in filtering and robust prompt engineering. Our approach not only accelerates signature creation but also enhances interpretability and adaptability in evolving threat environments. The framework lays the groundwork for future extensions involving dynamic analysis and automated post-validation of generated signatures.
ER  - 

TY  - JOUR
T1  - Enhancing user prompt confidentiality in Large Language Models through advanced differential encryption
AU  - Gupta, Brij B.
AU  - Gaurav, Akshat
AU  - Arya, Varsha
AU  - Alhalabi, Wadee
AU  - Alsalman, Dheyaaldin
AU  - Vijayakumar, P.
JO  - Computers and Electrical Engineering
VL  - 116
SP  - 109215
PY  - 2024
DA  - 2024/05/01/
SN  - 0045-7906
DO  - https://doi.org/10.1016/j.compeleceng.2024.109215
UR  - https://www.sciencedirect.com/science/article/pii/S0045790624001435
KW  - Cryptographic privacy
KW  - Large Language Models
KW  - Data anonymization
KW  - Secure AI framework
KW  - Personal data protection
AB  - In the era of artificial intelligence (AI) advancements heralded by Large Language Models (LLMs) like GPT-3, the capacity to parse and generate human-like text brings to light substantial privacy concerns. These arise notably from LLMs’ reliance on vast datasets often laden with personal information, underscoring the potential for inadvertent memorization and disclosure of sensitive data. Addressing these pivotal privacy concerns, our research introduces a novel two-fold approach aimed at bolstering the confidentiality and security of user data in LLM applications. Firstly, we deploy advanced cryptographic techniques, incorporating bespoke encryption and hashing protocols, to preprocess user data. This strategy effectively anonymizes personal identifiers prior to their processing by LLMs, directly tackling the challenges of sensitive information exposure. Concurrently, our methodology encompasses a secure mutual authentication protocol utilizing lightweight cryptographic measures. This ensures that system interactions are strictly reserved for authenticated users, thereby enhancing overall data security. Collectively, our approach not only preserves the utility of data for AI tasks but also fortifies the privacy framework surrounding LLMs, significantly reducing the likelihood of privacy breaches and steering AI development towards a more secure and ethically grounded future.
ER  - 

TY  - JOUR
T1  - Formal requirements engineering and large language models: A two-way roadmap
AU  - Ferrari, Alessio
AU  - Spoletini, Paola
JO  - Information and Software Technology
VL  - 181
SP  - 107697
PY  - 2025
DA  - 2025/05/01/
SN  - 0950-5849
DO  - https://doi.org/10.1016/j.infsof.2025.107697
UR  - https://www.sciencedirect.com/science/article/pii/S0950584925000369
KW  - Requirements engineering
KW  - Formal methods
KW  - Large language models
KW  - LLMs
KW  - Natural language processing
KW  - NLP
KW  - NLP4RE
KW  - Prompt engineering
KW  - Prompt requirements engineering
AB  - Context:
Large Language Models (LLMs) have made remarkable advancements in emulating human linguistic capabilities, showing potential also in executing various requirements engineering (RE) tasks. However, despite their generally good performance, the adoption of LLM-generated solutions and artefacts prompts concerns about their correctness, fairness, and trustworthiness.
Objective:
This paper aims to address the concerns associated with the use of LLMs in RE activities. Specifically, it seeks to develop a roadmap that leverages formal methods (FMs) to provide guarantees of correctness, fairness, and trustworthiness when LLMs are utilised in RE. Symmetrically, it aims to explore how LLMs can be employed to make FMs more accessible.
Methods:
We use two sets of examples to show the current limits of FMs when used in software development and of LLMs when used for RE tasks. The highlighted limitations are addressed by proposing two roadmaps grounded in the current literature and technologies.
Results:
The proposed examples show the potential and limits of FMs in supporting software development and of LLMs when used for RE tasks. The initial investigation into how these limitations can be overcome has been concretised in two detailed roadmaps for the RE and, more largely, the software engineering community.
Conclusion:
The proposed roadmaps offer a promising approach to address the concerns of correctness, fairness, and trustworthiness associated with the use of LLMs in RE tasks through the use of FMs and to enhance the accessibility of FMs by utilising LLMs.
ER  - 

TY  - JOUR
T1  - Mapping the research landscape of Large Language Models from 2018 to 2024: A bibliometric analysis
AU  - Yezhu, Wang
AU  - Yafang, Fan
AU  - Lu, Guo
AU  - Yundong, Xie
JO  - Procedia Computer Science
VL  - 266
SP  - 947
EP  - 954
PY  - 2025
DA  - 2025/01/01/
T2  - The 12th International Conference on Information Technology and Quantitative Management (ITQM 2025)
SN  - 1877-0509
DO  - https://doi.org/10.1016/j.procs.2025.08.117
UR  - https://www.sciencedirect.com/science/article/pii/S1877050925024287
KW  - large language models (LLMs)
KW  - research trends
KW  - scientific mapping
KW  - bibliometric analysis
AB  - Large language models (LLMs), built upon transformer-based architectures, have played a central role in the development of artificial intelligence since 2018. With applications extending to healthcare, education, and scientific domains, interest in LLMs has grown significantly. This study conducts a comprehensive bibliometric analysis of LLM-related publications from 2018 to 2024, based on data retrieved from the Web of Science Core Collection. A total of 24,918 records were examined through performance analysis and science mapping. The analysis covers publication trends, subject areas, publication venues, international collaborations, and keyword co-occurrence patterns. The findings reveal rapid growth and thematic diversification in the field, offering valuable insights for researchers and policymakers seeking to understand current developments and guide future research directions.
ER  - 

TY  - JOUR
T1  - Trusta: Reasoning about assurance cases with formal methods and large language models
AU  - Chen, Zezhong
AU  - Deng, Yuxin
AU  - Du, Wenjie
JO  - Science of Computer Programming
VL  - 244
SP  - 103288
PY  - 2025
DA  - 2025/09/01/
SN  - 0167-6423
DO  - https://doi.org/10.1016/j.scico.2025.103288
UR  - https://www.sciencedirect.com/science/article/pii/S0167642325000279
KW  - Assurance cases
KW  - Trustworthiness derivation trees
KW  - Large language models
KW  - Formal methods
KW  - Constraint solving
AB  - Assurance cases can be used to argue for the safety of products in safety engineering. In safety-critical areas, the construction of assurance cases is indispensable. We introduce the Trustworthiness Derivation Tree Analyzer (Trusta), a tool designed to enhance the development and evaluation of assurance cases by integrating formal methods and large language models (LLMs). The tool incorporates a Prolog interpreter and solvers like Z3 and MONA to handle various constraint types, enhancing the precision and efficiency of assurance case assessment. Beyond traditional formal methods, Trusta harnesses the power of LLMs including ChatGPT-3.5, ChatGPT-4, and PaLM 2, assisting humans in the development of assurance cases and the writing of formal constraints. Our evaluation, through qualitative and quantitative analyses, shows Trusta's impact on improving assurance case quality and efficiency. Trusta enables junior engineers to reach the skill level of experienced safety experts, narrowing the expertise gap and greatly benefiting those with limited experience. Case studies, including automated guided vehicles (AGVs), demonstrate Trusta's effectiveness in identifying subtle issues and improving the overall trustworthiness of complex systems.
ER  - 

TY  - JOUR
T1  - An innovative GPT-based open-source intelligence using historical cyber incident reports
AU  - Sufi, Fahim
JO  - Natural Language Processing Journal
VL  - 7
SP  - 100074
PY  - 2024
DA  - 2024/06/01/
SN  - 2949-7191
DO  - https://doi.org/10.1016/j.nlp.2024.100074
UR  - https://www.sciencedirect.com/science/article/pii/S2949719124000220
KW  - LLM for cyber
KW  - GPT API on OSINT
KW  - Cyber intelligence
KW  - Deep learning
KW  - Cyber threat
KW  - Chat GPT
KW  - Intelligence decision support system
AB  - In contemporary discourse, the pervasive influences of Generative Pre-Trained (GPT) and Large Language Models (LLM) are evident, showcasing diverse applications. GPT-based technologies, transcending mere summarization, exhibit adeptness in discerning critical information from extensive textual corpuses. Through prudent extraction of semantically meaningful content from textual representations, GPT technologies engender automated feature extraction, a departure from the fallible manual extraction methodologies. This study posits an innovative paradigm for extracting multidimensional cyber threat-related features from textual depictions of cyber events, leveraging the prowess of GPT. These extracted features serve as inputs for artificial intelligence (AI) and deep learning algorithms, including Convolutional Neural Network (CNN), Decomposition analysis, and Natural Language Processing (NLP)-based modalities tailored for non-technical cyber strategists. The proposed framework empowers cyber strategists or analysts to articulate inquiries regarding historical cyber incidents in plain English, with the NLP-based interaction facet of the system proffering cogent AI-driven insights in natural language. Furthermore, salient insights, often elusive in dynamic visualizations, are succinctly presented in plain language. Empirical validation of the entire system ensued through autonomous acquisition of semantically enriched contextual information concerning 214 major cyber incidents spanning from 2016 to 2023. GPT-based responses on Actor Type, Target, Attack Source (i.e., Country Originating Attack), Attack Destination (i.e., Targeted Country), Attack Level, Attack Type, and Attack Timeline, underwent critical AI-driven analysis. This comprehensive 7-dimensional information gleaned from the corpus of 214 incidents yielded a corpus of 1498 informative outputs, attaining a commendable precision of 96%, a recall rate of 98%, and an F1-Score of 97%.
ER  - 

TY  - JOUR
T1  - Real-time traffic flow optimization using large language models and reinforcement learning for smart urban mobility
AU  - Singh, Arvind R.
AU  - Ashraf, Muhammad Wasim Abbas
AU  - Rathore, Rajkumar Singh
AU  - Li, Bin
AU  - Sujatha, M.S.
JO  - Applied Soft Computing
VL  - 185
SP  - 113917
PY  - 2025
DA  - 2025/12/01/
SN  - 1568-4946
DO  - https://doi.org/10.1016/j.asoc.2025.113917
UR  - https://www.sciencedirect.com/science/article/pii/S156849462501230X
KW  - Traffic Flow Optimization
KW  - Large Language Models
KW  - Reinforcement Learning
KW  - Smart Urban Mobility
AB  - The exponential growth of metropolitan populations causes transportation network congestion, which increases fuel usage, travel time, and environmental damage. Traditional traffic management systems (TMS) seldom handle these issues in real time. Recently developed Large Language Models (LLMs), especially those using Reinforcement Learning (RL), may enhance urban transportation systems. Traffic management technology's real-time flexibility and shifting congestion patterns provide improved potential. Traditional approaches cannot estimate traffic flow or adapt to urban settings. A strong AI-driven method is needed to improve urban mobility and traffic flow. This paper introduces the LLM-RL Traffic Optimization Framework (LLM-RL-TOF). LLMs analyze real-time traffic data and give predictive insights in this context. Due to these new insights, the RL algorithm can improve traffic flow in real time and reduce congestion via dynamic traffic management. IoT sensors and urban traffic cameras capture real-time traffic data, including traffic volume and incidents. This data helps the LLM estimate bottlenecks, accidents, and traffic congestion. An RL agent uses LLM outputs to adjust traffic signal timing and suggest alternate routes. With real-time alternatives, traffic flow and urban mobility may be optimized. The junction throughput rate rose 17.5 %, the queue length accumulation index fell 22.3 %, and the average vehicle delay fell 18.6 %. The decrease in average vehicle delay enabled all these gains.
ER  - 

TY  - JOUR
T1  - CyberRAG: An agentic RAG cyber attack classification and reporting tool
AU  - Blefari, Francesco
AU  - Cosentino, Cristian
AU  - Pironti, Francesco Aurelio
AU  - Furfaro, Angelo
AU  - Marozzo, Fabrizio
JO  - Future Generation Computer Systems
VL  - 176
SP  - 108186
PY  - 2026
DA  - 2026/03/01/
SN  - 0167-739X
DO  - https://doi.org/10.1016/j.future.2025.108186
UR  - https://www.sciencedirect.com/science/article/pii/S0167739X25004807
KW  - Large language models
KW  - Agentic retrieval-augmented generation
KW  - Cyber threat detection
KW  - Fine-tuned security classifiers
KW  - Intrusion detection systems
AB  - Intrusion Detection and Prevention Systems (IDS/IPS) in large enterprises can generate hundreds of thousands of alerts per hour, overwhelming analysts with logs requiring rapidly evolving expertise. Conventional machine-learning detectors reduce alert volume but still yield many false positives, while standard Retrieval-Augmented Generation (RAG) pipelines often retrieve irrelevant context and fail to justify predictions. We present CyberRAG, a modular agent-based RAG framework that delivers real-time classification, explanation, and structured reporting for cyber-attacks. A central LLM agent orchestrates: (i) fine-tuned classifiers specialized by attack family; (ii) tool adapters for enrichment and alerting; and (iii) an iterative retrieval-and-reason loop that queries a domain-specific knowledge base until evidence is relevant and self-consistent. Unlike traditional RAG, CyberRAG adopts an agentic design that enables dynamic control flow and adaptive reasoning. This architecture autonomously refines threat labels and natural-language justifications, reducing false positives and enhancing interpretability. It is also extensible: new attack types can be supported by adding classifiers without retraining the core agent. CyberRAG was evaluated on SQL Injection, XSS, and SSTI, achieving over 94 % accuracy per class and a final classification accuracy of 94.92 % through semantic orchestration. Generated explanations reached 0.94 in BERTScore and 4.9/5 in GPT-4-based expert evaluation, with robustness preserved against adversarial and unseen payloads. These results show that agentic, specialist-oriented RAG can combine high detection accuracy with trustworthy, SOC-ready prose, offering a flexible path toward partially automated cyber-defense workflows.
ER  - 

TY  - JOUR
T1  - Bridging the gap between LLMs and structured program vulnerability analysis: An agent reasoning approach with first-order logic modeling
AU  - Yin, Zhongxu
AU  - Li, Junru
AU  - Song, Yiran
AU  - Kong, Liya
JO  - Expert Systems with Applications
VL  - 299
SP  - 130105
PY  - 2026
DA  - 2026/03/01/
SN  - 0957-4174
DO  - https://doi.org/10.1016/j.eswa.2025.130105
UR  - https://www.sciencedirect.com/science/article/pii/S0957417425037212
KW  - Vulnerability detection
KW  - Large language model
KW  - Open source software
KW  - Security
AB  - Large language models (LLMs) have shown promise in vulnerability detection but often produce hallucinations and lack precise logical reasoning, leading to high false positive rates and limited practical trust. The core challenge lies in reconciling the LLM’s strength in semantic generalization with the need for precise numeric and logical verification. To address this, we propose VulX, a framework that aims to achieve both high accuracy and high explainability in vulnerability detection. Our key innovation is a structured prompting language (SPL) based on first-order logic, which serves as an intermediary representation between formalized vulnerability semantics and LLM inference. SPL enables the LLM to learn vulnerability patterns with semantic guidance while being constrained by logical rules. Crucially, these SPL expressions are not only used to train the LLM but also serve as executable specifications for an agent reasoning architecture. Specialized agents use these specifications to perform fine-grained, step-by-step verification of variable bounds, pointer validity, and control-flow dependencies, systematically refining the LLM’s initial hypotheses. This unified representation allows the system to rapidly adapt to new types of vulnerability by defining their logical conditions and translating them into SPL queries. Evaluated on three real-world datasets (FFmpeg+Qemu, Real-Vul, DiverseVul), VulX achieves substantial improvements in F1 score, average outperforming baseline tools by 36.5 %, 15.3 %, and 17.4 %, respectively. It successfully identified five previously unknown vulnerabilities in open-source projects. Expert evaluation confirms that over half of its reasoning paths align with professional security practices. VulX demonstrates a new paradigm for building reliable, interpretable, and adaptable LLM-based security analysis tools.
ER  - 

TY  - JOUR
T1  - A classification-by-retrieval framework for few-shot anomaly detection to detect API injection
AU  - Aharon, Udi
AU  - Dubin, Ran
AU  - Dvir, Amit
AU  - Hajaj, Chen
JO  - Computers & Security
VL  - 150
SP  - 104249
PY  - 2025
DA  - 2025/03/01/
SN  - 0167-4048
DO  - https://doi.org/10.1016/j.cose.2024.104249
UR  - https://www.sciencedirect.com/science/article/pii/S0167404824005558
KW  - API security
KW  - Anomaly detection
KW  - Few-shot learning
KW  - Classification-by-retrieval
KW  - ANN
KW  - NLP
AB  - Application Programming Interface (API) Injection attacks refer to the unauthorized or malicious use of APIs, which are often exploited to gain access to sensitive data or manipulate online systems for illicit purposes. Identifying actors that deceitfully utilize an API poses a demanding problem. Although there have been notable advancements and contributions in the field of API security, there remains a significant challenge when dealing with attackers who use novel approaches that do not match the well-known payloads commonly seen in attacks. Also, attackers may exploit standard functionalities unconventionally and with objectives surpassing their intended boundaries. Thus, API security needs to be more sophisticated and dynamic than ever, with advanced computational intelligence methods, such as machine learning models that can quickly identify and respond to abnormal behavior. In response to these challenges, we propose a novel unsupervised few-shot anomaly detection framework composed of two main parts: First, we train a dedicated generic language model for API based on FastText embedding. Next, we use Approximate Nearest Neighbor search in a classification-by-retrieval approach. Our framework allows for training a fast, lightweight classification model using only a few examples of normal API requests. We evaluated the performance of our framework using the CSIC 2010 and ATRDF 2023 datasets. The results demonstrate that our framework improves API attack detection accuracy compared to the state-of-the-art (SOTA) unsupervised anomaly detection baselines.
ER  - 

TY  - JOUR
T1  - TIJERE: A novel threat intelligence joint extraction model based on analyst expert knowledge
AU  - Mouiche, Inoussa
AU  - Saad, Sherif
JO  - Knowledge-Based Systems
VL  - 329
SP  - 114346
PY  - 2025
DA  - 2025/11/04/
SN  - 0950-7051
DO  - https://doi.org/10.1016/j.knosys.2025.114346
UR  - https://www.sciencedirect.com/science/article/pii/S0950705125013851
KW  - Threat intelligence joint entity and relation extraction
KW  - Multisequence labeling representation
KW  - Expert domain features
KW  - Cyber threat intelligence
KW  - Cyber knowledge graphs
KW  - Pipeline extraction
KW  - Joint extraction
AB  - The extraction of entities and relationships from threat intelligence reports into structured formats, such as cybersecurity knowledge graphs, is essential for automated threat analysis, detection, and mitigation. However, existing joint extraction methods struggle with feature confusion, language ambiguity, noise propagation, and overlapping relations, resulting in low accuracy and poor model performance. This paper presents TIJERE, an innovative joint entity and relation extraction framework that formulates joint extraction as a multisequence labeling representation (MSLR) problem. Specifically, separate sequences are generated for each entity pair. Unlike prior tagging schemes, MSLR integrates expert domain features to enrich positional, contextual, and semantic representations of entities, thereby enhancing feature distinction and classification accuracy. Additionally, TIJERE reduces language ambiguity and enhances domain-specific generalization by leveraging SecureBERT+, a contextual language model fine-tuned on cybersecurity text. This improves both named entity recognition (NER) and relation extraction (RE). This paper also introduces DNRTI-JE, the first publicly available jointly labeled dataset for cybersecurity entity and RE, filling a crucial gap in cyber threat intelligence automation. Empirical evaluations on the curated DNRTI-JE dataset demonstrate that TIJERE achieves state-of-the-art performance, with F1-scores exceeding 0.93 for NER and 0.98 for RE, outperforming existing methods. Together, TIJERE and the standardized benchmarking DNRTI-JE dataset enable high-performance cybersecurity intelligence extraction, with transferable applications in healthcare, finance, and bioinformatics.
ER  - 

TY  - JOUR
T1  - Periodic watermarking for copyright protection of large language models in cloud computing security
AU  - Ye, Pei-Gen
AU  - Li, Zhengdao
AU  - Yang, Zuopeng
AU  - Chen, Pengyu
AU  - Zhang, Zhenxin
AU  - Li, Ning
AU  - Zheng, Jun
JO  - Computer Standards & Interfaces
VL  - 94
SP  - 103983
PY  - 2025
DA  - 2025/08/01/
SN  - 0920-5489
DO  - https://doi.org/10.1016/j.csi.2025.103983
UR  - https://www.sciencedirect.com/science/article/pii/S0920548925000121
KW  - Large language model
KW  - Watermark technology
KW  - Copyright protection
KW  - Cloud computing security
AB  - Large Language Models (LLMs) have become integral in advancing content understanding and generation, leading to the proliferation of Embedding as a Service (EaaS) within cloud computing platforms. EaaS leverages LLMs to offer scalable, on-demand linguistic embeddings, enhancing various cloud-based applications. However, the proprietary nature of EaaS makes it a target for model extraction attacks, where the timing of such infringements often remains elusive. This paper introduces TimeMarker, a novel framework that enhances temporal traceability in cloud computing environments by embedding distinct watermarks at different sub-periods, marking the first attempt to identify the timing of model extraction attacks. TimeMarker employs an adaptive watermark strength method based on information entropy and frequency domain transformations to refine the detection accuracy of model extraction attacks within cloud infrastructures. The granularity of time frame identification for theft improves as more sub-periods are used. Furthermore, our approach investigates single sub-period theft and extends to multi-sub-period theft scenarios where attackers steal data across many sub-periods to train their models in cloud settings. Validated across five widely used datasets, TimeMarker is capable of detecting model extraction over various sub-periods and assessing its impact on the accuracy and robustness of large models deployed in the cloud. The results demonstrate that TimeMarker effectively identifies different periods of extraction attacks, enhancing EaaS security within cloud computing and extending traditional watermarking to offer copyright protection for LLMs.
ER  - 

TY  - JOUR
T1  - EnhanceCTI: Enhanced semantic filtering and feature extraction framework for industry-specific cyber threat intelligence
AU  - Chen, Sheng-Shan
AU  - Pai, Tun-Wen
AU  - Sun, Chin-Yu
JO  - Computers & Security
VL  - 158
SP  - 104649
PY  - 2025
DA  - 2025/11/01/
SN  - 0167-4048
DO  - https://doi.org/10.1016/j.cose.2025.104649
UR  - https://www.sciencedirect.com/science/article/pii/S0167404825003384
KW  - Cyber threat intelligence (CTI)
KW  - Threat intelligence platform (TIP)
KW  - Natural language processing (NLP)
KW  - Industry-specific classification
KW  - Semantic similarity
AB  - The rapid digitization of various industries has created an urgent need for robust cyber threat intelligence (CTI) systems. Organizations are increasingly developing cyber threat intelligence platforms (TIPs) to gather open-source intelligence (OSINT) and transform it into actionable defenses against information security breaches. However, the overwhelming volume and complexity of OSINT data, often including false or misleading information, pose significant challenges for effective CTI analysis. This study introduces EnhanceCTI, a novel system designed to improve the quality and industry-specific applicability of threat intelligence. EnhanceCTI employs an enhanced bidirectional encoder representations from transformers (DistilBERT)-based semantic filtering method to filter intelligence data and determine its alignment with industry-specific data extracted from TIPs. This filtering is applied across eight major industries: healthcare, finance, government, technology, education, telecommunications, critical infrastructure, and a miscellaneous “others” category. Additionally, EnhanceCTI leverages high-credibility CTI features, integrating them with SentenceBERT to create a merging judgment model. This model determines whether a given piece of intelligence should be merged with existing data or stored independently, thereby ensuring relevance and minimizing redundancy. Finally, a dedicated platform was developed, providing cybersecurity analysts with tools to rapidly assess both intelligence quality and the accuracy of industry-specific classification models. Experimental results demonstrate EnhanceCTI’s effectiveness, achieving an F1-score of 0.99 for intelligence identification and a 0.89 cosine Pearson correlation for SentenceBERT. A random forest algorithm, trained on 750 manually annotated samples, achieved an F1-score of 0.97 on the merging judgment model. These findings highlight EnhanceCTI’s ability to accurately identify threats, offering a valuable, industry-tailored solution for institutions facing the growing challenges of cybersecurity in the modern digital landscape.
ER  - 

TY  - JOUR
T1  - Cyber threat indicators extraction based on contextual knowledge prompt
AU  - Tang, Hailiang
AU  - Lin, Dawei
AU  - Li, Wanyu
AU  - Zhang, Wenxiao
AU  - Zhao, Jun
JO  - Computer Networks
VL  - 254
SP  - 110839
PY  - 2024
DA  - 2024/12/01/
SN  - 1389-1286
DO  - https://doi.org/10.1016/j.comnet.2024.110839
UR  - https://www.sciencedirect.com/science/article/pii/S1389128624006716
KW  - Cyber threat intelligence
KW  - Cyber security
KW  - IOC extraction
KW  - Interpretability
KW  - Social data
AB  - Extracting Indicators of Compromise (IOC) from security-related social data (e.g., security blogs, hacker forums) is crucial for predicting cyber risks and mitigating cyber attacks proactively. However, existing IOC extraction approaches suffer from two serious limitations. First, they fail to learn the multiculti-granular and fine-grained IOC features, resulting in high false positives. Second, current methods cannot incorporate symbolic rules and contextual knowledge, resulting in poor interpretability. In this paper, we propose AIIOC, an Accurate and Interpretable I O C extraction model based on contextual knowledge prompts. Particularly, AIIOC first proposes a multi-granularity attention mechanism to learn fine-grained IOC features and boost the accuracy of IOC identification. Additionally, AIIOC designs a novel sequence labeling method that integrates symbolic rules and contextual knowledge prompts, which can encode symbolic rules and contextual semantics of IOC in trainable recurrent neural networks to improve both accuracy and interpretability. Experimental results on two real-world datasets verify that AIIOC outperforms state-of-the-art methods and showcases promising interpretability by incorporating symbolic rules and contextual knowledge prompts.
ER  - 

TY  - JOUR
T1  - Enhancing reliability in LLM-integrated robotic systems: A unified approach to security and safety
AU  - Zhang, Wenxiao
AU  - Kong, Xiangrui
AU  - Dewitt, Conan
AU  - Bräunl, Thomas
AU  - Hong, Jin B.
JO  - Journal of Systems and Software
VL  - 231
SP  - 112614
PY  - 2026
DA  - 2026/01/01/
SN  - 0164-1212
DO  - https://doi.org/10.1016/j.jss.2025.112614
UR  - https://www.sciencedirect.com/science/article/pii/S0164121225002833
KW  - LLM
KW  - Robotics
KW  - Navigation
KW  - Reliability
AB  - Integrating Large Language Models (LLMs) into robotic systems has revolutionised embodied artificial intelligence, enabling advanced decision-making and adaptability. However, ensuring reliability — encompassing both security against adversarial attacks and safety in complex environments — remains a critical challenge. To address this, we propose a unified framework that mitigates prompt injection attacks while enforcing operational safety through robust validation mechanisms. Our approach combines prompt assembling, state management, and safety validation, evaluated using both performance and security metrics. Experiments show a 30.8% improvement under injection attacks and up to a 325% improvement in complex environment settings under adversarial conditions compared to baseline scenarios. This work bridges the gap between safety and security in LLM-based robotic systems, offering actionable insights for deploying reliable LLM-integrated mobile robots in real-world settings. The framework is open-sourced with simulation and physical deployment demos at https://llmeyesim.vercel.app/.
ER  - 

TY  - JOUR
T1  - Large language model-based interpretable machine learning control in building energy systems
AU  - Zhang, Liang
AU  - Chen, Zhelun
JO  - Energy and Buildings
VL  - 313
SP  - 114278
PY  - 2024
DA  - 2024/06/15/
SN  - 0378-7788
DO  - https://doi.org/10.1016/j.enbuild.2024.114278
UR  - https://www.sciencedirect.com/science/article/pii/S0378778824003943
KW  - Building control
KW  - Machine learning control
KW  - Interpretable machine learning
KW  - Shapley value
KW  - Large language model
AB  - The potential of Machine Learning Control (MLC) in HVAC systems is hindered by its opaque nature and inference mechanisms, which is challenging for users and modelers to fully comprehend, ultimately leading to a lack of trust in MLC-based decision-making. To address this challenge, this paper investigates and explores Interpretable Machine Learning (IML), a branch of Machine Learning (ML) that enhances transparency and understanding of models and their inferences, to improve the credibility of MLC and its industrial application in HVAC systems. Specifically, we developed an innovative framework that combines the principles of Shapley values and the in-context learning feature of Large Language Models (LLMs). While the Shapley values are instrumental in dissecting the contributions of various features in ML models, LLM provides an in-depth understanding of the non-data-driven or rule-based elements in MLC; combining them, LLM further packages these insights into a coherent, human-understandable narrative. The paper presents a case study to demonstrate the feasibility of the developed IML framework for model predictive control-based precooling under demand response events in a virtual testbed. The results indicate that the developed framework generates and explains the control signals in accordance with the rule-based rationale.
ER  - 

TY  - JOUR
T1  - Revolutionizing power electronics design through large language models: Applications and future directions
AU  - Ibrahim, Khalifa Aliyu
AU  - Luk, Patrick Chi-Kwong
AU  - Luo, Zhenhua
AU  - Ng, Seng Yim
AU  - Harrison, Lee
JO  - Computers and Electrical Engineering
VL  - 123
SP  - 110248
PY  - 2025
DA  - 2025/04/01/
SN  - 0045-7906
DO  - https://doi.org/10.1016/j.compeleceng.2025.110248
UR  - https://www.sciencedirect.com/science/article/pii/S0045790625001910
KW  - Artificial intelligence
KW  - Power electronics design
KW  - AI driven design
KW  - Large language model
KW  - High frequency AC (HFAC)
KW  - Wireless power transfer
KW  - Generative pre-train transformer (GPT)
AB  - The design of electronic circuits is critical for a wide range of applications, from the electrification of transportation to the Internet of Things (IoT). It demands substantial resources, is time-intensive, and can be highly intricate. Current design methods often lead to inefficiencies, prolonged design cycles, and susceptibility to human error. Advancements in artificial intelligence (AI) play a crucial role in power electronics design by increasing efficiency, promoting automation, and enhancing sustainability of electrical systems. Research has demonstrated the applications of AI in power electronics to enhance system performance, optimization, and control strategy using machine learning, fuzzy logic, expert systems, and metaheuristic methods. However, a review that includes the recent AI advancements and potential of large language models (LLMs) like generative pre-train transformers (GPT) has not been reported. This paper presents an overview of applications of AI in power electronics (PE) including the potential of LLMs. The influence of LLMs-AI on the design process of PE and future research directions is also highlighted. The development of advanced AI algorithms such as pre-train transformers, real-time implementations, interdisciplinary collaboration, and data-driven approaches are also discussed. The proposed LLMs-AI is used to design parameters of high-frequency wireless power transfer (HFWPT) using MATLAB as a first case study, and high-frequency alternating current (HFAC) inverter using PSIM as a second case study. The proposed LLM-AI driven design is verified based on a similar design reported in the literature and Wilcoxon signed-rank test was conducted to further validate the result. Results show that the LLM-AI driven design based on the OpenAI foundation model has the potential to streamline the design process of power electronics. These findings provide a good reference on the feasibility of LLMs-AI on power electronic design.
ER  - 

TY  - JOUR
T1  - IDS-GPT: A Novel Deep Learning-Powered Framework for Network Traffic Intrusion Detection
AU  - Saidi, Firas
JO  - Procedia Computer Science
VL  - 270
SP  - 1611
EP  - 1620
PY  - 2025
DA  - 2025/01/01/
T2  - 29th International Conference on Knowledge-Based and Intelligent Information & Engineering Systems (KES 2025)
SN  - 1877-0509
DO  - https://doi.org/10.1016/j.procs.2025.09.281
UR  - https://www.sciencedirect.com/science/article/pii/S1877050925029540
KW  - Network security
KW  - IDS
KW  - IDS-GPT
KW  - Deep Learning
KW  - LLM
KW  - Fine-tuning
KW  - Accuracy
AB  - Intrusion Detection Systems (IDS) play a pivotal role in network security, serving as essential tools to detect and mitigate malicious activities, thereby ensuring the protection of systems and the integrity of sensitive data. The rapid evolution of artificial intelligence, particularly in the realms of machine Learning (ML) and Deep Learning (DL), has ushered in groundbreaking methodologies that substantially improve the effectiveness of IDS. This paper investigates the utilization of ChatGPT, an advanced deep learning-driven language model, for the detection of malicious intrusions through the analysis of network traffic. By harnessing ChatGPT’s superior contextual understanding and analytical capabilities, we introduce IDS-GPT as an innovative model that achieves remarkable precision in identifying anomalies within network traffic. Rigorous testing on benchmark dataset CICIDS2018 and comparison to existing techniques such as Decision Tree, CNN-BiLSTM and GBM, reveal IDS-GPT ’s outstanding performance in detecting malicious activities, delivering high accuracy while maintaining exceptionally low false-positive rates. The findings, supported by a robust suite of precise evaluation metrics, underscore the transformative impact of integrating ChatGPT into IDS frameworks. This advancement sets the stage for the development of more resilient and intelligent cybersecurity solutions, capable of addressing the growing complexities of modern network threats.
ER  - 

TY  - JOUR
T1  - Role of cybersecurity for a secure global communication eco-system: A comprehensive cyber risk assessment for satellite communications
AU  - Ansong, Samuel
AU  - Rankothge, Windhya
AU  - Sadeghi, Somayeh
AU  - Mohammadian, Hesamodin
AU  - Rashid, Farrukh Bin
AU  - Ghorbani, Ali
JO  - Computers & Security
VL  - 149
SP  - 104156
PY  - 2025
DA  - 2025/02/01/
SN  - 0167-4048
DO  - https://doi.org/10.1016/j.cose.2024.104156
UR  - https://www.sciencedirect.com/science/article/pii/S0167404824004619
KW  - Satellite communication
KW  - Cyber threats
KW  - Risk assessment
AB  - In an age where global connectivity has become pivotal to socio-economic development, satellite communication (SATCOM) systems have become the backbone of modern telecommunication infrastructure. However, the increasing reliance on SATCOM also elevates the potential impact of cyber threats. Cyber risk assessment is a critical component of any satellite communications risk management strategy. It plays a pivotal role in identifying and managing risks to satellite communications, which helps stakeholders isolate the most critical threats and select the appropriate cybersecurity measures. To the best of our knowledge, the field of satellite communications lacks an established framework for cyber risk assessment. Moreover, previous research work has focused only on a limited number of security threats and categories. Therefore, in this paper, we propose a comprehensive risk assessment methodology to qualitatively assess the risk associated with satellite communications cyber threats, following the NIST special publication 800-30: Guide for Conducting Risk Assessments. We analyze existing literature and real-world scenarios to identify potential satellite communications cyber threats and employ the STRIDE threat model for threat modeling. We validate the proposed methodology by performing a risk assessment for the cyber threats identified. Finally, we discuss existing challenges and open research problems for satellite communications cyber risk assessment.
ER  - 

TY  - JOUR
T1  - CTFAgent: An LLM-powered Agent for CTF Challenge Solving
AU  - Zou, Yuwen
AU  - Liu, Jia
AU  - Fan, Wenjun
JO  - Journal of Information Security and Applications
VL  - 96
SP  - 104305
PY  - 2026
DA  - 2026/01/01/
SN  - 2214-2126
DO  - https://doi.org/10.1016/j.jisa.2025.104305
UR  - https://www.sciencedirect.com/science/article/pii/S2214212625003424
KW  - Large Language Models
KW  - LLM agent
KW  - Capture the Flag
KW  - Cybersecurity
AB  - Capture-the-Flag (CTF) competitions play an important role in the cybersecurity landscape by simulating realistic attack and defense scenarios and offering diverse categories of challenges. This diversity demands flexible reasoning and adaptive problem-solving, which traditional automation tools struggle to provide, as they are typically designed for specific tasks. Large Language Models (LLMs) with their vast knowledge and strong reasoning capabilities, present a promising approach to overcome these limitations. In this work, we propose CTFAgent, an LLM-powered agent featuring a new plan-and-execute paradigm with a stateful task tree for long-horizon reasoning. To handle diverse challenges, CTFAgent integrates challenge-specific prompting and specialized tools for multimodal analysis and concrete operations. The agent comprises two modes: a fully automated mode and a human-in-the-loop (HITL) mode, which incorporates human operational support for tool execution beyond the automation. Evaluated on challenges from PicoCTF with GPT-4o, Gemini-2.5-Pro and DeepSeek-V3, CTFAgent outperforms 88% of human teams in its automated mode. This performance rises significantly in HITL mode, where it surpasses approximately 94% of teams. These results demonstrate that CTFAgent can effectively solve a wide range of complex tasks, highlighting the potential of LLM-powered agents to advance autonomous cybersecurity solutions.
ER  - 

TY  - JOUR
T1  - Golden Jackal Driven Optimization for a transparent and interpretable Intrusion Detection System using explainable AI to revolutionize cybersecurity
AU  - Shah, Syed Haider Ali
AU  - Akhtar, Lalarukh Haseeb
AU  - Ali, Muhammad Nadeem
AU  - Kim, Byung-Seo
JO  - Egyptian Informatics Journal
VL  - 32
SP  - 100837
PY  - 2025
DA  - 2025/12/01/
SN  - 1110-8665
DO  - https://doi.org/10.1016/j.eij.2025.100837
UR  - https://www.sciencedirect.com/science/article/pii/S1110866525002300
KW  - IDS
KW  - XAI
KW  - DL
KW  - DoS
KW  - GJOA
KW  - Digital contents
KW  - Copyright protection
AB  - The increasing connectivity of today’s digital world and contents necessitate robust security solutions to address the escalating range of cyber threats as well as Copyright infringement. Intrusion Detection Systems (IDS) have emerged as essential tools for managing, analyzing, and generating cybersecurity responses against potential cyberattacks. However, IDS face persistent challenges, including detection performance, deployment efficiency, and reliability in real-time scenarios, which remain active areas of research. In addition to these challenges, a novel issue has recently gained attention: the lack of explainability and transparency in IDS predictions. This limitation significantly affects security practitioners’ confidence in system reliability and restricts the practical utilization of the insights produced. To address these concerns, this paper proposes a joint approach to enhance both the performance and explainability of IDS by integrating the Golden Jackal Optimization (GJO) algorithm for cyberattack detection. Furthermore, we incorporate Explainable Artificial Intelligence (XAI) to provide a clear and comprehensive interpretation of the model’s predictions. Notably, the proposed XAI-based model achieved an impressive accuracy of 99.82% and a miss rate of just 0.19%, underscoring its efficiency, trustworthiness, transparency, and interpretability key attributes essential for human operators managing intelligent cybersecurity systems.
ER  - 

TY  - JOUR
T1  - Automating software size measurement with language models: Insights from industrial case studies
AU  - Ünlü, Hüseyin
AU  - Tenekeci, Samet
AU  - Kennouche, Dhia Eddine
AU  - Demirörs, Onur
JO  - Journal of Systems and Software
VL  - 231
SP  - 112638
PY  - 2026
DA  - 2026/01/01/
SN  - 0164-1212
DO  - https://doi.org/10.1016/j.jss.2025.112638
UR  - https://www.sciencedirect.com/science/article/pii/S0164121225003073
KW  - Software size measurement
KW  - COSMIC
KW  - MicroM
KW  - Natural language processing
KW  - NLP
KW  - BERT
KW  - Case study
AB  - Objective software size measurement is critical for accurate effort estimation, yet many organizations avoid it due to high costs, required expertise, and time-consuming manual effort. This often leads to vague predictions, poor planning, and project overruns. To address this challenge, we investigate the use of pre-trained language models — BERT and SE-BERT — to automate size measurement based on textual requirements using COSMIC and MicroM methods. We constructed one heterogeneous dataset and two industrial datasets, each manually measured by experienced analysts. Models were evaluated in three settings: (i) generic model evaluation, where the models are trained and tested on heterogeneous data, (ii) internal evaluation, where the models are trained and tested on organization-specific data, and (iii) external evaluation, where generic models were tested on organization-specific data. Results show that organization-specific models significantly outperform generic models, indicating that aligning training data with the target organization’s requirement style is critical for accuracy. SE-BERT, a domain-adapted variant of BERT, improves performance, particularly in low-resource settings. These findings highlight the practical potential of tailoring training data for broader adoption and cost-effective software size measurement in industrial contexts.
ER  - 

TY  - JOUR
T1  - Cybersecurity in smart microgrids using blockchain-federated learning and quantum-safe approaches: A comprehensive review
AU  - Ahmad, Jameel
AU  - Rizwan, Muhammad
AU  - Ali, Syed Farooq
AU  - Inayat, Usman
AU  - Abdul Muqeet, Hafiz
AU  - Imran, Muhammad
AU  - Awotwe, Tabbi
JO  - Applied Energy
VL  - 393
SP  - 126118
PY  - 2025
DA  - 2025/09/01/
SN  - 0306-2619
DO  - https://doi.org/10.1016/j.apenergy.2025.126118
UR  - https://www.sciencedirect.com/science/article/pii/S0306261925008487
KW  - AC microgrid
KW  - Blockchain
KW  - Cybersecurity
KW  - DC microgrid
KW  - Federated learning
KW  - Internet of energy
KW  - Machine leaning
KW  - Quantum computing
AB  - Smart microgrids are decentralized energy systems that effectively integrate renewable energy sources, energy storage technologies, and advanced communication and control frameworks, thereby facilitating efficient and sustainable energy distribution while enhancing grid resilience. These systems empower active participation from consumers and prosumers in energy trading, which significantly transforms traditional energy management practices. However, the increased connectivity and dependence on digital infrastructure inherent in smart microgrids introduce substantial cybersecurity vulnerabilities, underscoring the necessity for robust security protocols. This article provides a comprehensive review of cybersecurity threats directed at distributed generation in both AC and DC microgrids, energy trading platforms, and transactive energy management frameworks within the broader context of the smart grid. We systematically analyze both conventional and sophisticated stealth cyberattacks, identifying critical countermeasures essential for safeguarding modern smart grids. Furthermore, we explore the integration of emerging technologies, including machine learning, federated learning, blockchain security, and quantum-safe cryptographic mechanisms, as synergistic strategies to enhance cyber resilience in smart microgrids. Ultimately, this study identifies existing research gaps, barriers to adopting emerging technologies and proposes future research directions, with the goal of advancing the cybersecurity of these complex and evolving energy systems.
ER  - 

TY  - JOUR
T1  - Large language Models-empowered automatic knowledge graph development based on multi-modal data for building health resilience
AU  - Shan, Tianlong
AU  - Zhang, Fan
AU  - Chan, Albert P.C.
AU  - Zhu, Shiyao
AU  - Li, Kaijian
JO  - Advanced Engineering Informatics
VL  - 68
SP  - 103655
PY  - 2025
DA  - 2025/11/01/
SN  - 1474-0346
DO  - https://doi.org/10.1016/j.aei.2025.103655
UR  - https://www.sciencedirect.com/science/article/pii/S1474034625005488
KW  - Building health resilience
KW  - Knowledge graph
KW  - Large language models
KW  - Multi-modal data
KW  - Rainstorm
AB  - Improving the health resilience of building (BHR) helps keep stable health status of both the building and its occupants under disasters. As BHR is an emerging concept, there is no structured knowledge graph to understand the whole process of BHR under disasters. Therefore, this study aims to build a structured BHR knowledge graph based on multi-modal data, providing sufficient structured knowledge for BHR enhancement. An automated knowledge graph construction approach is proposed to empower the ontology design and triple extraction by large language models (LLMs), and validation processes based on In-context Learning (ICL) prompts. A case study is conducted to construct the knowledge graph of BHR under rainstorms in Hong Kong. The performance of the proposed LLMs-empowered knowledge extraction is also validated based on natural language processing metrics and LLMs-based Evaluation (LLMs-Eval). BHR knowledge graph indicates the potential relations between disasters, factors, response actions, and the health status of the building and occupants, and provides insight to guide the BHR enhancement. The superiority of the proposed LLMs-empowered automated knowledge graph construction approach is proven, implying LLMs have great potential in knowledge graph construction, not only for BHR but also for other concepts that require structured knowledge for further explorations and analyses.
ER  - 

TY  - JOUR
T1  - Leveraging large language models for Human-Machine collaborative troubleshooting of complex industrial equipment faults
AU  - Wen, Sijie
AU  - Li, Fei
AU  - Zhuang, Weibin
AU  - Pan, Xinyu
AU  - Yu, Weigang
AU  - Bao, Jinsong
AU  - Li, Xinyu
JO  - Advanced Engineering Informatics
VL  - 65
SP  - 103235
PY  - 2025
DA  - 2025/05/01/
SN  - 1474-0346
DO  - https://doi.org/10.1016/j.aei.2025.103235
UR  - https://www.sciencedirect.com/science/article/pii/S1474034625001284
KW  - Fault Troubleshooting
KW  - Human-Machine Collaboration
KW  - Industrial Knowledge Graph
KW  - Large Language Models
KW  - Knowledge Extraction and Reasoning
AB  - The continuous advancement of mechanical manufacturing technology has significantly elevated the importance of large mechanical equipment in industrial enterprises, necessitating effective fault diagnosis methods to ensure operational efficiency and safety. Traditional manual fault diagnosis methods are increasingly proving inefficient, especially when dealing with long fault chains in complex industrial equipment. Moreover, the harsh and corrosive environments in factories often prevent the large-scale deployment of sensors, making automated fault detection methods challenging to implement. This paper introduces an innovative fault diagnosis approach that leverages Large Language Models (LLMs) to enhance human–machine collaborative troubleshooting for complex industrial equipment faults. By employing methods based on LLM, such as triplet extraction, complex semantic mapping, and “5-why” causal analysis, this approach achieves semantically lossless knowledge parsing and extraction of fault knowledge. An Enhanced Causal Fault Knowledge Graph (ECFKG) is constructed, which integrates theory-based and scenario-based knowledge graphs to support fault troubleshooting. Additionally, an Interactive Diagnostic and Troubleshooting Method (IDTM) is introduced, utilizing human–machine collaboration in the fault troubleshooting process to enable dynamic fault diagnosis, thereby enhancing the accuracy and efficiency of diagnosing and resolving faults. A case study on a bridge crane in a steel mill demonstrates the feasibility and improved performance of the proposed methods in real-life scenarios. This study contributes to the field by effectively integrating human intuition with machine computational power, setting a precedent for future advancements in cognitive intelligence-enabled fault diagnosis systems. This research paves the way for more sophisticated, real-time troubleshooting solutions in industrial settings.
ER  - 

TY  - JOUR
T1  - Hyper attack graph: Constructing a hypergraph for cyber threat intelligence analysis
AU  - Jia, Junbo
AU  - Yang, Li
AU  - Wang, Yuchen
AU  - Sang, Anyuan
JO  - Computers & Security
VL  - 149
SP  - 104194
PY  - 2025
DA  - 2025/02/01/
SN  - 0167-4048
DO  - https://doi.org/10.1016/j.cose.2024.104194
UR  - https://www.sciencedirect.com/science/article/pii/S0167404824004991
KW  - Cyber threat intelligence
KW  - Hypergraph
KW  - Relation extraction
KW  - Knowledge graph
AB  - Cybersecurity experts are actively exploring and implementing automated technologies to extract and present attack information from Cyber Threat Intelligence. However, there are multiple relations among security entities within Cyber Threat Intelligence, a feature that existing technologies often overlook. Additionally, integrating external security knowledge into cyber threat intelligence intuitively during analysis and presentation poses challenges. We propose the Hyper Attack Graph (HAG) framework, the first work to apply hypergraph data structures in the analysis of cyber threat intelligence. Our approach uses a joint extraction model that incorporates a multi-head selection mechanism, effectively addressing the extraction of multiple relations among security entities. We use hypergraph to display tactics and techniques in cyber threat intelligence. Our evaluation of the HAG framework on 685 real-world cyber threat intelligence reports shows an increase in the F1 score for security entity extraction by 11.12% and for relation extraction by 6.71% over existing efforts. Furthermore, HAG’s ability to visually represent external security knowledge on hypergraphs demonstrates its potential as a valuable tool in cybersecurity analysis.
ER  - 

TY  - JOUR
T1  - Exploring and mitigating fawning hallucinations in large language models
AU  - Shangguan, Zixuan
AU  - Dong, Yanjie
AU  - Wang, Lanjun
AU  - Fan, Xiaoyi
AU  - Leung, Victor C.M.
AU  - Hu, Xiping
JO  - Neurocomputing
VL  - 665
SP  - 132166
PY  - 2026
DA  - 2026/02/07/
SN  - 0925-2312
DO  - https://doi.org/10.1016/j.neucom.2025.132166
UR  - https://www.sciencedirect.com/science/article/pii/S0925231225028383
KW  - Hallucination
KW  - Large language models
KW  - Contrastive decoding
AB  - Large language models (LLMs) have demonstrated exceptional proficiency in language understanding. However, when LLMs align their outputs with deceptive and/or misleading prompts, the generated responses can deviate from the de facto information. Such observations are known as fawning hallucinations, where the model prioritizes alignment with the input’s implied perspective over accuracy and truthfulness. In this work, we analyze fawning hallucinations in various natural language processing tasks and tailor the so-called contrastive decoding method for fawning-hallucination mitigation. Specifically, we design two paradigms to generate corresponding deceptive and/or misleading inputs for the consistent induction of fawning hallucinations. Then, we propose the collaborative contrastive decoding (CCD) method to handle the fawning hallucinations across different tasks in LLMs. By contrasting the deviation in output distribution between induced and transformed neutral inputs, the proposed CCD can reduce reliance on deceptive and/or misleading information without requiring additional training. Extensive experiments demonstrate that the proposed CCD can effectively mitigate fawning hallucinations and improve the factuality of the generated responses across various tasks.
ER  - 

TY  - JOUR
T1  - Hierarchical multimodal robust spam detection using Large Language Models and Convolutional Networks
AU  - Gahar, Rania Mkhinini
AU  - Hidri, Adel
AU  - Arfaoui, Olfa
AU  - Hidri, Minyar Sassi
JO  - Procedia Computer Science
VL  - 270
SP  - 3879
EP  - 3888
PY  - 2025
DA  - 2025/01/01/
T2  - 29th International Conference on Knowledge-Based and Intelligent Information & Engineering Systems (KES 2025)
SN  - 1877-0509
DO  - https://doi.org/10.1016/j.procs.2025.09.513
UR  - https://www.sciencedirect.com/science/article/pii/S1877050925031862
KW  - Large Language Models
KW  - Convolutional Networks
KW  - Multimodal SPAM detection
KW  - Universal Sentence Encoder
AB  - With the growth in the use of email and social media, spam has become a major challenge. With the rise of multimedia technologies, the prevalence of multimodal spam containing a mixture of text and images has significantly increased. However, most of the methods proposed to detect spam in the past are mainly based on text analysis. The development of a multimodal approach to spam filtering is therefore of paramount importance. The paper aims to develop an improved method for multimedia spam detection using hierarchical and multimodal message analysis, combined with deep learning (DL). Our approach is based on extracting several representative characteristics from multimodal data (text, links, images) using models based on large language models and convolutional networks. This aims to obtain a fine-grained semantic representation with focus on the key elements of the messages for more effective classification of multimedia spam. We experimented our method on a large corpus and used qualitative and quantitative analysis to compare accuracy and robustness. The experiments demonstrate the model’s capability to detect effectively complex spam strategies, reflecting significant improvement in spam identifying techniques.
ER  - 

TY  - JOUR
T1  - Considerations for adapting higher education technology courses for AI large language models: A critical review of the impact of ChatGPT
AU  - Tayan, Omar
AU  - Hassan, Ali
AU  - Khankan, Khaled
AU  - Askool, Sanaa
JO  - Machine Learning with Applications
VL  - 15
SP  - 100513
PY  - 2024
DA  - 2024/03/01/
SN  - 2666-8270
DO  - https://doi.org/10.1016/j.mlwa.2023.100513
UR  - https://www.sciencedirect.com/science/article/pii/S266682702300066X
KW  - Artificial intelligence
KW  - ChatGPT
KW  - Higher education
KW  - Machine learning
AB  - Following the very recent launch of the ChatGPT chatbot, numerous comments and speculations were posted concerning the potential aspects of society that are expected to benefit from this AI revolution. In particular, the education sector is considered as one of the primary domains affected by this application, the impact of which remains yet to be fully understood. Furthermore, many Higher Education institutions are required to get to terms with its impact on teaching and learning, and to clarify their stances on the use of ChatGPT software. This study was developed to investigate some critical case studies considered as relevant to the inevitable re-evaluation of educational aspects needed, ranging from academic missions to student and course learning outcomes and its ethical uses. Following a review of some of the pros and cons of ChatGPT in the higher educational sector, this paper shall demonstrate several case studies of early trials in teaching and learning assessments related to various specializations. Next, the ability of some well-known AI detector software and analyzed in terms of their capacity to successfully detect AI-generated content. Analysis shall be made of the foreseen impact on important aspects including challenges and benefits related to its use in course assessments as well as academic integrity and ethical use. The study concludes with a set of recommendations made from our findings and benchmarks obtained from top universities in order to assist faculty members and decision makers at Higher Education institutions concerning their response strategy and use of ChatGPT.
ER  - 

TY  - JOUR
T1  - A large language model-based platform for real-time building monitoring and occupant interaction
AU  - Xu, Yifang
AU  - Zhu, Siyao
AU  - Cai, Jiannan
AU  - Chen, Jianli
AU  - Li, Shuai
JO  - Journal of Building Engineering
VL  - 100
SP  - 111488
PY  - 2025
DA  - 2025/04/15/
SN  - 2352-7102
DO  - https://doi.org/10.1016/j.jobe.2024.111488
UR  - https://www.sciencedirect.com/science/article/pii/S2352710224030560
KW  - Building management
KW  - Health
KW  - Energy
KW  - Thermal comfort
KW  - Conversational persuading system
AB  - Effective management of indoor environments requires a comprehensive evaluation of health, energy consumption, and thermal comfort. However, real-time assessment of these factors is challenging due to the lack of integrated applications that combine IoT technology, real-time simulation, and user-friendly interfaces for communication. To address these challenges, this research introduces a novel platform specifically designed to manage health, energy consumption, and thermal comfort in smart buildings, leveraging IoT-based building information modeling (BIM), cloud computing, and an AI-powered conversational suggestion system based on the large language model (GPT). The platform integrates real-time monitoring, simulation, alerting, and persuasion capabilities to manage health, energy consumption, and thermal comfort, enabling responsive building environment controls by assessing tradeoffs among these dimensions and providing timely recommendations. Additionally, it employs persuasive techniques to encourage occupants to adopt environmentally-friendly practices. A case study in a university building demonstrated the platform's functionality and visualization capability. A survey assessing the persuasive system revealed high adoption rates—95.59 % for switching rooms to improve indoor air quality and health, and 79.90 % for adjusting clothing to enhance thermal comfort—indicating strong participant willingness to adopt sustainable practices through the platform's strategies. The key contribution of this research is the development of a comprehensive, real-time platform that enhances indoor environmental quality and sustainability through advanced monitoring, analysis, and social interaction.
ER  - 

TY  - JOUR
T1  - Context-aware decision making in autonomous vehicles: Integrating social behavior modeling with large language models
AU  - Lamichhane, Badri Raj
AU  - Aueawatthanaphisut, Aueaphum
AU  - Srijuntongsiri, Gun
AU  - Horanont, Teerayut
JO  - Array
VL  - 27
SP  - 100420
PY  - 2025
DA  - 2025/09/01/
SN  - 2590-0056
DO  - https://doi.org/10.1016/j.array.2025.100420
UR  - https://www.sciencedirect.com/science/article/pii/S2590005625000475
KW  - Autonomous vehicles
KW  - Social context
KW  - Scene understanding
KW  - Large language model
KW  - Reinforcement learning
AB  - Integrating context-aware decision-making in autonomous vehicles (AVs) is critical for advancing operational efficiency, safety, and user experience. However, existing frameworks struggle to incorporate social context into real-time navigation, relying on deterministic algorithms or reinforcement learning models that overlook implicit social norms and face challenges in translating LLM-derived reasoning into safety-compliant control policies. This paper investigates the application of social behavior modeling fused with large language models (LLMs) to establish a comprehensive framework for context-aware understanding and decision-making processes by AVs. Through understanding of the scene and the deployment of LLMs, this framework enables AVs to interpret and respond to complex social interactions and contextual cues, enhancing adaptability in dynamic environments. We propose concepts and approaches to foster context-aware and socially responsible decision-making processes, including test cases for validation to some level. The results demonstrate substantial improvements in decision accuracy adopting virtual simulation, providing a foundation for addressing complex ethical dilemmas and real-time decision-making challenges that AVs encounter in diverse and dynamic settings.
ER  - 

TY  - JOUR
T1  - MALM-CLIP: A generative multi-agent framework for multimodal fusion in few-shot industrial anomaly detection
AU  - Chen, Hanzhi
AU  - Que, Jingbin
AU  - Zhu, Kexin
AU  - Chen, Zhide
AU  - Zhu, Fei
AU  - Yang, Wencheng
AU  - Yang, Xu
AU  - Yang, Xuechao
JO  - Information Fusion
VL  - 127
SP  - 103765
PY  - 2026
DA  - 2026/03/01/
SN  - 1566-2535
DO  - https://doi.org/10.1016/j.inffus.2025.103765
UR  - https://www.sciencedirect.com/science/article/pii/S1566253525008279
KW  - CLIP
KW  - Few-shot learning
KW  - Industrial anomaly detection
KW  - Multi-agent systems
KW  - GenAI
AB  - The Contrastive Language-Image Pre-training (CLIP) model has significantly improved few-shot industrial anomaly detection. However, existing approaches often rely on manually crafted visual description texts, which lack robustness and generalizability in real-world production settings. This limitation is evident as these methods struggle to adapt to new or evolving anomalies, where original prompts fail to generalize beyond their initial design. This paper proposes a novel method, Multi-agent Language Models with CLIP (MALM-CLIP), which integrates the generative capabilities of large language models (LLMs) with CLIP within a multi-agent framework. In this system, specialized agents handle different subtasks such as prompt generation and model evaluation, enabling automated and context-aware multimodal information fusion. By eliminating manual prompt engineering, MALM-CLIP enhances both the accuracy and efficiency of anomaly detection. Experimental results on standard datasets such as MVTec and VisA demonstrate that our approach outperforms existing methods in detecting image-level anomalies with minimal training data. This work highlights the potential of combining Generative Artificial Intelligence (GenAI) and multi-agent systems for robust few-shot industrial anomaly detection.
ER  - 

TY  - JOUR
T1  - A comprehensive survey on integrating large language models with knowledge-based methods
AU  - Yang, Wenli
AU  - Some, Lilian
AU  - Bain, Michael
AU  - Kang, Byeong
JO  - Knowledge-Based Systems
VL  - 318
SP  - 113503
PY  - 2025
DA  - 2025/06/07/
SN  - 0950-7051
DO  - https://doi.org/10.1016/j.knosys.2025.113503
UR  - https://www.sciencedirect.com/science/article/pii/S0950705125005490
KW  - LLMs
KW  - Knowledge-based
KW  - Knowledge integration
KW  - RAG
KW  - KG
AB  - The rapid development of artificial intelligence has led to marked progress in the field. One interesting direction for research is whether Large Language Models (LLMs) can be integrated with structured knowledge-based systems. This approach aims to combine the generative language understanding of LLMs and the precise knowledge representation systems by which they are integrated. This article surveys the relationship between LLMs and knowledge bases, looks at how they can be applied in practice, and discusses related technical, operational, and ethical challenges. Utilizing a comprehensive examination of the literature, the study both identifies important issues and assesses existing solutions. It demonstrates the merits of incorporating generative AI into structured knowledge-base systems concerning data contextualization, model accuracy, and utilization of knowledge resources. The findings give a full list of the current situation of research, point out the main gaps, and propose helpful paths to take. These insights contribute to advancing AI technologies and support their practical deployment across various sectors.
ER  - 

TY  - JOUR
T1  - A survey of cyber threat attribution: Challenges, techniques, and future directions
AU  - Prasad, Nilantha
AU  - Diro, Abebe
AU  - Warren, Matthew
AU  - Fernando, Mahesh
JO  - Computers & Security
VL  - 157
SP  - 104606
PY  - 2025
DA  - 2025/10/01/
SN  - 0167-4048
DO  - https://doi.org/10.1016/j.cose.2025.104606
UR  - https://www.sciencedirect.com/science/article/pii/S0167404825002950
KW  - Cyber threats
KW  - Cyber security
KW  - Threat attribution
KW  - Machine learning
KW  - Artificial intelligence
AB  - The escalating sophistication of cyberattacks, exemplified by supply chain compromises, AI-driven obfuscation, and politically motivated campaigns, makes accurate attribution a critical yet elusive challenge for national security and economic stability. The inability to reliably trace attacks to their source undermines deterrence, distorts policy responses, and erodes trust in digital ecosystems. Traditional methods struggle with the sheer volume of digital evidence, rapidly evolving adversary tactics, and the inherent complexities of cross-border operations. Moreover, existing literature often provides fragmented analyses, focuses narrowly on cyber threat intelligence sharing or specific threat types, or predates significant advancements in AI/ML tailored for attribution. This survey offers a comprehensive, interdisciplinary review of cyber threat attribution, bridging these critical gaps by systematically analyzing its multifaceted dimensions: technical, legal, geopolitical, social, and economic. Employing a rigorous, PRISMA-ScR compliant methodology that included structured screening and quality assessment across six major databases, we critically appraise current techniques and identify a paradigm shift toward data-driven, intelligent approaches. A key contribution is our novel taxonomy, which structures attribution research by attribution confidence & granularity (the Level of attribution), analytical domains (the “How” and “Where” of evidence processing) and adversarial motivation & profile (the “Why” and “Who”), providing a crucial framework for systematic cross-study comparisons in a complex field. Our findings underscore the transformative potential of emerging AI/ML techniques, particularly graph neural networks, in automating analysis, identifying subtle patterns, and extracting crucial insights from vast datasets, thereby revolutionizing attribution accuracy. This research provides actionable insights for practitioners and policymakers, offering a comprehensive roadmap to advance cyber defense and foster a more resilient and secure global digital ecosystem.
ER  - 

TY  - JOUR
T1  - Safeguarding user data privacy in online Large Language Model services
AU  - Bai, Tianyu
AU  - Feng, Yunhe
AU  - Fu, Song
JO  - Journal of Systems Architecture
VL  - 168
SP  - 103555
PY  - 2025
DA  - 2025/11/01/
SN  - 1383-7621
DO  - https://doi.org/10.1016/j.sysarc.2025.103555
UR  - https://www.sciencedirect.com/science/article/pii/S1383762125002279
KW  - Large Language Models
KW  - Data privacy
KW  - Online services
KW  - Enhanced transformers
AB  - Large Language Models (LLMs), such as GPT, have become central to modern AI applications, including conversational agents, language translation, and document processing. Due to their computational demands, these models are typically hosted on remote servers, requiring users to transmit potentially sensitive data for inference. This raises serious privacy concerns, as user inputs may contain personally identifiable information (PII) and are vulnerable to misuse or unauthorized retention. To address this challenge, we present PPGPT, a novel and practical privacy-preserving GPT framework. PPGPT employs additive secret sharing to protect user input by enabling secure inference on secret shares rather than raw data. We design secure versions of key transformer components, including GELU and Softmax layers using Beaver’s triples and Taylor series, and introduce an optimized secure layer normalization protocol to reduce overhead. Experimental results show that PPGPT achieves comparable generation quality to the base model, with a negligible logits difference of 10−5 and an average inference time of 1.98 s. The framework is lightweight, generalizable to transformer-based LLMs, and suitable for deployment in real-world online services requiring strong privacy guarantees.
ER  - 

TY  - JOUR
T1  - Enhancing maritime cyber situational awareness: A cybersecurity visualisation for non-experts
AU  - Too, Dominic
AU  - Axon, Louise
AU  - Agrafiotis, Ioannis
AU  - Goldsmith, Michael
AU  - Creese, Sadie
JO  - Computers & Security
VL  - 154
SP  - 104433
PY  - 2025
DA  - 2025/07/01/
SN  - 0167-4048
DO  - https://doi.org/10.1016/j.cose.2025.104433
UR  - https://www.sciencedirect.com/science/article/pii/S0167404825001221
KW  - Visualisation
KW  - Maritime
KW  - Intrusion detection
KW  - Cyber–physical systems
KW  - Situational awareness
AB  - Cyber situational awareness is key to mitigating the impacts of cyber threats. However, maritime falls short of its comparative industries, with very little attention given to cyber threats despite the growing concern. In this paper, we explore the use of visualisations as a way to improve the situational awareness of non-experts onboard ships. We designed a visualisation tool with focus on systems that are accessible once onboard. In order to elicit requirements for our visualisations, we conducted semi-structured interviews with experts. We further created a synthetic dataset of attacks that target the systems of ships, which we used to assess the usability of our visualisation. In order to evaluate our visualisations, we conducted a user study with both expert and non-expert users. Our results show that non-expert participants were able to accurately and efficiently detect synthetic attacks targeting ships in an experimental setting, and they were able to use the visualisation to consider what the consequences of these attacks might be. Expert evaluations further suggest the visualisation has merit as a training tool for raising awareness among maritime employees.
ER  - 

TY  - JOUR
T1  - KnowCTI: Knowledge-based cyber threat intelligence entity and relation extraction
AU  - Wang, Gaosheng
AU  - Liu, Peipei
AU  - Huang, Jintao
AU  - Bin, Haoyu
AU  - Wang, Xi
AU  - Zhu, Hongsong
JO  - Computers & Security
VL  - 141
SP  - 103824
PY  - 2024
DA  - 2024/06/01/
SN  - 0167-4048
DO  - https://doi.org/10.1016/j.cose.2024.103824
UR  - https://www.sciencedirect.com/science/article/pii/S0167404824001251
KW  - Cyber threat intelligence
KW  - Entity and relation extraction
KW  - Knowledge graph
KW  - Attention mechanism
KW  - Graph attention network
AB  - Structured cyber threat intelligence enables security researchers to know the occurrence of cyber threats in time, thereby improving the efficiency of security defense and analysis. Previous works usually use general deep learning and NLP techniques to extract intelligence. Such methods suffer from insufficient semantic understanding in the field of security. To address these issues, we propose a novel method called Knowledge-based Cyber Threat Intelligence Entity and Relation Extraction (KnowCTI), which incorporates cybersecurity knowledge into the model to enhance the understanding of the realm of cybersecurity and has a full picture of threats with the threat intelligence graph generation. Specifically, we first build a cybersecurity knowledge base and train cybersecurity-aware knowledge embeddings based on the base. Secondly, we refine the most related knowledge triples by attention mechanism and gate mechanism, and then construct a sentence tree through these triples. Next, we employ graph attention networks to incorporate knowledge information into the sentence by considering the sentence tree as a graph. Finally, we consider entity extraction as a sequence labeling problem and relation extraction as a classification problem to decode the entities and relation triples according to the threat intelligence ontology we designed. Experimental results demonstrate the superior performance with the F1 score exceeding 90.16 and 81.83 on entity and relation extraction separately.
ER  - 

TY  - JOUR
T1  - The influence of persuasive techniques on large language models: A scenario-based study
AU  - Singh, Sonali Uttam
AU  - Namin, Akbar Siami
JO  - Computers in Human Behavior: Artificial Humans
VL  - 6
SP  - 100197
PY  - 2025
DA  - 2025/12/01/
SN  - 2949-8821
DO  - https://doi.org/10.1016/j.chbah.2025.100197
UR  - https://www.sciencedirect.com/science/article/pii/S2949882125000817
KW  - Large language models (LLM)
KW  - CHATGPT-4
KW  - Persuasion
KW  - Cialdini’s persuasion principles
AB  - Large Language Models (LLMs), such as CHATGPT-4, have introduced comprehensive capabilities in generating human-like text. However, they also raise significant ethical concerns due to their potential to produce misleading or manipulative content. This paper investigates the intersection of LLM functionalities and Cialdini’s six principles of persuasion: reciprocity, commitment and consistency, social proof, authority, liking, and scarcity. We explore how these principles can be exploited to deceive LLMs, particularly in scenarios designed to manipulate these models into providing misleading or harmful outputs. Through a scenario-based approach, over 30 prompts were crafted to test the susceptibility of LLMs to various persuasion principles. The study analyzes the success or failure of these prompts using interaction analysis, identifying different stages of deception ranging from spontaneous deception to more advanced, socially complex forms. Results indicate that LLMs are highly susceptible to manipulation, with 15 scenarios achieving advanced, socially aware deceptions (Stage 3), particularly through principles like liking and scarcity. Early stage manipulations (Stage 1) were also common, driven by reciprocity and authority, while intermediate efforts (Stage 2) highlighted in-stage tactics such as social proof. These findings underscore the urgent need for robust mitigation strategies, including resistance mechanisms at lower stages and training LLMs with counter persuasive strategies to prevent their exploitation. More than technical details, it raises important concerns about how AI might be used to mislead people. From online scams to the spread of misinformation, persuasive content generated by LLMs has the potential to impact both individual safety and public trust. These tools can shape how people think, what they believe, and even how they act often without users realizing it. With this work, we hope to open up a broader conversation across disciplines about these risks and encourage the development of practical, ethical safeguards that ensure language models remain helpful, transparent, and trustworthy. This research contributes to the broader discourse on AI ethics, highlighting the vulnerabilities of LLMs and advocating for stronger responsibility measures to prevent their misuse in producing deceptive content. The results describe the importance of developing secure, transparent AI technologies that maintain integrity in human–machine interactions.
ER  - 

TY  - JOUR
T1  - Simulation of prompt injection attacks on generative pre-trained transformers models
AU  - Kurniawan, Aditya
AU  - Chandra, Michael Bryan
JO  - Procedia Computer Science
VL  - 269
SP  - 400
EP  - 410
PY  - 2025
DA  - 2025/01/01/
T2  - The 10th International Conference on Computer Science and Computational Intelligence 2025
SN  - 1877-0509
DO  - https://doi.org/10.1016/j.procs.2025.08.292
UR  - https://www.sciencedirect.com/science/article/pii/S1877050925026377
KW  - Large language models (LLMs)
KW  - model vulnerabilities
KW  - prompt injection attacks
KW  - data set privacy
KW  - AI security
KW  - cyber security
AB  - Large language models (LLMs), such as GPT-2, GPT-3.5, and GPT-4o Mini, are widely used in various applications, yet their growing adoption has also revealed critical security vulnerabilities. Among the most pressing threats are prompt injection attacks, sensitive information disclosure, and unbounded consumption, which can lead to model manipulation, data leakage, and denial of service (DoS) conditions. Although prior research has examined these risks individually, this study uniquely conducts a com- prehensive side-by-side evaluation of all three adversarial vectors in multiple LLM architectures. Through an automated attack simulation framework, we systematically assess the weaknesses of different models, uncovering notable security gaps. Our results show that newer, more advanced models, such as GPT-4o Mini, are paradoxically more susceptible to adversarial manipulation, achieving a 100% success rate in prompt injection attacks on GPT-4o Mini, compared to up to 90% on GPT-3.5 and 0% on GPT-2. We also analyze how encoding techniques such as Base64, Hex, and Rot-13 affect attack success rates. In sensitive information disclosure tests, GPT-4o Mini achieved 78.57% accuracy with an average similarity score of 0.8401, while GPT-2 still leaked data with 75% accuracy. Furthermore, this research quantifies how unbounded consumption attacks significantly degrade system per- formance, posing risks to real-world AI deployments. Under simulated unbounded consumption attacks, the response time of the GPT-2 increased from 94.68 seconds at low concurrency to more than 35,000 seconds at 10,000 concurrent requests, indicating extreme vulnerability to DoS exploitation. By bridging the gap between theoretical vulnerabilities and practical adversarial sim- ulations, this study provides actionable insights into AI security. Our findings emphasize the urgent need for robust adversa rial defenses, real-time anomaly detection, and privacy-preserving mechanisms to ensure the safe and ethical deployment of LLMs in high-risk environments. These experimental findings challenge the assumption that larger models are more secure and emphasize the importance of embedding robust safeguards in LLM deployments.
ER  - 

TY  - JOUR
T1  - Generative AI and Large Language Models - Benefits, Drawbacks, Future and Recommendations
AU  - Håkansson, Anne
AU  - Phillips-Wren, Gloria
JO  - Procedia Computer Science
VL  - 246
SP  - 5458
EP  - 5468
PY  - 2024
DA  - 2024/01/01/
T2  - 28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)
SN  - 1877-0509
DO  - https://doi.org/10.1016/j.procs.2024.09.689
UR  - https://www.sciencedirect.com/science/article/pii/S1877050924027492
KW  - Natural Language Processing
KW  - Generative AI
KW  - Large Language Models
AB  - Natural language processing, with parsing and generation, has a long tradition. Parsing has been easier to perform than a generation but with generative artificial intelligence (a.k.a Gen AI) and large language models (abbr. LLMs), this has changed. Generative artificial intelligence is a type of artificial intelligence that uses a large data set to create something in the genre of that data set. It can generate different outputs ranging from texts, audio, objects, pictures, and paintings to videos, but also synthetic data. LLMs use deep learning and deep neural networks to train on large text corpora for recognizing and generating texts. These models are based on massive data sets, collected from databases and the web. They use transformer models to detect how elements in sequences relate to each other. This provides context support. Two well-known large language models are the Generative Pre-trained Transformer, GPT, used in ChatGPT and Bidirectional Encoder Representations from Transformers, BERT. Although LLMs have advantages, they have problems. This paper presents generative artificial intelligence and LLMs with benefits and drawbacks. Results from applying these models have shown that they can work well for accuracy in specificity, user personalization and human-computer communication but they may not provide acceptable, reliable and truthful results. For example, ethics, hallucinations and incorrect information, or misjudgments, are some major problems. The paper ends with future directions, research questions on LLMs, and recommendations.
ER  - 

TY  - JOUR
T1  - Integrative innovation of large language models in industries: technologies, applications, and challenges
AU  - Wang, Shikai
AU  - Shao, Yiwen
JO  - Data Science and Management
PY  - 2025
DA  - 2025/07/02/
SN  - 2666-7649
DO  - https://doi.org/10.1016/j.dsm.2025.06.005
UR  - https://www.sciencedirect.com/science/article/pii/S2666764925000323
KW  - Large Language Models (LLMs)
KW  - Natural Language Processing (NLP)
KW  - Human-Centric AI
KW  - Artificial General Intelligence (AGI)
AB  - This paper examines the transformative potential of large language models (LLMs) across diverse industries, emphasizing their ability to enhance natural language processing tasks through pre-training on extensive datasets. Although LLMs offer significant opportunities to automate customer interactions, improve decision-making, and optimize workflows, their rapid adoption also presents challenges such as information security, data quality, model interpretability, ethical implications, and regulatory compliance. To address these issues, this review proposes integrative strategies, including scenario-based applications, methodological innovation, and data-model integration, to boost LLMs’ performance and adaptability. It also explores the evolution of LLMs toward multimodal and multitask general-purpose models, highlighting future trends in sustainable development and human-centric AI. Future research directions include achieving a balance between enhancing model capabilities and managing energy consumption, as well as improving transparency and explainability to strengthen user trust.
ER  - 

TY  - JOUR
T1  - RETO: Reinforcement learning enhanced terminology optimization for cyber threat intelligence summarization
AU  - Ding, Junmei
AU  - Peng, Wei
AU  - Lu, Yueming
AU  - Wang, Zhiqiang
AU  - Feng, Yan
JO  - Neurocomputing
VL  - 654
SP  - 131339
PY  - 2025
DA  - 2025/11/14/
SN  - 0925-2312
DO  - https://doi.org/10.1016/j.neucom.2025.131339
UR  - https://www.sciencedirect.com/science/article/pii/S0925231225020119
KW  - Intelligence summarization
KW  - Reinforcement learning
KW  - LLMs
AB  - Cyber threat intelligence summarization consolidates raw intelligence into concise summaries for decision-makers, playing a crucial role in rapid, accurate detection and response to cyber threats. However, existing research primarily focuses on improving the fluency, overlap, and accuracy of summaries, often neglecting the optimization of the correctness and coverage of specialized terminology. While recent advances in LLMs technology show promise, they frequently produce hallucinations, especially in specialized areas, undermining the reliability and accuracy of summaries. In this paper, a Reinforcement Learning Enhanced Terminology Optimization (RETO) approach is proposed for cyber threat intelligence summarization. RETO’s three modules include a two-stage terminology annotator, a threat intelligence-specific terminology generator, and a multi-reward-aware terminology optimizer. The first module provides high-quality annotated data, the second module trains an automatic terminology generator, and the final module optimizes the terminology generator, guiding LLMs to generate more accurate summaries within the reinforcement learning framework. Additionally, a multi-reward-aware function is designed to improve the terminology generation quality by maximizing a combination of ROUGE-L, Terminology Correctness, and Homologous Terminology Coverage. Experimental results on the CTISum dataset show that RETO achieves state-of-the-art performance in zero-shot summarization tasks, and the introduction of domain-specific terminology effectively reduces hallucinations generated by LLMs, leading to more accurate and reliable summaries. We also make our code publicly available at https://github.com/JmeiDing/RETO.
ER  - 

TY  - JOUR
T1  - Automated robustness testing for LLM-based natural language processing software
AU  - Xiao, Mingxuan
AU  - Xiao, Yan
AU  - Ji, Shunhui
AU  - Cai, Hanbo
AU  - Xue, Lei
AU  - Zhang, Pengcheng
JO  - Expert Systems with Applications
VL  - 303
SP  - 130642
PY  - 2026
DA  - 2026/03/25/
SN  - 0957-4174
DO  - https://doi.org/10.1016/j.eswa.2025.130642
UR  - https://www.sciencedirect.com/science/article/pii/S0957417425042575
KW  - Software testing
KW  - Natural language processing
KW  - Intelligent systems
KW  - Beam search
AB  - With the rapid advancements of Large Language Models (LLMs), Natural Language Processing (NLP) software has evolved quickly and is now widely used in safety-critical tasks such as financial sentiment analysis, toxic content moderation, and log generation. Unlike traditional software, LLM-based NLP software relies on prompts and examples as inputs. Due to the complexity of LLMs and the unpredictability of real-world inputs, quantitatively assessing their robustness is essential. However, to our knowledge, no automated robustness testing methods have been specifically designed to evaluate the overall inputs, including prompts and examples, of LLM-based NLP software. To this end, we propose an AutOmated Robustness Testing frAmework, AORTA, which formalizes the testing process as a combinatorial optimization problem. While AORTA can incorporate traditional testing methods for LLM-based software testing, their effectiveness remains limited. To overcome this, we introduce a novel method within AORTA–Adaptive Beam Search (ABS)–designed for the vast feature space of LLMs. ABS enhances testing by using an adaptive beam width and supporting backtracking. We integrated 18 testing methods into AORTA and evaluated ABS with three datasets and five models under test. ABS enables more comprehensive and accurate robustness assessments prior to deployment, achieving an average success rate of 86.14 %. Compared to the baseline with the highest effectiveness in our experiments, PWWS, ABS reduces computational overhead by up to 3441.90 s per successful test and cuts query numbers by an average of 218.76. Additionally, test cases generated by ABS exhibit greater naturalness and transferability.
ER  - 

TY  - JOUR
T1  - Enhancing user information disclosure intention in dynamic conversations of intelligent recommendation systems based on large language models: A perspective of user gratification and privacy calculus
AU  - Xu, Chunze
AU  - Gao, Fengqiang
AU  - Han, Lei
JO  - International Journal of Human-Computer Studies
VL  - 200
SP  - 103511
PY  - 2025
DA  - 2025/05/01/
SN  - 1071-5819
DO  - https://doi.org/10.1016/j.ijhcs.2025.103511
UR  - https://www.sciencedirect.com/science/article/pii/S1071581925000680
KW  - Human-computer interaction
KW  - Large language model
KW  - Intelligent recommender systems
KW  - User satisfaction
KW  - Information disclosure
AB  - Intelligent recommender systems, as powerful tools for users to provide personalised recommended content, have penetrated into all aspects of life, influencing users' behaviour and experience. However, research on the impact of the interaction process between users and intelligent recommender systems on user use and disclosure is still limited. In order to fill this research gap, this study is based on the Use & Gratification Theory and the Privacy Disclosure Theory, conducting a research model under the Use-Gratification-Disclosure framework, exploring the use and satisfaction process of users when using intelligent recommender systems as well as the mechanism of the influence on the information disclosure intention, and analysing the variations between the elders and youths. The results indicate that diverse types of use processes lead to different sorts (utility, hedonic, and social) as well as various degrees of satisfaction for users. The relative satisfaction obtained by users had a significant positive effect on intention to disclose information. Moreover, this effect differed significantly between different age groups. The above results provide novel and valuable insights into understanding users' experiential needs, use satisfaction, and disclosure behaviour in the field of human-computer interaction.
ER  - 

TY  - JOUR
T1  - A large language model-based building operation and maintenance information query
AU  - Li, Yan
AU  - Ji, Minxuan
AU  - Chen, Junyu
AU  - Wei, Xin
AU  - Gu, Xiaojun
AU  - Tang, Juemin
JO  - Energy and Buildings
VL  - 334
SP  - 115515
PY  - 2025
DA  - 2025/05/01/
SN  - 0378-7788
DO  - https://doi.org/10.1016/j.enbuild.2025.115515
UR  - https://www.sciencedirect.com/science/article/pii/S0378778825002452
KW  - Building operation and maintenance
KW  - Information query
KW  - Large language model
KW  - Knowledge graph
KW  - Retrieval-augmented generation
AB  - In smart buildings, operation and maintenance (O&M) are critical for achieving energy optimization and ensuring comfort. However, as smart buildings incorporate more systems and generate increasingly complex data, efficiently querying this information has become a major challenge for O&M tasks. Traditional methods require complex queries, considering data standards and building structure information, which is demanding, while current artificial intelligence (AI) querying technologies lack accuracy and interactivity.To addressthis, this paper proposes a large language model (LLM)-basedinformation query paradigm for building O&M. Under this paradigm, the first step is data processing, which adopts the isolation forest and the prompt-guided data structure reconstruction methods. The second step is O&M LLM fine-tuning, which fine-tunes the Qwen-32B LLM by a constructed O&M hybrid dataset. The third step is knowledge enhancement for the fine-tuned LLM by constructing an external O&M knowledge base and connecting it to the model using the retrieval-augmented generation (RAG) technique. Additionally, a search algorithm is proposed to integrate the O&M knowledge graph (KG), enabling the retrieval of pertinent O&M information. The fourth step is alignment fine-tuning for the knowledge enhanced-LLM which utilizes human feedback to further promote its interactive effectiveness.Experiments show that the proposed paradigmachieved execution accuracy rates of 95.5% in the O&M information query evaluationand itoutperforms state-of-the-art (SOTA)LLMs including GPT-4, Llama-70B, CodeLlama-70B. Feedback from practical use and surveys by twenty O&M engineers and senior management confirm the interactivity and practicalityof this paradigm in real-world smart building scenarios.
ER  - 

TY  - JOUR
T1  - Sentiment analysis of the United States public support of nuclear power on social media using large language models
AU  - Kwon, O. Hwang
AU  - Vu, Katie
AU  - Bhargava, Naman
AU  - Radaideh, Mohammed I.
AU  - Cooper, Jacob
AU  - Joynt, Veda
AU  - Radaideh, Majdi I.
JO  - Renewable and Sustainable Energy Reviews
VL  - 200
SP  - 114570
PY  - 2024
DA  - 2024/08/01/
SN  - 1364-0321
DO  - https://doi.org/10.1016/j.rser.2024.114570
UR  - https://www.sciencedirect.com/science/article/pii/S136403212400296X
KW  - Sentiment analysis
KW  - Natural language processing
KW  - Nuclear power
KW  - Public policy
KW  - Social media
KW  - Large language models
AB  - This study utilized large language models (LLMs) to analyze public sentiment in the United States (US) regarding nuclear power on social media, focusing on X/Twitter, considering climate change challenges and advancements in nuclear power technology. Approximately, 1.26 million nuclear tweets from 2008–2023 were examined to fine-tune LLMs for sentiment classification. We found the crucial role of accurate data labeling for model performance, with potential implications for a 15% improvement, achieved through high-confidence labels. LLMs demonstrated better performance compared to traditional machine learning classifiers, with reduced susceptibility to overfitting and up to 96% classification accuracy. LLMs are used to segment the US public tweets into policy and energy-related categories, revealing that 68% are politically themed. Policy tweets tended to convey negative sentiment, often reflecting opposing political perspectives and focusing on nuclear deals and international relations. Energy-related tweets covered diverse topics with predominantly neutral to positive sentiment, indicating broad support for nuclear power in 48 out of 50 US states. The US public positive sentiments toward nuclear power stemmed from its high power density, reliability regardless of weather conditions, environmental benefits, application versatility, and recent innovations and advancements in both fission and fusion technologies. Negative sentiments primarily focused on waste management, high capital costs, and safety concerns. The neutral campaign highlighted global nuclear facts and advancements, with varying tones leaning towards positivity or negativity. An interesting neutral theme was the advocacy for the combined use of renewable and nuclear energy to attain net-zero goals.
ER  - 

TY  - JOUR
T1  - A knowledge graphs construction method enhanced by multimodal large language model for industrial equipment operation and maintenance
AU  - Zhang, Zhengping
AU  - Yu, Junyuan
AU  - Yang, Bo
AU  - Du, Kaze
AU  - Wang, Shilong
AU  - Qi, Xing
JO  - Advanced Engineering Informatics
VL  - 68
SP  - 103705
PY  - 2025
DA  - 2025/11/01/
SN  - 1474-0346
DO  - https://doi.org/10.1016/j.aei.2025.103705
UR  - https://www.sciencedirect.com/science/article/pii/S1474034625005981
KW  - Equipment operation and maintenance
KW  - Multi-modal knowledge graph
KW  - Multimodal large language modals
KW  - Attention mechanism
AB  - The industrial equipment Operation and Maintenance (O&M) is a core component in ensuring production safety and efficiency, urgently requiring the support of intelligent technologies. Knowledge graphs, which represent equipment and faults in graph structures, are widely utilized to enable efficient association and rapid retrieval of maintenance knowledge, thereby being extensively applied in intelligent decision-making for the equipment O&M. Traditional knowledge graph construction methods, which rely on a single textual modality, are confronted with challenges such as the scarcity of annotated samples, difficulties in dynamically associating old and new equipment, and insufficient parsing of complex equipment relationships. As a result, issues like missing graph entities and broken causal chains are often encountered, thereby negatively impacting the quality of maintenance decision-making. Therefore, a dual-attention model enhanced by multimodal large language models (MllmDA-KGC) is proposed in this paper. Multimodal large language model(MLLM) is introduced to fully utilize multi-modal knowledge from the O&M domain, thereby enabling a more effective understanding and modeling of complex O&M knowledge. As a result, the quality of knowledge graph construction is significantly improved. In the MllmDA-KGC framework, first, QWEN2-VL is introduced into a dual-stream Transformer architecture to achieve dynamic alignment between images and text while supplementing semantics. As a result, the precision of identifying relationships between parts and problem entities in the O&M domain is significantly enhanced; second, the MT-Transformer module is proposed, which integrates causal convolution, dilated convolution, and Memory_Bank mechanisms to achieve cross-modal temporal embedding fusion. As a result, the continuity of associations between new and old parts, as well as the precision of causal chain embeddings, is significantly improved; third, a multimodal dynamic weight attention-guided module is designed, in which weighted key-value guided attention mechanisms are introduced to focus on critical aspects. Schematic diagrams and textual features are fused to enhance the precision of entity relationship modeling between parts and faults; finally, to fully leverage the multimodal understanding capabilities of the MLLM, the image embeddings and positional embeddings generated and marked by MLLM are integrated into the feature embeddings of RoBERTa. Subsequently, CRF and Softmax are combined to accomplish the MNER (Multimodal Named Entity Recognition) and MRE (Multimodal Relation Extraction) tasks, thereby enabling the construction of a multimodal equipment O&M knowledge graph. In this paper, the vehicle O&M dataset from an automobile company was utilized to validate the proposed method. The experimental results showed that the F1 scores of the model in the MNER and MRE tasks reached 88.40% and 93.79%, respectively, demonstrating its effectiveness in constructing a multimodal knowledge graph for equipment O&M. Furthermore, during the process of graph inference, the performance of multimodal graph inference was significantly better than that of the unimodal approach, further confirming the superiority of the multimodal knowledge graph.
ER  - 

TY  - JOUR
T1  - LLM+m: Dual-model chatGPT-based product training and testing with adversarial attack and defense
AU  - Hwang, Ren-Hung
AU  - Hsiao, Yu-Hung
AU  - Lin, Ying-Dar
AU  - Lai, Yuan-Cheng
JO  - Future Generation Computer Systems
VL  - 179
SP  - 108328
PY  - 2026
DA  - 2026/06/01/
SN  - 0167-739X
DO  - https://doi.org/10.1016/j.future.2025.108328
UR  - https://www.sciencedirect.com/science/article/pii/S0167739X25006223
KW  - Large language models (LLM)
KW  - Prompt engineering
KW  - Adversarial attacks
KW  - Adversarial training
KW  - Ensemble models
KW  - Intrusion detection systems
KW  - Image recognition systems
AB  - Large language models (LLMs) like ChatGPT are increasingly leveraged in diverse applications. This study explores the integration of ChatGPT with machine learning models (LLM + m) to develop dual-model systems for intrusion detection–covering host-based (HIDS) and network-based (NIDS) systems–and image classification. By employing ChatGPT for data preprocessing and model generation, we evaluated the systems’ robustness against a range of adversarial attacks, including hypnotic attacks targeting the LLM, traditional adversarial attacks on ML models, and combined attacks affecting both. To counter these threats, we implemented adversarial training, ensemble models, and robustness prompts, significantly enhancing system resilience. Experimental results showed that combined attacks caused F1 score drops of up to 50 %, exposing critical vulnerabilities in dual-model systems. However, the proposed defenses reduced these losses to approximately 5 %, demonstrating their effectiveness in securing LLM-based dual-model systems against increasingly sophisticated adversarial threats.
ER  - 

TY  - JOUR
T1  - Serious games as a tool to model attack and defense scenarios for cyber-security exercises
AU  - Yamin, Muhammad Mudassar
AU  - Katt, Basel
AU  - Nowostawski, Mariusz
JO  - Computers & Security
VL  - 110
SP  - 102450
PY  - 2021
DA  - 2021/11/01/
SN  - 0167-4048
DO  - https://doi.org/10.1016/j.cose.2021.102450
UR  - https://www.sciencedirect.com/science/article/pii/S0167404821002741
KW  - Cyber range
KW  - Cyber-security
KW  - Exercises
KW  - Scenarios
KW  - Attack
KW  - Defense
AB  - Technology is evolving rapidly; this poses a problem for security specialists and average citizens as their technological skill sets are quickly made obsolete. This makes the knowledge and understanding of cyber-security in a technologically evolving world difficult. Global IT infrastructure and individuals’ privacy are constantly under threat. One way to tackle this problem is by providing continuous training and self-learning platforms. Cyber-security exercises can provide a necessary platform for training people’s cyber-security skills. However, conducting cyber-security exercises with new and unique scenarios requires comprehensive planning and commitment to the preparation time and resources. In this work, we propose a serious game for the development of cyber-security exercise scenarios. The game provides a platform to model simulated cyber-security exercise scenarios, transforming them into an emulated cyber-security exercise environment using domain-specific language (DSL) and infrastructure orchestration. In this game, players can play as cyber attackers or defenders in a multiplayer environment to make operational cyber-security decisions in real-time. The decisions are evaluated for the development of operational cyber-attack and defense strategies.
ER  - 

TY  - JOUR
T1  - Privacy issues in Large Language Models: A survey
AU  - Kibriya, Hareem
AU  - Khan, Wazir Zada
AU  - Siddiqa, Ayesha
AU  - Khan, Muhammad Khurram
JO  - Computers and Electrical Engineering
VL  - 120
SP  - 109698
PY  - 2024
DA  - 2024/12/01/
SN  - 0045-7906
DO  - https://doi.org/10.1016/j.compeleceng.2024.109698
UR  - https://www.sciencedirect.com/science/article/pii/S0045790624006256
AB  - In the fast-paced world of modern technology, the development of Large Language Models (LLMs) has increased drastically. However, this growth has also increased privacy concerns associated with these models. This paper investigates privacy concerns in the existing LLMs and their far-reaching implications. The paper categorizes privacy concerns of LLMs into two main groups: those occurring during training and those during inference, both of which can contribute to re-identification risks. Through an in-depth literature analysis, we have highlighted different requirements for safeguarding user privacy when interacting with LLMs. Moreover, this paper discusses the challenges that can arise in implementing privacy-preserving mechanisms in LLMs. It examines the complex interactions between ethical issues, legal requirements, and technology developments, highlighting the need for stakeholder collaboration to traverse this challenging environment successfully. This paper contributes to the ongoing discussion on the responsible development and deployment of LLMs. It aims to open the door for ethically acceptable Artificial Intelligence innovation processes by promoting a better awareness of privacy issues.
ER  - 

TY  - JOUR
T1  - A novel hybrid cybersecurity assessment methodology for HTTPS deployment
AU  - Zineddine, Abdelhadi
AU  - Belfaik, Yousra
AU  - Sadqi, Yassine
AU  - Safi, Said
JO  - High-Confidence Computing
SP  - 100344
PY  - 2025
DA  - 2025/08/20/
SN  - 2667-2952
DO  - https://doi.org/10.1016/j.hcc.2025.100344
UR  - https://www.sciencedirect.com/science/article/pii/S2667295225000480
KW  - HTTPS deployment
KW  - Cybersecurity assessment
KW  - GPT-4o
KW  - Human factors
KW  - Hybrid framework
KW  - HTTPS-secured websites
AB  - Implementing HTTPS is a complex process encompassing technical and human factors, with a significant reliance on the webmaster’s expertise. Although various evaluation methods have been proposed in the scientific literature to address HTTPS deployment challenges, including vulnerabilities related to X.509 certificate fields, cipher suites, mixed content, encryption libraries, and the behaviors of users and webmasters, there remains a lack of a comprehensive methodology that integrates these metrics into a unified framework. To address this gap, this paper introduces a novel hybrid assessment methodology that combines three main cybersecurity assessment techniques: examination, testing, and interviewing. The methodology is further enhanced by integrating K-Means clustering with Large Language Model (LLM) reasoning to interpret interview-based insights and uncover behavioral patterns. It evaluates 12 critical security measures and mechanisms essential for robust HTTPS deployment. The effectiveness of the proposed methodology is demonstrated through a case study analyzing the security posture of HTTPS-secured websites across five domains: e-commerce, e-finance, education, government, and e-newspapers. The obtained results prove the applicability of the methodology in real-world scenarios and offer actionable insights for practitioners and researchers. In addition, the generated dataset of findings provides a comprehensive overview of the analyzed HTTPS website’s security levels and establishes a valuable foundation for future research to improve HTTPS implementation.
ER  - 

TY  - JOUR
T1  - ICSThreatQA: A knowledge-graph enhanced question answering model for industrial control system threat intelligence
AU  - Rani, Ruby
AU  - Kumar, Mahender
AU  - Epiphaniou, Gregory
AU  - Maple, Carsten
JO  - Expert Systems with Applications
VL  - 301
SP  - 130180
PY  - 2026
DA  - 2026/03/10/
SN  - 0957-4174
DO  - https://doi.org/10.1016/j.eswa.2025.130180
UR  - https://www.sciencedirect.com/science/article/pii/S0957417425037959
KW  - Industrial control system
KW  - Question answer model
KW  - Retrieval augmented graph
KW  - Knowledge graph
AB  - Industrial Control Systems (ICS) underpin critical infrastructure but remain vulnerable to sophisticated cyberattacks. Existing threat intelligence tools emphasise static knowledge bases and isolated indicators, offering limited support for analysts who must navigate complex adversarial tactics. To address this gap, we present ICSThreatQA, the first QA framework tailored to ICS threat intelligence. ICSThreatQA introduces four retrieval-augmented architectures - including a novel Knowledge Graph-enhanced RAG (KG-RAG) - designed to provide accurate, context-aware responses to natural-language security queries. We construct a curated dataset of 620 expert-validated QA pairs from the MITRE ATT&CK ICS knowledge base, encompassing factual, contrastive, inferential, and opinion-based queries. Through extensive evaluation, including automated metrics, human expert ratings, adversarial robustness, and multi-turn dialogues, ICSThreatQA demonstrates significant improvements over baseline RAG and large language models. Notably, KG-RAG achieves the highest answer relevance (0.968) and correctness (0.660), while the Hybrid model balances precision and faithfulness under few-shot settings. These results confirm ICSThreatQA’s potential to transform static ICS threat knowledge into an interactive, analyst-ready intelligence system, reducing cognitive burden and accelerating incident response.
ER  - 

TY  - JOUR
T1  - Prompt Engineering for Virtual Asset Transaction Analysis using Large Language Models: A Pilot Study
AU  - Kuan, Chen-Yu
AU  - Tsai, Fu-Ching
JO  - Procedia Computer Science
VL  - 270
SP  - 5675
EP  - 5684
PY  - 2025
DA  - 2025/01/01/
T2  - 29th International Conference on Knowledge-Based and Intelligent Information & Engineering Systems (KES 2025)
SN  - 1877-0509
DO  - https://doi.org/10.1016/j.procs.2025.10.036
UR  - https://www.sciencedirect.com/science/article/pii/S1877050925033691
KW  - Large Language Models (LLMs)
KW  - Prompt Engineering
KW  - Virtual Assets
KW  - Financial Crime Investigation
KW  - Blockchain Analysis
AB  - Since their emergence, virtual assets have rapidly become a primary tool in criminal activities. Their advantages of speed and low cost, combined with the anonymity afforded by decentralization, necessitate that law enforcement agencies analyze virtual asset flows by cross-referencing vast numbers of transactions to identify suspicious behavioral patterns. The specialized skills and experience required for virtual asset flow analysis, coupled with the extensive time needed for case-by-case analysis, overwhelm law enforcement personnel in the face of a rapidly increasing number of cases. The rapid iterative development of large language models (LLMs) has shown impressive performance across diverse tasks in various fields. With their understanding of human semantics and needs, and through a reasonable logical thinking process, LLMs are well-equipped to serve as crucial assistants for law enforcement agencies in analyzing virtual asset flows. This study serves as a pilot investigation presenting the effectiveness of different prompting techniques on the analysis of temporal numerical data structures by large language models. It aims to stimulate further research in relevant academic fields and provide a basis for inference for law enforcement personnel during the initial stages of evidence investigation.
ER  - 

TY  - JOUR
T1  - GPT on the wire: Towards realistic network traffic conversations generated with large language models
AU  - Delgado-Soto, Javier Aday
AU  - López de Vergara, Jorge E.
AU  - González, Iván
AU  - Perdices, Daniel
AU  - de Pedro, Luis
JO  - Computer Networks
VL  - 265
SP  - 111308
PY  - 2025
DA  - 2025/06/01/
SN  - 1389-1286
DO  - https://doi.org/10.1016/j.comnet.2025.111308
UR  - https://www.sciencedirect.com/science/article/pii/S1389128625002762
KW  - Network traffic generation
KW  - Generative AI
KW  - GPT
KW  - Prompt engineering
KW  - Fine-tuning
KW  - Mixture of Experts (MoE)
AB  - Realistic network traffic generation is essential for evaluating the performance, security, and scalability of modern communication systems. Traditional methods, such as traffic replay systems and statistical models, while useful, often fall short in capturing the complexity and variability of real-world network scenarios. Recent advancements in Artificial Intelligence (AI), especially Large Language Models (LLMs) like ChatGPT, have introduced new approaches to synthetic traffic generation. This paper presents a novel architecture using OpenAI’s GPT-3.5 Turbo to generate synthetic network traffic, with a focus on creating multi-protocol conversations that are indistinguishable from real-world interactions. Through fine-tuning and prompt engineering, the proposed system successfully generates packet- and conversation-level network traffic for ICMP, ARP, DNS, TCP and HTTP protocols. Additionally, by integrating a Mixture of Experts (MoE) architecture, this model simulates real-world network conversations with high accuracy, being able to generate a conversation combining ARP, DNS, TCP and HTTP without packet or protocol errors. The results show how the application of LLMs in network traffic generation improves realism and adaptability, establishing this approach as a valuable tool for future security testing and network performance evaluation. In addition, the proposed methodology is easily adaptable to other LLMs available both through APIs and to be downloaded and executed on your own computer.
ER  - 

TY  - JOUR
T1  - Randomized CNN based deep learning technique for the cyber-attacks detection in SCADA industrial control systems
AU  - Mubarak, Sinil
AU  - Habaebi, Mohamed Hadi
AU  - Islam, Md Rafiqul
AU  - Jaleel, Nubila
AU  - Siddique, Mohammed Tahir
JO  - Measurement
VL  - 254
SP  - 117933
PY  - 2025
DA  - 2025/10/01/
SN  - 0263-2241
DO  - https://doi.org/10.1016/j.measurement.2025.117933
UR  - https://www.sciencedirect.com/science/article/pii/S0263224125012928
KW  - Cyber-attacks
KW  - CNN
KW  - Random CNN
KW  - RNN LSTM
KW  - SCADA
AB  - The increasing digitization and connectivity of Industrial Control Systems (ICS) have exposed them to highly sophisticated cyber threats to a great extent. Traditional security mechanisms like rule-based and signature-based intrusion detection systems fail to detect new and emerging attacks. In such limitations, the current research is on the Randomized Convolutional Neural Network (R-CNN) model for cyber-attack detection in ICS networks. The proposed model leverages convolutional layers to improve feature extraction and reduce the likelihood of overfitting. Additionally, advanced data preprocessing, augmentation, and hyperparameter optimization methods to maximize classification performance. With the increasing frequency of cybersecurity attacks in the Industrial Internet of Things (IIoT), addressing such challenges are hindered by outdated public datasets and scarcity of testbeds, to design appropriate solutions to detect and prevent cyberattacks. To overcome this limitation, we have developed an in-house cyber testbed based on standard industrial operations, simulated with hacking scenarios. The model is trained and tuned with ICS cyber-attack benchmark datasets on the various attacks. Randomized layers provide controlled variability at training time, enhancing the model to recognize known and unknown attacks. Experimental results indicate that the proposed R-CNN method performs better than the baseline machine learning and conventional CNN methods with a detection accuracy of 98.7%. The model also achieves 98.2% precision, 97.9% recall, and an F1-score of 98.0%, making the threat detection robust and the false positive rate imperceptible. The computational overhead of the architecture also enables real-time deployment within an industrial setup. The improved Randomized CNN-based IDS in deep learning-based cyber security applications facilitates real-time cyber threat detection feasibility within ICS. It enhances the model for zero-day attack detection and integration with adaptive security paradigms for predictive threat blocking.
ER  - 

TY  - JOUR
T1  - Enhancing vulnerability detection efficiency: An exploration of light-weight LLMs with hybrid code features
AU  - Liu, Jianing
AU  - Lin, Guanjun
AU  - Mei, Huan
AU  - Yang, Fan
AU  - Tai, Yonghang
JO  - Journal of Information Security and Applications
VL  - 88
SP  - 103925
PY  - 2025
DA  - 2025/02/01/
SN  - 2214-2126
DO  - https://doi.org/10.1016/j.jisa.2024.103925
UR  - https://www.sciencedirect.com/science/article/pii/S2214212624002278
KW  - Vulnerability discovery
KW  - Deep learning
KW  - Source code
AB  - Vulnerability detection is a critical research topic. However, the performance of existing neural network-based approaches requires further improvement. The emergence of large language models (LLMs) has demonstrated their superior performance in natural language processing (NLP) compared to conventional neural architectures, motivating researchers to apply LLMs for vulnerability detection. This paper focuses on evaluating the performance of various Transformer-based LLMs for source-code-level vulnerability detection. We propose a framework named VulACLLM (AST & CFG-based LLMs Vulnerability Detection), which leverages combined feature sets derived from abstract Syntax Tree (AST) and Control Flow Graph (CFG). The recall rate of VulACLLM in the field of vulnerability detection reached 0.73, while the F1-score achieved 0.725. Experimental results show that the proposed feature sets significantly enhance detection performance. To further improve the efficiency of LLM-based detection, we examine the performance of LLMs compressed using two techniques: Knowledge Distillation (KD) and Low-Rank Adaptation (LoRA). To assess the performance of these compressed models, we introduce efficiency metrics that quantify both performance loss and efficiency gains achieved through compression. Our findings reveal that, compared to KD, LLMs compressed with LoRA achieve higher recall, achieving a maximum recall rate of 0.82, while substantially reducing training time, taking only 20 min to complete one epoch, and disk size, requiring only 4.89 MB of memory. The experimental results demonstrate that LoRA compression effectively mitigates deployment challenges associated with large model sizes and high video memory consumption, enabling the deployment of LoRA-compressed LLMs on consumer-level GPUs without compromising vulnerability detection performance.
ER  - 

TY  - JOUR
T1  - Enhanced automated code vulnerability repair using large language models
AU  - de-Fitero-Dominguez, David
AU  - Garcia-Lopez, Eva
AU  - Garcia-Cabot, Antonio
AU  - Martinez-Herraiz, Jose-Javier
JO  - Engineering Applications of Artificial Intelligence
VL  - 138
SP  - 109291
PY  - 2024
DA  - 2024/12/01/
SN  - 0952-1976
DO  - https://doi.org/10.1016/j.engappai.2024.109291
UR  - https://www.sciencedirect.com/science/article/pii/S0952197624014490
KW  - Automated code repair
KW  - Deep learning
KW  - Large language models
KW  - Vulnerability repair
KW  - Mistral
KW  - Code llama
AB  - This research addresses the complex challenge of automated repair of code vulnerabilities, vital for enhancing digital security in an increasingly technology-driven world. The study introduces a novel and efficient format for the representation of code modification, using advanced Large Language Models (LLMs) such as Code Llama and Mistral. These models, fine-tuned on datasets featuring C/C++ code vulnerabilities, significantly improve the accuracy and adaptability of automated code repair techniques. A key finding is the enhanced repair accuracy of these models when compared to previous methods such as VulRepair, which underscores their practical utility and efficiency. The research also offers a critical assessment of current evaluation metrics, such as “Perfect Predictions”, and their limitations in reflecting the true capabilities of automated repair models in real-world scenarios. Following this, it underscores the importance of using test datasets devoid of train samples, emphasizing the need for dataset integrity to enhance the effectiveness of LLMs in code repair tasks. The significance of this work is its contribution to digital security, setting new standards for automated code vulnerability repair and paving the way for future advancements in the fields of cybersecurity and artificial intelligence. The study does not only highlight the potential of LLMs in enhancing code security but also fosters further exploration and research in these crucial areas.
ER  - 

TY  - JOUR
T1  - PMANet: Malicious URL detection via post-trained language model guided multi-level feature attention network
AU  - Liu, Ruitong
AU  - Wang, Yanbin
AU  - Xu, Haitao
AU  - Qin, Zhan
AU  - Zhang, Fan
AU  - Liu, Yiwei
AU  - Cao, Zheng
JO  - Information Fusion
VL  - 113
SP  - 102638
PY  - 2025
DA  - 2025/01/01/
SN  - 1566-2535
DO  - https://doi.org/10.1016/j.inffus.2024.102638
UR  - https://www.sciencedirect.com/science/article/pii/S1566253524004160
KW  - Malicious URL detection
KW  - Feature fusion
KW  - Transformer
KW  - Unsupervised post-training
KW  - Pre-trained language model
AB  - The expansion of the Internet has led to the widespread proliferation of malicious URLs, becoming a primary vector for cyber threats. Detecting malicious URLs is now essential for improving network security. The technological revolution spurred by pre-trained language models holds great promise for advancing the detection of malicious URLs. However, current research applying these models to URLs fails to address several crucial factors, including the lack of domain-specific adaptability, the omission of character-level information, and the neglect of both local detail extraction and low-order encoding information. In this paper, we propose PMANet, a pre-trained Language Model-Guided multi-level feature attention network, for addressing these issues. To facilitate a smooth transition of the pre-trained Transformer into the URL domain and to enable it to effectively capture information at both subword and character levels, we propose a post-training program that continues training the model on URLs using three self-supervised learning objectives: masked language model, noisy language model, and domain discrimination task. Subsequently, we develop a module to capture the output of each encoding layer, thus extracting hierarchical representations of URLs spanning from low-level to high-level. In addition, we propose a layer-wise attention mechanism that dynamically assigns weight coefficients to these feature layers based on their relevance. Finally, we apply spatial pyramid pooling to perform multi-scale down-sampling in order to obtain both local features and global context. PMANet achieves multifaceted integration in URL feature extraction, including capturing information at both the lexical and character levels, extracting features from low to high order, and discerning patterns at both global and local scales. We evaluate PMANet against challenging real-world scenarios, such as small-scale data, class imbalance, cross-dataset, adversarial attacks, and case studies on active malicious URLs. All experiments demonstrate that PMANet exhibits superiority over both the previous state-of-the-art pre-trained models and conventional deep learning models. Specifically, PMANet still achieves a 0.9941 AUC under adversarial attacks and correctly identifies all 20 actively malicious URLs in the case study. The code and data for our research are available at: https://github.com/Alixyvtte/Malicious-URL-Detection-PMANet.
ER  - 

TY  - JOUR
T1  - Multimodal Large Language Models in Health Care: Applications, Challenges, and Future Outlook
AU  - AlSaad, Rawan
AU  - Abd-alrazaq, Alaa
AU  - Boughorbel, Sabri
AU  - Ahmed, Arfan
AU  - Renault, Max-Antoine
AU  - Damseh, Rafat
AU  - Sheikh, Javaid
JO  - Journal of Medical Internet Research
VL  - 26
PY  - 2024
DA  - 2024/01/01/
SN  - 1438-8871
DO  - https://doi.org/10.2196/59505
UR  - https://www.sciencedirect.com/science/article/pii/S1438887124005983
KW  - artificial intelligence
KW  - large language models
KW  - multimodal large language models
KW  - multimodality
KW  - multimodal generative artificial intelligence
KW  - multimodal generative AI
KW  - generative artificial intelligence
KW  - generative AI
KW  - health care
AB  - In the complex and multidimensional field of medicine, multimodal data are prevalent and crucial for informed clinical decisions. Multimodal data span a broad spectrum of data types, including medical images (eg, MRI and CT scans), time-series data (eg, sensor data from wearable devices and electronic health records), audio recordings (eg, heart and respiratory sounds and patient interviews), text (eg, clinical notes and research articles), videos (eg, surgical procedures), and omics data (eg, genomics and proteomics). While advancements in large language models (LLMs) have enabled new applications for knowledge retrieval and processing in the medical field, most LLMs remain limited to processing unimodal data, typically text-based content, and often overlook the importance of integrating the diverse data modalities encountered in clinical practice. This paper aims to present a detailed, practical, and solution-oriented perspective on the use of multimodal LLMs (M-LLMs) in the medical field. Our investigation spanned M-LLM foundational principles, current and potential applications, technical and ethical challenges, and future research directions. By connecting these elements, we aimed to provide a comprehensive framework that links diverse aspects of M-LLMs, offering a unified vision for their future in health care. This approach aims to guide both future research and practical implementations of M-LLMs in health care, positioning them as a paradigm shift toward integrated, multimodal data–driven medical practice. We anticipate that this work will spark further discussion and inspire the development of innovative approaches in the next generation of medical M-LLM systems.
ER  - 

TY  - JOUR
T1  - Automated speech therapy through personalized pronunciation correction using reinforcement learning and large language models
AU  - Lakshminarayanan, Ritika
AU  - Shaik, Ayesha
AU  - Balasundaram, Ananthakrishnan
JO  - Results in Engineering
VL  - 25
SP  - 103943
PY  - 2025
DA  - 2025/03/01/
SN  - 2590-1230
DO  - https://doi.org/10.1016/j.rineng.2025.103943
UR  - https://www.sciencedirect.com/science/article/pii/S2590123025000313
KW  - Automatic speech recognition
KW  - Reinforcement learning
KW  - Proximal policy optimization
KW  - Large language model
KW  - Phonetic transcription
KW  - Speech synthesis markup language
AB  - Traditional approaches to pronunciation correction often face challenges in personalization, adaptability, and consistent feedback. This study introduces a novel AI-powered system that integrates Reinforcement Learning (RL) and Large Language Models (LLMs) to address these limitations. The system employs a custom Proximal Policy Optimization (PPO) algorithm for precise pronunciation evaluation and an Large Language Models to deliver detailed, encouraging, and user-specific feedback. It was evaluated using the CMU Sphinx Dictionary dataset, a foundational phonetic resource, alongside dynamically generated user-specific session data for personalized feedback and model refinement. Further validation utilized datasets such as TIMIT, LibriTTS, SpeechOcean762, and the Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS), enabling direct comparisons with contemporary methods. Results demonstrate the system's robustness in handling diverse phonetic variations. While primarily tested on English data, its modular architecture supports adaptation to other languages and dialects through language-specific phonetic datasets. The system achieved exceptional performance metrics: 97.9 % phoneme-level accuracy, 87.7 % word-level accuracy, 95.2 % syllable count accuracy, and 89.4 % perfect accuracy on the CMU Sphinx dataset. This innovative approach underscores the potential of advanced AI techniques to enhance the personalization and effectiveness of pronunciation correction systems. All findings are quantitatively validated and thoroughly documented.
ER  - 

TY  - JOUR
T1  - Generative fuzzer-driven vulnerability detection in the Internet of Things networks
AU  - Masud, Mohammed Tanvir
AU  - Koroniotis, Nickolaos
AU  - Keshk, Marwa
AU  - Turnbull, Benjamin
AU  - Kermanshahi, Shabnam Kasra
AU  - Moustafa, Nour
JO  - Applied Soft Computing
VL  - 174
SP  - 112973
PY  - 2025
DA  - 2025/04/01/
SN  - 1568-4946
DO  - https://doi.org/10.1016/j.asoc.2025.112973
UR  - https://www.sciencedirect.com/science/article/pii/S1568494625002844
KW  - Generative Adversarial Networks (GAN)
KW  - Fuzzing for vulnerability detection
KW  - Risk assessment
KW  - Deep learning-based fuzzing
KW  - Large language Model
AB  - The Internet of Things (IoT) paradigm has displayed tremendous growth in recent years, driving innovations such as Industry 4.0 and the creation of smart environments that enhance efficiency and asset management and enable intelligent decision-making. However, these benefits come with considerable cybersecurity risks due to inherent vulnerabilities within IoT ecosystems. Introducing potentially vulnerable IoT devices into secure environments, like smart airports, introduces new attack surfaces and vectors for exploitation. Identifying such vulnerabilities is challenging, and while traditional methods like penetration testing and vulnerability identification offer solutions, they often fall short due to IoT’s unique data diversity, hardware constraints, and complexity. We propose an intelligent mutation-based fuzzer for IoT vulnerability detection in networks to address these limitations, demonstrated through a smart airport case study. This method leverages Generative Adversarial Network (GAN)-based mutation, utilizing legitimate network communications (i.e., payloads) to produce fuzzed payloads that expose vulnerabilities. Additionally, we incorporate a large language model (LLM)-based risk assessment framework to evaluate the likelihood and impact of identified vulnerabilities, which is crucial for effectively prioritizing threats in interconnected IoT environments. This dual approach of vulnerability detection and LLM-driven risk assessment provides comprehensive insights into IoT security, enabling prioritized response actions. Experiments conducted in the UNSW Canberra IoT testbed confirm that our approach outperforms conventional vulnerability identification methods, offering a scalable solution for effective vulnerability detection and risk prioritization in complex IoT networks.
ER  - 

TY  - JOUR
T1  - Automatic instantiation of assurance cases from patterns using large language models
AU  - Odu, Oluwafemi
AU  - Belle, Alvine B.
AU  - Wang, Song
AU  - Kpodjedo, Segla
AU  - Lethbridge, Timothy C.
AU  - Hemmati, Hadi
JO  - Journal of Systems and Software
VL  - 222
SP  - 112353
PY  - 2025
DA  - 2025/04/01/
SN  - 0164-1212
DO  - https://doi.org/10.1016/j.jss.2025.112353
UR  - https://www.sciencedirect.com/science/article/pii/S0164121225000214
KW  - Requirement engineering
KW  - Assurance cases
KW  - Assurance case patterns
KW  - Pattern formalization
KW  - Generative artificial intelligence
KW  - Large language models
KW  - GPT
AB  - An assurance case is a structured set of arguments supported by evidence, demonstrating that a system’s non-functional requirements (e.g., safety, security, reliability) have been correctly implemented. Assurance case patterns serve as templates derived from previous successful assurance cases, aimed at facilitating the creation of new assurance cases. Despite using these patterns to generate assurance cases, their instantiation remains a largely manual and error-prone process that heavily relies on domain expertise. Thus, exploring techniques to support their automatic instantiation becomes crucial. This study aims to investigate the potential of Large Language Models (LLMs) in automating the generation of assurance cases that comply with specific patterns. Specifically, we formalize assurance case patterns using predicate-based rules and then utilize LLMs, i.e., GPT-4o and GPT-4 Turbo, to automatically instantiate assurance cases from these formalized patterns. Our findings suggest that LLMs can generate assurance cases that comply with the given patterns. However, this study also highlights that LLMs may struggle with understanding some nuances related to pattern-specific relationships. While LLMs exhibit potential in the automatic generation of assurance cases, their capabilities still fall short compared to human experts. Therefore, a semi-automatic approach to instantiating assurance cases may be more practical at this time.
ER  - 

TY  - JOUR
T1  - Evaluating large language models for software testing
AU  - Li, Yihao
AU  - Liu, Pan
AU  - Wang, Haiyang
AU  - Chu, Jie
AU  - Wong, W. Eric
JO  - Computer Standards & Interfaces
VL  - 93
SP  - 103942
PY  - 2025
DA  - 2025/04/01/
SN  - 0920-5489
DO  - https://doi.org/10.1016/j.csi.2024.103942
UR  - https://www.sciencedirect.com/science/article/pii/S0920548924001119
KW  - Large language models
KW  - LLM-driven testing
KW  - Evaluation
KW  - Hallucination
AB  - Large language models (LLMs) have demonstrated significant prowess in code analysis and natural language processing, making them highly valuable for software testing. This paper conducts a comprehensive evaluation of LLMs applied to software testing, with a particular emphasis on test case generation, error tracing, and bug localization across twelve open-source projects. The advantages and limitations, as well as recommendations associated with utilizing LLMs for these tasks, are delineated. Furthermore, we delve into the phenomenon of hallucination in LLMs, examining its impact on software testing processes and presenting solutions to mitigate its effects. The findings of this work contribute to a deeper understanding of integrating LLMs into software testing, providing insights that pave the way for enhanced effectiveness in the field.
ER  - 

TY  - JOUR
T1  - Training a language model to learn the syntax of commands
AU  - Hussain, Zafar
AU  - Nurminen, Jukka K.
AU  - Ranta-aho, Perttu
JO  - Array
VL  - 23
SP  - 100355
PY  - 2024
DA  - 2024/09/01/
SN  - 2590-0056
DO  - https://doi.org/10.1016/j.array.2024.100355
UR  - https://www.sciencedirect.com/science/article/pii/S2590005624000213
KW  - Command-line
KW  - Commands
KW  - Clustering
KW  - Random tokens
KW  - LLM
KW  - Language model
KW  - Markov
KW  - Command syntax
KW  - BERT
AB  - To protect systems from malicious activities, it is important to differentiate between valid and harmful commands. One way to achieve this is by learning the syntax of the commands, which is a complex task because of the expansive and evolving nature of command syntax. To address this, we harnessed the power of a language model. Our methodology involved constructing a specialized vocabulary from our commands dataset, and training a custom tokenizer with a Masked Language Model head, resulting in the development of a BERT-like language model. This model exhibits proficiency in learning command syntax by predicting masked tokens. In comparative analyses, our language model outperformed the Markov Model in categorizing commands using clustering algorithms (DBSCAN, HDBSCAN, OPTICS). The language model achieved higher Silhouette scores (0.72, 0.88, 0.85) compared to the Markov Model (0.53, 0.25, 0.06) and demonstrated significantly lower noise levels (2.63%, 5.39%, 8.49%) versus the Markov Model’s higher noise rates (9.31%, 29.85%, 50.35%). Further validation with manually crafted syntax and BERTScore assessments consistently produced metrics above 0.90 for precision, recall, and F1-score. Our language model excels at learning command syntax, enhancing protective measures against malicious activities.
ER  - 

TY  - JOUR
T1  - Cyber threat detection: Unsupervised hunting of anomalous commands (UHAC)
AU  - Kayhan, Varol O.
AU  - Agrawal, Manish
AU  - Shivendu, Shivendu
JO  - Decision Support Systems
VL  - 168
SP  - 113928
PY  - 2023
DA  - 2023/05/01/
SN  - 0167-9236
DO  - https://doi.org/10.1016/j.dss.2023.113928
UR  - https://www.sciencedirect.com/science/article/pii/S0167923623000039
KW  - Cybersecurity
KW  - Threat hunting
KW  - Anomaly detection
KW  - Autoencoder
AB  - The cyber security industry is rapidly adopting threat hunting as a proactive tool for early and faster detection of suspected malicious actors. In this paper, we propose a machine learning-based method, Unsupervised Hunting of Anomalous Commands (UHAC), to detect text-based anomalous commands in security information and event management (SIEM) logs that are good candidates for threat hunting. A unique feature of the proposed method is that it first creates a feature set based on the augmentation of document-term and document-character matrices. Then, an autoencoder-based detector is trained on this feature set using a custom loss function. UHAC consistently outperforms other feature sets and algorithms such as one-class support vector machine, density-based spatial clustering of applications with noise, and word-embedding based models such as word2vec. The UHAC detector identifies 84–89% of anomalies in the top 10% of the data. Findings have implications for cybersecurity analysts who perform threat hunting in SIEM logs for process auditing on endpoint devices.
ER  - 

TY  - JOUR
T1  - Zero- and few-shot Chinese cybersecurity event detection via meta-distillation learning
AU  - Zhang, Han
AU  - Xu, Bingzhi
AU  - Xiao, Shijie
AU  - Zhang, Chengfang
AU  - Ji, Lixia
JO  - Information Processing & Management
VL  - 63
IS  - 1
SP  - 104344
PY  - 2026
DA  - 2026/01/01/
SN  - 0306-4573
DO  - https://doi.org/10.1016/j.ipm.2025.104344
UR  - https://www.sciencedirect.com/science/article/pii/S0306457325002857
KW  - Cyber threat intelligence
KW  - Dataset construction
KW  - Chinese event detection
KW  - Deep learning
AB  - Traditional cybersecurity event detection has primarily focused on English corpora. However, Chinese corpora pose challenges due to linguistic complexity and the lack of annotated datasets, particularly in recognizing nested compound trigger words and handling zero- and few-shot scenarios. To address these issues, we propose a method, named zero- and few-shot Chinese cybersecurity event detection via meta-distillation learning (CCED). Firstly, we introduce a dynamic dimension transformation mechanism to embed geometric information into span representations for nested compound trigger words extraction in a Chinese corpus. Secondly, we propose meta-distillation learning, which integrates meta-learning with contrastive knowledge distillation to improve model performance. This method boosts accuracy in zero- and few-shot scenarios by facilitating knowledge transfer across tasks. Moreover, to fill the gap in datasets for Chinese cybersecurity event detection, we develop CSED, to the best of our knowledge, the first publicly available annotated dataset in this domain. It includes a large collection of news articles from sources like CNCERT and Twitter, with 17,542 event instances, categorized into 2 event types and 9 sub-types. CCED achieves state-of-the-art F1 scores on CSED, with 57.61%, 76.83%, and 79.14% in zero-shot and few-shot settings, respectively. The dataset and code can be accessed on GitHub: https://github.com/vegetable-edu/CCED.
ER  - 

TY  - JOUR
T1  - Potential of multimodal large language models for data mining of medical images and free-text reports
AU  - Zhang, Yutong
AU  - Pan, Yi
AU  - Zhong, Tianyang
AU  - Dong, Peixin
AU  - Xie, Kangni
AU  - Liu, Yuxiao
AU  - Jiang, Hanqi
AU  - Wu, Zihao
AU  - Liu, Zhengliang
AU  - Zhao, Wei
AU  - Zhang, Wei
AU  - Zhao, Shijie
AU  - Zhang, Tuo
AU  - Jiang, Xi
AU  - Shen, Dinggang
AU  - Liu, Tianming
AU  - Zhang, Xin
JO  - Meta-Radiology
VL  - 2
IS  - 4
SP  - 100103
PY  - 2024
DA  - 2024/12/01/
SN  - 2950-1628
DO  - https://doi.org/10.1016/j.metrad.2024.100103
UR  - https://www.sciencedirect.com/science/article/pii/S2950162824000572
AB  - Medical images and radiology reports are essential for physicians to diagnose medical conditions. However, the vast diversity and cross-source heterogeneity inherent in these data have posed significant challenges to the generalizability of current data-mining methods for clinical decision-making. Recently, multimodal large language models (MLLMs), especially Gemini-Vision-series (Gemini) and GPT-4-series (GPT-4) models, have revolutionized numerous domains, significantly impacting the medical field. In this study, we conducted a detailed evaluation of the performance of the Gemini series models (including Gemini-1.0-Pro-Vision, Gemini-1.5-Pro, and Gemini-1.5-Flash) and GPT series models (including GPT-4o, GPT-4-Turbo, and GPT-3.5-Turbo) across 14 medical datasets, covering 5 medical imaging categories (dermatology, radiology, dentistry, ophthalmology, and endoscopy) and 3 radiology report datasets. The investigated tasks encompass disease classification, lesion segmentation, anatomical localization, disease diagnosis, report generation, and lesion detection. Moreover, we also validated the performance of the Claude-3-Opus, Yi-Large, Yi-Large-Turbo, and LLaMA 3 models to gain a comprehensive understanding of the MLLM models in the medical field. Our experimental results demonstrated that Gemini-series models excelled in report generation and lesion detection but faces challenges in disease classification and anatomical localization. Conversely, GPT-series models exhibited proficiency in lesion segmentation and anatomical localization but encountered difficulties in disease diagnosis and lesion detection. Additionally, both the Gemini series and GPT series contain models that have demonstrated commendable generation efficiency. While both models hold promise in reducing physician workload, alleviating pressure on limited healthcare resources, and fostering collaboration between clinical practitioners and artificial intelligence technologies, substantial enhancements and comprehensive validations remain imperative before clinical deployment.
ER  - 

TY  - JOUR
T1  - SpAIware: Uncovering a novel artificial intelligence attack vector through persistent memory in LLM applications and agents
AU  - Herrador, Manuel
AU  - Rehberger, Johann
JO  - Future Generation Computer Systems
VL  - 174
SP  - 107994
PY  - 2026
DA  - 2026/01/01/
SN  - 0167-739X
DO  - https://doi.org/10.1016/j.future.2025.107994
UR  - https://www.sciencedirect.com/science/article/pii/S0167739X25002894
KW  - SpAIware
KW  - Prompt injection
KW  - Data exfiltration
KW  - AI security
KW  - long-term memory
AB  - As generative AI systems become more advanced, new security vulnerabilities emerge, particularly in Large Language Models (LLMs) like GPT (Generative Pre-trained Transformer) with persistent memory capabilities. This paper introduces "SpAIware", a novel cybersecurity threat exploiting persistent memory vulnerabilities in LLM applications. We demonstrate how malicious actors can leverage generative AI to inject and persistently store harmful instructions across multiple chat sessions, enabling continuous data exfiltration. Our proof-of-concept on ChatGPT reveals critical security flaws in AI systems with long-term memory capabilities, showcasing an advanced form of automated hacking. We analyze the potential impacts on vulnerability assessment, cyber defense automation, and incident response. The study also examines the ethical implications of using generative AI in both attack and defense scenarios. We propose a range of technical, regulatory, and educational countermeasures, underscoring the urgent need for AI-specific security protocols. Our findings highlight a significant gap in current cybersecurity solutions, potentially spawning a new industry of AI-focused security tools. This research emphasizes the critical importance of proactive security measures and ethical considerations in the rapidly evolving landscape of generative AI technologies in cybersecurity.
ER  - 

TY  - JOUR
T1  - LaAeb: A comprehensive log-text analysis based approach for insider threat detection
AU  - Fei, Kexiong
AU  - Zhou, Jiang
AU  - Zhou, Yucan
AU  - Gu, Xiaoyan
AU  - Fan, Haihui
AU  - Li, Bo
AU  - Wang, Weiping
AU  - Chen, Yong
JO  - Computers & Security
VL  - 148
SP  - 104126
PY  - 2025
DA  - 2025/01/01/
SN  - 0167-4048
DO  - https://doi.org/10.1016/j.cose.2024.104126
UR  - https://www.sciencedirect.com/science/article/pii/S0167404824004310
KW  - Insider threat detection
KW  - Log analysis
KW  - Anomaly detection
KW  - Cyber security
KW  - User behavior analysis
AB  - Insider threats have increasingly become a critical issue that modern enterprises and organizations faced. They are mainly initiated by insider attackers, which may cause disastrous impacts. Numerous research studies have been conducted for insider threat detection. However, most of them are limited due to a small number of malicious samples. Moreover, as existing methods often concentrate on feature information or statistical characteristics for anomaly detection, they still lack effective use of comprehensive textual content information contained in logs and thus will affect detection efficiency. We propose LaAeb, a novel unsupervised insider threat detection framework that leverages rich linguistic information in log contents to enable conventional methods, such as an Isolation Forest-based anomaly detection, to better detect insider threats besides using various features and statistical information. To find malicious acts under different scenarios, we consider three patterns of insider threats, including attention, emotion, and behavior anomaly. The attention anomaly detection analyzes textual contents of operation objects (e.g., emails and web pages) in logs to detect threats, where the textual information reflects the areas that employees focus on. When the attention seriously deviates from daily work, an employee may involve malicious acts. The emotion anomaly detection analyzes all dialogs between every two employees’ daily communicated texts and uses the degree of negative to find potential psychological problems. The behavior anomaly detection analyzes the operations of logs to detect threats. It utilizes information acquired from attention and emotion anomalies as ancillary features, integrating them with features and statistics extracted from log operations to create log embeddings. With these log embeddings, LaAeb employs anomaly detection algorithm like Isolation Forest to analyze an employee’s malicious operations, and further detects the employee’s behavior anomaly by considering all employees’ acts in the same department. Finally, LaAeb consolidates detection results of three patterns indicative of insider threats in a comprehensive manner. We implement the prototype of LaAeb and test it on CERT and LANL datasets. Our evaluations demonstrate that compared with state-of-the-art unsupervised methods, LaAeb reduces FPR by 50% to reach 0.05 on CERT dataset under the same AUC (0.93), and gets the best AUC (0.97) with 0.06 higher value on LANL dataset.
ER  - 

TY  - JOUR
T1  - Forecasting Cyber Threats and Pertinent Mitigation Technologies
AU  - Almahmoud, Zaid
AU  - Yoo, Paul D.
AU  - Damiani, Ernesto
AU  - Choo, Kim-Kwang Raymond
AU  - Yeun, Chan Yeob
JO  - Technological Forecasting and Social Change
VL  - 210
SP  - 123836
PY  - 2025
DA  - 2025/01/01/
SN  - 0040-1625
DO  - https://doi.org/10.1016/j.techfore.2024.123836
UR  - https://www.sciencedirect.com/science/article/pii/S0040162524006346
KW  - Cyber threat trend forecasting
KW  - Mitigation technology trend forecasting
KW  - Alleviation technology trend forecasting
KW  - Proactive approach
KW  - Big data analytics
KW  - Graph machine learning
KW  - Technology cycle
AB  - Geopolitical instability is exacerbating the risk of catastrophic cyber-attacks striking where defences are weak. Nevertheless, cyber-attack trend forecasting predominantly relies on human expertise, which is susceptible to subjectivity and potential bias. As a solution, we have recently presented a novel study that harnesses machine learning for long-term cyber-attack forecasting. Building upon this groundwork, our research advances to the next level, by predicting the disparity between cyber-attack trends and the trend of the relevant alleviation technologies. The proposed approach applies key constructs of Protection Motivation Theory while introducing a proactive version of the theory. Our predictive analysis aims to offer strategic insights for the decision of investment in cyber security technologies. It also provides a sound foundation for the strategic decisions of national defence agencies. To achieve this objective, we have expanded our dataset, which now encompasses records spanning 42 distinct cyber-attack types and various related features, alongside data concerning the trends of 98 pertinent technologies, dating back to 2011. The dataset features were meticulously curated from diverse sources, including news articles, blogs, government advisories, as well as from platforms such as Elsevier, Twitter, and Python APIs. With our comprehensive dataset in place, we construct a graph that elucidates the intricate interplay between cyber threats and the development of pertinent alleviation technologies. To forecast the graph, we introduce a novel Bayesian adaptation of a recently proposed graph neural network model, which effectively captures and predicts these trends. We further demonstrate the efficacy of our proposed features in this context. Furthermore, our study extends its horizon by generating future data projections for the next three years, encompassing forecasts for the evolving graph, including predictions of the gap between cyber-attack trends and the trend of the associated technologies. As a consequential outcome of our forecasting efforts, we introduce the concept of “alleviation technologies cycle”, delineating the key phases in the life cycle of 98 technologies. These findings serve as a foundational resource, offering valuable guidance for future investment and strategic defence decisions within the realm of cyber security related technologies.
ER  - 

TY  - JOUR
T1  - Vul-LMGNNs: Fusing language models and online-distilled graph neural networks for code vulnerability detection
AU  - Liu, Ruitong
AU  - Wang, Yanbin
AU  - Xu, Haitao
AU  - Sun, Jianguo
AU  - Zhang, Fan
AU  - Li, Peiyue
AU  - Guo, Zhenhao
JO  - Information Fusion
VL  - 115
SP  - 102748
PY  - 2025
DA  - 2025/03/01/
SN  - 1566-2535
DO  - https://doi.org/10.1016/j.inffus.2024.102748
UR  - https://www.sciencedirect.com/science/article/pii/S1566253524005268
KW  - Code vulnerability detection
KW  - Graph information fusion
KW  - Pre-trained code model
KW  - Joint training
AB  - Code Language Models (codeLMs) and Graph Neural Networks (GNNs) are widely used in code vulnerability detection. However, a critical yet often overlooked issue is that GNNs primarily rely on aggregating information from adjacent nodes, limiting structural information transfer to single-layer updates. In code graphs, nodes and relationships typically require cross-layer information propagation to fully capture complex program logic and potential vulnerability patterns. Furthermore, while some studies utilize codeLMs to supplement GNNs with code semantic information, existing integration methods have not fully explored the potential of their collaborative effects. To address these challenges, we introduce Vul-LMGNNs that integrates pre-trained CodeLMs with GNNs, leveraging knowledge distillation to facilitate cross-layer propagation of both code semantic knowledge and structural information. Specifically, Vul-LMGNNs utilizes Code Property Graphs (CPGs) to incorporate code syntax, control flow, and data dependencies, while employing gated GNNs to extract structural information in the CPG. To achieve cross-layer information transmission, we implement an online knowledge distillation (KD) program that enables a single student GNN to acquire structural information extracted from a simultaneously trained counterpart through an alternating training procedure. Additionally, we leverage pre-trained CodeLMs to extract semantic features from code sequences. Finally, we propose an ”implicit-explicit” joint training framework to better leverage the strengths of both CodeLMs and GNNs. In the implicit phase, we utilize CodeLMs to initialize the node embeddings of each student GNN. Through online knowledge distillation, we facilitate the propagation of both code semantics and structural information across layers. In the explicit phase, we perform linear interpolation between the CodeLM and the distilled GNN to learn a late fusion model. The proposed method, evaluated across four real-world vulnerability datasets, demonstrated superior performance compared to 17 state-of-the-art approaches. Our source code can be accessed via GitHub: https://github.com/Vul-LMGNN/vul-LMGGNN.
ER  - 

TY  - JOUR
T1  - A novel feature integration and entity boundary detection for named entity recognition in cybersecurity
AU  - Wang, Xiaodi
AU  - Liu, Jiayong
JO  - Knowledge-Based Systems
VL  - 260
SP  - 110114
PY  - 2023
DA  - 2023/01/25/
SN  - 0950-7051
DO  - https://doi.org/10.1016/j.knosys.2022.110114
UR  - https://www.sciencedirect.com/science/article/pii/S0950705122012102
KW  - Cyber threat intelligence
KW  - Named entity recognition
KW  - Cybersecurity
KW  - PERT
KW  - Entity boundary
AB  - Owing to continuous cyberattacks, a large amount of threat intelligence is generated online every day. However, threat intelligence is mostly unstructured and multisource heterogeneous text. It is difficult for security analysts to understand the implicit threat in time. Knowledge Graph (KG) is an important research topic in recent years, which can perform automated and real-time analysis of threat intelligence in cybersecurity. As one of the critical technologies of KG, named entity recognition (NER) can identify cyberattack-related entities. It has been proved that long-distance structured information captured by dependency trees provides a rich semantic expression for the neural network. However, existing research works are more focused on the simple linear stack of neural networks when utilizing structured features. The interaction between different types of neural networks is vague. In addition, the existing models are insensitive to the boundaries of complex entity terms in cybersecurity. In this study, we propose a new feature integration and entity boundary detection (FIEBD) model. In our model, a new pretrained language model, PERT, is applied to obtain word embedding of cyber texts. Moreover, a novel neural network cell, namely GARU, is developed to incorporate different types of features extracted from graph neural networks and recurrent neural networks. It combines the graph encoder with the gate mechanism, aiming to obtain better hidden representation by explicit interaction. Furthermore, considering a large number of complex entities in cybersecurity, we contribute an entity boundary detection module to perform entity head and tail prediction as an augmentation task. We conduct extensive experiments on cybersecurity datasets. The results demonstrate that the proposed model achieves better performance than existing baseline methods.
ER  - 

TY  - JOUR
T1  - Digital innovation and supply chain risk: A large language model-based analysis
AU  - Fan, Siyu
AU  - Kong, Dongmin
AU  - Wu, Yifei
AU  - Yu, Honghai
JO  - Pacific-Basin Finance Journal
VL  - 92
SP  - 102799
PY  - 2025
DA  - 2025/09/01/
SN  - 0927-538X
DO  - https://doi.org/10.1016/j.pacfin.2025.102799
UR  - https://www.sciencedirect.com/science/article/pii/S0927538X25001362
KW  - Supply chain risk
KW  - Digital technology
KW  - Large language models
AB  - We construct a firm-level supply chain risk measure using a novel approach based on large language models (LLMs) and explore whether digital innovation impacts this risk. Our findings reveal that firms with higher levels of digital innovation exhibit significantly lower supply chain risk exposure. These results are robust and remain significant after controlling for endogeneity issues. Moreover, the mitigating effect of digital innovation is particularly pronounced in firms with greater geographical supply chain distances, higher operational complexity, extensive overseas operations, short-term relationships with partners, and those in the manufacturing sector. We further demonstrate that digital innovation enhances information sharing and improves operational efficiency, serving as potential mechanisms for supply chain risk reduction. Overall, our results emphasize the significant role of digital innovation in enhancing supply chain resilience and contribute to the expanding literature on applying LLMs in finance.
ER  - 

TY  - JOUR
T1  - Vulnerability detection using BERT based LLM model with transparency obligation practice towards trustworthy AI
AU  - Haurogné, Jean
AU  - Basheer, Nihala
AU  - Islam, Shareeful
JO  - Machine Learning with Applications
VL  - 18
SP  - 100598
PY  - 2024
DA  - 2024/12/01/
SN  - 2666-8270
DO  - https://doi.org/10.1016/j.mlwa.2024.100598
UR  - https://www.sciencedirect.com/science/article/pii/S2666827024000744
KW  - Large language model
KW  - Transparency obligation
KW  - Vulnerability
KW  - Explainability
KW  - BERT model
KW  - EU AI Act
AB  - Vulnerabilities in the source code are one of the main causes of potential threats in software-intensive systems. There are a large number of vulnerabilities published each day, and effective vulnerability detection is critical to identifying and mitigating these vulnerabilities. AI has emerged as a promising solution to enhance vulnerability detection, offering the ability to analyse vast amounts of data and identify patterns indicative of potential threats. However, AI-based methods often face several challenges, specifically when dealing with large datasets and understanding the specific context of the problem. Large Language Model (LLM) is now widely considered to tackle more complex tasks and handle large datasets, which also exhibits limitations in terms of explaining the model outcome and existing works focus on providing overview of explainability and transparency. This research introduces a novel transparency obligation practice for vulnerability detection using BERT based LLMs. We address the black-box nature of LLMs by employing XAI techniques, unique combination of SHAP, LIME, heat map. We propose an architecture that combines the BERT model with transparency obligation practices, which ensures the assurance of transparency throughout the entire LLM life cycle. An experiment is performed with a large source code dataset to demonstrate the applicability of the proposed approach. The result shows higher accuracy of 91.8 % for the vulnerability detection and model explainability outcome is highly influenced by “vulnerable”, “function”, "mysql_tmpdir_list", “strmov” tokens using both SHAP and LIME framework. Heatmap of attention weights, highlights the local token interactions that aid in understanding the model's decision points.
ER  - 

TY  - JOUR
T1  - LLMs are one-shot URL classifiers and explainers
AU  - Rashid, Fariza
AU  - Ranaweera, Nishavi
AU  - Doyle, Ben
AU  - Seneviratne, Suranga
JO  - Computer Networks
VL  - 258
SP  - 111004
PY  - 2025
DA  - 2025/02/01/
SN  - 1389-1286
DO  - https://doi.org/10.1016/j.comnet.2024.111004
UR  - https://www.sciencedirect.com/science/article/pii/S1389128624008363
KW  - Phishing detection
KW  - Large language models
KW  - URL classification
AB  - Malicious URL classification represents a crucial aspect of cybersecurity. Although existing work comprises numerous machine learning and deep learning-based URL classification models, most suffer from generalisation and domain-adaptation issues arising from the lack of representative training datasets. Furthermore, these models fail to provide explanations for a given URL classification in natural human language. In this work, we investigate and demonstrate the use of Large Language Models (LLMs) to address this issue. Specifically, we propose an LLM-based one-shot learning framework to predict whether a given URL is benign or phishing. Inspired by work done in the area of Chain-of-Thought reasoning, our framework draws on LLMs’ reasoning capabilities to produce more accurate predictions. We evaluate our framework using three URL datasets and five state-of-the-art LLMs, and show that one-shot LLM prompting indeed provides performances close to supervised models, with GPT 4-Turbo being the best model returning an average F1 score of 0.92 in the one-shot setting. We conduct a quantitative analysis of the LLM explanations and show that most of the explanations provided by LLMs align with the post-hoc explanations of the supervised classifiers, and the explanations have high readability, coherency, and informativeness.
ER  - 

TY  - JOUR
T1  - Safety analysis in the era of large language models: A case study of STPA using ChatGPT
AU  - Qi, Yi
AU  - Zhao, Xingyu
AU  - Khastgir, Siddartha
AU  - Huang, Xiaowei
JO  - Machine Learning with Applications
VL  - 19
SP  - 100622
PY  - 2025
DA  - 2025/03/01/
SN  - 2666-8270
DO  - https://doi.org/10.1016/j.mlwa.2025.100622
UR  - https://www.sciencedirect.com/science/article/pii/S2666827025000052
KW  - STPA
KW  - Safety–critical systems
KW  - Large language models
KW  - Safe AI
KW  - Human machine interaction
KW  - Hazards identification
KW  - Safety assurance
AB  - Can safety analysis leverage Large Language Models (LLMs)? This study examines the application of Systems Theoretic Process Analysis (STPA) to Automatic Emergency Brake (AEB) and Electricity Demand Side Management (DSM) systems, utilising Chat Generative Pre-Trained Transformer (ChatGPT). We investigate the impact of collaboration schemes, input semantic complexity, and prompt engineering on STPA results. Comparative results indicate that using ChatGPT without human intervention may be inadequate due to reliability issues. However, with careful design, it has the potential to outperform human experts. No statistically significant differences were observed when varying the input semantic complexity or using domain-agnostic prompt guidelines. While STPA-specific prompt engineering produced statistically significant and more pertinent results, ChatGPT generally yielded more conservative and less comprehensive outcomes. We also identify future challenges, such as concerns regarding the trustworthiness of LLMs and the need for standardisation and regulation in this field. All experimental data are publicly accessible.
ER  - 

TY  - JOUR
T1  - LEGF-DST: LLMs-Enhanced Graph-Fusion Dual-Stream Transformer for Fine-Grained Chinese Malicious SMS Detection
AU  - Tong, Xin
AU  - Wang, Jingya
AU  - Yang, Ying
AU  - Peng, Tian
AU  - Zhai, Hanming
AU  - Ling, Guangming
JO  - Computers, Materials and Continua
VL  - 82
IS  - 2
SP  - 1901
EP  - 1924
PY  - 2025
DA  - 2025/02/17/
SN  - 1546-2218
DO  - https://doi.org/10.32604/cmc.2024.059018
UR  - https://www.sciencedirect.com/science/article/pii/S1546221825000906
KW  - Transformers
KW  - malicious SMS
KW  - multi-task learning
KW  - large language models
AB  - With the widespread use of SMS (Short Message Service), the proliferation of malicious SMS has emerged as a pressing societal issue. While deep learning-based text classifiers offer promise, they often exhibit suboptimal performance in fine-grained detection tasks, primarily due to imbalanced datasets and insufficient model representation capabilities. To address this challenge, this paper proposes an LLMs-enhanced graph fusion dual-stream Transformer model for fine-grained Chinese malicious SMS detection. During the data processing stage, Large Language Models (LLMs) are employed for data augmentation, mitigating dataset imbalance. In the data input stage, both word-level and character-level features are utilized as model inputs, enhancing the richness of features and preventing information loss. A dual-stream Transformer serves as the backbone network in the learning representation stage, complemented by a graph-based feature fusion mechanism. At the output stage, both supervised classification cross-entropy loss and supervised contrastive learning loss are used as multi-task optimization objectives, further enhancing the model’s feature representation. Experimental results demonstrate that the proposed method significantly outperforms baselines on a publicly available Chinese malicious SMS dataset.
ER  - 

TY  - JOUR
T1  - A cognitive platform for collecting cyber threat intelligence and real-time detection using cloud computing
AU  - Balasubramanian, Prasasthy
AU  - Nazari, Sadaf
AU  - Kholgh, Danial Khosh
AU  - Mahmoodi, Alireza
AU  - Seby, Justin
AU  - Kostakos, Panos
JO  - Decision Analytics Journal
VL  - 14
SP  - 100545
PY  - 2025
DA  - 2025/03/01/
SN  - 2772-6622
DO  - https://doi.org/10.1016/j.dajour.2025.100545
UR  - https://www.sciencedirect.com/science/article/pii/S2772662225000013
KW  - Cyber threat intelligence
KW  - Machine learning operations
KW  - Classification
KW  - Indicators of compromise
KW  - Bidirectional encoder representations from transformers (BERT)
KW  - Longformer
AB  - The extraction of cyber threat intelligence (CTI) from open sources is a rapidly expanding defensive strategy that enhances the resilience of both Information Technology (IT) and Operational Technology (OT) environments against large-scale cyber-attacks. However, for most organizations, collecting actionable CTI remains both a technical bottleneck and a black box. While previous research has focused on improving individual components of the extraction process, the community lacks open-source platforms for deploying streaming CTI data pipelines in the wild. This study proposes an efficient platform capable of processing compute-intensive data pipelines, based on cloud computing, for real-time detection, collection, and sharing of CTI from various online sources. We developed a prototype platform (TSTEM) with a containerized microservice architecture that uses Tweepy, Scrapy, Terraform, Elasticsearch, Logstash, and Kibana (ELK), Kafka, and Machine Learning Operations (MLOps) to autonomously search, extract, and index indicators of compromise (IOCs) in the wild. Moreover, the provisioning, monitoring, and management of the platform are achieved through infrastructure as code (IaC). Custom focus-crawlers collect web content, processed by a first-level classifier to identify potential IOCs. Relevant content advances to a second level for further examination. State-of-the-art natural language processing (NLP) models are used for classification and entity extraction, enhancing the IOC extraction methodology. Our results indicate these models exhibit high accuracy (exceeding 98%) in classification and extraction tasks, achieving this performance within less than a minute. The system’s effectiveness is due to a finely-tuned IOC extraction method that operates at multiple stages, ensuring precise identification with low false positives.
ER  - 

TY  - JOUR
T1  - Vulcan: Automatic extraction and analysis of cyber threat intelligence from unstructured text
AU  - Jo, Hyeonseong
AU  - Lee, Yongjae
AU  - Shin, Seungwon
JO  - Computers & Security
VL  - 120
SP  - 102763
PY  - 2022
DA  - 2022/09/01/
SN  - 0167-4048
DO  - https://doi.org/10.1016/j.cose.2022.102763
UR  - https://www.sciencedirect.com/science/article/pii/S0167404822001584
KW  - Cyber threat intelligence
KW  - CTI
KW  - Cybersecurity
KW  - Information extraction
KW  - Language model
AB  - To counteract the rapidly evolving cyber threats, many research efforts have been made to design cyber threat intelligence (CTI) systems that extract CTI data from publicly available sources. Specifically, indicators of compromise (IOC), such as file hash and IP address, receives the most attention among security researchers. However, the ability of IOC-centric CTI systems to understand and detect threats remains questionable for two reasons. First, IOCs are forensic artifacts that indicate that an endpoint or network has been compromised. They cannot depict the technical details of threats. Second, attackers frequently change infrastructure and static indicators, which makes IOCs have a very short lifespan. Therefore, when designing a CTI system, we should turn our attention to other types of CTI data that are helpful in threat understanding and detection (e.g., attack vector, tool). In this work, we propose Vulcan, a novel CTI system that extracts descriptive or static CTI data from unstructured text and determines their semantic relationships. To do this, we design a neural language model-based named entity recognition (NER) and relation extraction (RE) models tailored for cybersecurity domain. The experimental results confirm that Vulcan is highly accurate with an average F1-score of 0.972 and 0.985 for NER and RE tasks, respectively. Vulcan also provides an environment where security practitioners can develop applications for threat analysis. To prove the applicability of Vulcan, we introduce two applications, evolution identification and threat profiling. The applications save time and labor costs to analyze cyber threats and show the detailed characteristics of the threats.
ER  - 
